{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51927743-a89d-48bc-870f-a4e70f8eb100",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://scholar.google.es/citations?user=n-YN5poAAAAJ&hl=es'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cf91b7-b323-433f-906d-ea2f90800874",
   "metadata": {},
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# URL of the website to scrape PDFs from\n",
    "url = 'https://scholar.google.com/citations?user=Y8sFm4oAAAAJ&hl=en'\n",
    "\n",
    "# Create the main directory for saving the PDFs\n",
    "main_directory = 'scraper'\n",
    "domain_name = url.split('//')[1].split('/')[0]  # Extract domain name from URL\n",
    "subdirectory = os.path.join(main_directory, domain_name)\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(subdirectory, exist_ok=True)\n",
    "\n",
    "# Send a GET request to the webpage\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    pdf_links = []\n",
    "    \n",
    "    # Find all anchor tags with href ending in .pdf\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        if link['href'].endswith('.pdf'):\n",
    "            full_url = urljoin(url, link['href'])\n",
    "            pdf_links.append(full_url)\n",
    "        \n",
    "        # Stop after finding 5 files\n",
    "        if len(pdf_links) >= 5:\n",
    "            break\n",
    "\n",
    "    # Download and save each PDF\n",
    "    for i, pdf_url in enumerate(pdf_links):\n",
    "        pdf_response = requests.get(pdf_url)\n",
    "        \n",
    "        if pdf_response.status_code == 200:\n",
    "            pdf_path = os.path.join(subdirectory, f'file_{i + 1}.pdf')\n",
    "            with open(pdf_path, 'wb') as pdf_file:\n",
    "                pdf_file.write(pdf_response.content)\n",
    "            print(f'Downloaded {pdf_path}')\n",
    "        else:\n",
    "            print(f'Failed to download {pdf_url}')\n",
    "else:\n",
    "    print(f'Failed to retrieve the webpage. Status code: {response.status_code}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0854a41e-6741-450c-a711-8906a4b204fb",
   "metadata": {},
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# URL of the website to scrape PDFs from\n",
    "url = 'https://scholar.google.com/citations?user=Y8sFm4oAAAAJ&hl=en'\n",
    "\n",
    "# Headers to mimic a browser request\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Create the main directory for saving the PDFs\n",
    "main_directory = 'scraper'\n",
    "domain_name = url.split('//')[1].split('/')[0]  # Extract domain name from URL\n",
    "subdirectory = os.path.join(main_directory, domain_name)\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(subdirectory, exist_ok=True)\n",
    "\n",
    "# Send a GET request to the webpage\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    pdf_links = []\n",
    "\n",
    "    # Find all anchor tags with href ending in .pdf\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        if link['href'].endswith('.pdf'):\n",
    "            full_url = urljoin(url, link['href'])\n",
    "            pdf_links.append(full_url)\n",
    "\n",
    "        # Stop after finding 5 files\n",
    "        if len(pdf_links) >= 5:\n",
    "            break\n",
    "\n",
    "    # Download and save each PDF\n",
    "    for i, pdf_url in enumerate(pdf_links):\n",
    "        time.sleep(2)  # Add delay to avoid hitting rate limits\n",
    "        pdf_response = requests.get(pdf_url, headers=headers)\n",
    "\n",
    "        if pdf_response.status_code == 200:\n",
    "            pdf_path = os.path.join(subdirectory, f'file_{i + 1}.pdf')\n",
    "            with open(pdf_path, 'wb') as pdf_file:\n",
    "                pdf_file.write(pdf_response.content)\n",
    "            print(f'Downloaded {pdf_path}')\n",
    "        else:\n",
    "            print(f'Failed to download {pdf_url}. Status code: {pdf_response.status_code}')\n",
    "else:\n",
    "    print(f'Failed to retrieve the webpage. Status code: {response.status_code}')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "84db32a8-4789-40e0-8605-a2b2a8f4b926",
   "metadata": {},
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Base URL of the website to scrape\n",
    "base_url = 'https://scholar.google.com/citations?user=Y8sFm4oAAAAJ&hl=en'\n",
    "\n",
    "# Headers to mimic a browser request\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Create the main directory for saving the PDFs\n",
    "main_directory = 'scraper'\n",
    "domain_name = base_url.split('//')[1].split('/')[0]  # Extract domain name from URL\n",
    "subdirectory = os.path.join(main_directory, domain_name)\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(subdirectory, exist_ok=True)\n",
    "\n",
    "# Send a GET request to the base URL\n",
    "response = requests.get(base_url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    pdf_links = []\n",
    "\n",
    "    # Find all anchor tags that direct to individual pages\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        page_url = urljoin(base_url, link['href'])\n",
    "        \n",
    "        # Send a GET request to the individual page\n",
    "        page_response = requests.get(page_url, headers=headers)\n",
    "        time.sleep(2)  # Add delay to avoid rate limiting\n",
    "        \n",
    "        if page_response.status_code == 200:\n",
    "            page_soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "            \n",
    "            # Find PDF links on the individual page\n",
    "            for pdf_link in page_soup.find_all('a', href=True):\n",
    "                if pdf_link['href'].endswith('.pdf'):\n",
    "                    full_pdf_url = urljoin(page_url, pdf_link['href'])\n",
    "                    pdf_links.append(full_pdf_url)\n",
    "                    \n",
    "                    # Stop after finding 5 files\n",
    "                    if len(pdf_links) >= 5:\n",
    "                        break\n",
    "        else:\n",
    "            print(f'Failed to retrieve the page: {page_url}. Status code: {page_response.status_code}')\n",
    "\n",
    "        # Stop if 5 PDF links have already been found\n",
    "        if len(pdf_links) >= 5:\n",
    "            break\n",
    "\n",
    "    # Download and save each PDF\n",
    "    for i, pdf_url in enumerate(pdf_links):\n",
    "        time.sleep(2)  # Add delay to avoid hitting rate limits\n",
    "        pdf_response = requests.get(pdf_url, headers=headers)\n",
    "\n",
    "        if pdf_response.status_code == 200:\n",
    "            pdf_path = os.path.join(subdirectory, f'file_{i + 1}.pdf')\n",
    "            with open(pdf_path, 'wb') as pdf_file:\n",
    "                pdf_file.write(pdf_response.content)\n",
    "            print(f'Downloaded {pdf_path}')\n",
    "        else:\n",
    "            print(f'Failed to download {pdf_url}. Status code: {pdf_response.status_code}')\n",
    "else:\n",
    "    print(f'Failed to retrieve the base webpage. Status code: {response.status_code}')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b2df784-33d3-46b3-889f-05225e900cc7",
   "metadata": {},
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "\n",
    "\n",
    "# Headers to mimic a browser request\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Create the main directory for saving the PDFs\n",
    "main_directory = 'scraper'\n",
    "domain_name = base_url.split('//')[1].split('/')[0]  # Extract domain name from URL\n",
    "subdirectory = os.path.join(main_directory, domain_name)\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(subdirectory, exist_ok=True)\n",
    "\n",
    "# Send a GET request to the base URL\n",
    "response = requests.get(base_url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    pdf_links = []\n",
    "\n",
    "    # Find all anchor tags that direct to individual pages\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link['href']\n",
    "\n",
    "        # Filter out invalid links like 'javascript:void(0)'\n",
    "        if href.startswith('javascript:') or not urlparse(href).scheme in ['http', 'https']:\n",
    "            continue\n",
    "\n",
    "        page_url = urljoin(base_url, href)\n",
    "        \n",
    "        # Send a GET request to the individual page\n",
    "        page_response = requests.get(page_url, headers=headers)\n",
    "        time.sleep(2)  # Add delay to avoid rate limiting\n",
    "        \n",
    "        if page_response.status_code == 200:\n",
    "            page_soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "            \n",
    "            # Find PDF links on the individual page\n",
    "            for pdf_link in page_soup.find_all('a', href=True):\n",
    "                if pdf_link['href'].endswith('.pdf'):\n",
    "                    full_pdf_url = urljoin(page_url, pdf_link['href'])\n",
    "                    pdf_links.append(full_pdf_url)\n",
    "                    \n",
    "                    # Stop after finding 5 files\n",
    "                    if len(pdf_links) >= 5:\n",
    "                        break\n",
    "        else:\n",
    "            print(f'Failed to retrieve the page: {page_url}. Status code: {page_response.status_code}')\n",
    "\n",
    "        # Stop if 5 PDF links have already been found\n",
    "        if len(pdf_links) >= 5:\n",
    "            break\n",
    "\n",
    "    # Download and save each PDF\n",
    "    for i, pdf_url in enumerate(pdf_links):\n",
    "        time.sleep(2)  # Add delay to avoid hitting rate limits\n",
    "        pdf_response = requests.get(pdf_url, headers=headers)\n",
    "\n",
    "        if pdf_response.status_code == 200:\n",
    "            pdf_path = os.path.join(subdirectory, f'file_{i + 1}.pdf')\n",
    "            with open(pdf_path, 'wb') as pdf_file:\n",
    "                pdf_file.write(pdf_response.content)\n",
    "            print(f'Downloaded {pdf_path}')\n",
    "        else:\n",
    "            print(f'Failed to download {pdf_url}. Status code: {pdf_response.status_code}')\n",
    "else:\n",
    "    print(f'Failed to retrieve the base webpage. Status code: {response.status_code}')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6d849138-69fc-4033-8ebf-89625dd4b88e",
   "metadata": {},
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "\n",
    "# Headers to mimic a browser request\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Create the main directory for saving the PDFs\n",
    "main_directory = 'scraper'\n",
    "domain_name = base_url.split('//')[1].split('/')[0]  # Extract domain name from URL\n",
    "subdirectory = os.path.join(main_directory, domain_name)\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(subdirectory, exist_ok=True)\n",
    "\n",
    "# Helper function to make requests with retries and timeout\n",
    "def make_request(url, retries=3, timeout=10):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=timeout)\n",
    "            return response\n",
    "        except requests.exceptions.ConnectTimeout:\n",
    "            print(f\"Attempt {attempt + 1}: Connection timed out. Retrying...\")\n",
    "            time.sleep(2)  # Wait before retrying\n",
    "    return None  # Return None if all retries fail\n",
    "\n",
    "# Send a GET request to the base URL\n",
    "response = make_request(base_url)\n",
    "\n",
    "if response and response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    pdf_links = []\n",
    "\n",
    "    # Find all anchor tags that direct to individual pages\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link['href']\n",
    "\n",
    "        # Filter out invalid links like 'javascript:void(0)'\n",
    "        if href.startswith('javascript:') or not urlparse(href).scheme in ['http', 'https']:\n",
    "            continue\n",
    "\n",
    "        page_url = urljoin(base_url, href)\n",
    "        \n",
    "        # Send a GET request to the individual page\n",
    "        page_response = make_request(page_url)\n",
    "        time.sleep(2)  # Add delay to avoid rate limiting\n",
    "        \n",
    "        if page_response and page_response.status_code == 200:\n",
    "            page_soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "            \n",
    "            # Find PDF links on the individual page\n",
    "            for pdf_link in page_soup.find_all('a', href=True):\n",
    "                if pdf_link['href'].endswith('.pdf'):\n",
    "                    full_pdf_url = urljoin(page_url, pdf_link['href'])\n",
    "                    pdf_links.append(full_pdf_url)\n",
    "                    \n",
    "                    # Stop after finding 5 files\n",
    "                    if len(pdf_links) >= 5:\n",
    "                        break\n",
    "        else:\n",
    "            print(f'Failed to retrieve the page: {page_url}. Status code: {page_response.status_code if page_response else \"No response\"}')\n",
    "\n",
    "        # Stop if 5 PDF links have already been found\n",
    "        if len(pdf_links) >= 5:\n",
    "            break\n",
    "\n",
    "    # Download and save each PDF\n",
    "    for i, pdf_url in enumerate(pdf_links):\n",
    "        time.sleep(2)  # Add delay to avoid hitting rate limits\n",
    "        pdf_response = make_request(pdf_url)\n",
    "\n",
    "        if pdf_response and pdf_response.status_code == 200:\n",
    "            pdf_path = os.path.join(subdirectory, f'file_{i + 1}.pdf')\n",
    "            with open(pdf_path, 'wb') as pdf_file:\n",
    "                pdf_file.write(pdf_response.content)\n",
    "            print(f'Downloaded {pdf_path}')\n",
    "        else:\n",
    "            print(f'Failed to download {pdf_url}. Status code: {pdf_response.status_code if pdf_response else \"No response\"}')\n",
    "else:\n",
    "    print(f'Failed to retrieve the base webpage. Status code: {response.status_code if response else \"No response\"}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043091c7-0a1f-4b0a-9d39-6cad2c278738",
   "metadata": {},
   "source": [
    "Now, adjust the code to use the same filename as the one in the website. Also, to create a subfolder with the corresponding date and to only save files that have CICIMA or cicima in its pdf content.\n",
    "\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3acca66c-c203-4985-a0b6-84b89486300f",
   "metadata": {},
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from datetime import datetime\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "\n",
    "# Headers to mimic a browser request\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Create the main directory for saving the PDFs\n",
    "main_directory = 'scraper'\n",
    "domain_name = base_url.split('//')[1].split('/')[0]  # Extract domain name from URL\n",
    "date_folder = datetime.now().strftime('%Y-%m-%d')  # Current date as folder name\n",
    "subdirectory = os.path.join(main_directory, domain_name, date_folder)\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(subdirectory, exist_ok=True)\n",
    "\n",
    "# Helper function to make requests with retries and timeout\n",
    "def make_request(url, retries=1, timeout=2):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=timeout)\n",
    "            return response\n",
    "        except requests.exceptions.ConnectTimeout:\n",
    "            print(f\"Attempt {attempt + 1}: Connection timed out. Retrying...\")\n",
    "            time.sleep(2)  # Wait before retrying\n",
    "    return None  # Return None if all retries fail\n",
    "\n",
    "# Helper function to check if PDF contains 'CICIMA' or 'cicima'\n",
    "def contains_cicima(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as pdf_file:\n",
    "            reader = PdfReader(pdf_file)\n",
    "            for page in reader.pages:\n",
    "                text = page.extract_text() or ''\n",
    "                if 'CICIMA' in text or 'cicima' in text:\n",
    "                    return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "    return False\n",
    "\n",
    "# Send a GET request to the base URL\n",
    "response = make_request(base_url)\n",
    "\n",
    "if response and response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    pdf_links = []\n",
    "\n",
    "    # Find all anchor tags that direct to individual pages\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link['href']\n",
    "\n",
    "        # Filter out invalid links like 'javascript:void(0)'\n",
    "        if href.startswith('javascript:') or not urlparse(href).scheme in ['http', 'https']:\n",
    "            continue\n",
    "\n",
    "        page_url = urljoin(base_url, href)\n",
    "        \n",
    "        # Send a GET request to the individual page\n",
    "        page_response = make_request(page_url)\n",
    "        time.sleep(2)  # Add delay to avoid rate limiting\n",
    "        \n",
    "        if page_response and page_response.status_code == 200:\n",
    "            page_soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "            \n",
    "            # Find PDF links on the individual page\n",
    "            for pdf_link in page_soup.find_all('a', href=True):\n",
    "                if pdf_link['href'].endswith('.pdf'):\n",
    "                    full_pdf_url = urljoin(page_url, pdf_link['href'])\n",
    "                    pdf_links.append(full_pdf_url)\n",
    "                    \n",
    "                    # Stop after finding 5 files\n",
    "                    if len(pdf_links) >= 5:\n",
    "                        break\n",
    "        else:\n",
    "            print(f'Failed to retrieve the page: {page_url}. Status code: {page_response.status_code if page_response else \"No response\"}')\n",
    "\n",
    "        # Stop if 5 PDF links have already been found\n",
    "        if len(pdf_links) >= 5:\n",
    "            break\n",
    "\n",
    "    # Download and save each PDF\n",
    "    for pdf_url in pdf_links:\n",
    "        time.sleep(2)  # Add delay to avoid hitting rate limits\n",
    "        pdf_response = make_request(pdf_url)\n",
    "\n",
    "        if pdf_response and pdf_response.status_code == 200:\n",
    "            filename = os.path.basename(pdf_url.split('?')[0])  # Extract filename from URL\n",
    "            pdf_path = os.path.join(subdirectory, filename)\n",
    "            \n",
    "            with open(pdf_path, 'wb') as pdf_file:\n",
    "                pdf_file.write(pdf_response.content)\n",
    "            \n",
    "            # Check if the PDF contains 'CICIMA' or 'cicima'\n",
    "            if contains_cicima(pdf_path):\n",
    "                print(f'Downloaded and saved {pdf_path}')\n",
    "            else:\n",
    "                os.remove(pdf_path)  # Delete the file if it doesn't contain the keyword\n",
    "                print(f'Deleted {filename} as it did not contain \"CICIMA\" or \"cicima\"')\n",
    "        else:\n",
    "            print(f'Failed to download {pdf_url}. Status code: {pdf_response.status_code if pdf_response else \"No response\"}')\n",
    "else:\n",
    "    print(f'Failed to retrieve the base webpage. Status code: {response.status_code if response else \"No response\"}')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f576dc9f-873f-4ee3-ab3d-e34d72740411",
   "metadata": {},
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "def search_engine_query(query, num_results=10):\n",
    "    \"\"\"\n",
    "    Queries an open search engine (e.g., DuckDuckGo) and returns a list of result URLs.\n",
    "    \n",
    "    Args:\n",
    "    - query (str): The search query string.\n",
    "    - num_results (int): Number of result URLs to retrieve.\n",
    "    \n",
    "    Returns:\n",
    "    - List of result URLs.\n",
    "    \"\"\"\n",
    "    base_url = 'https://duckduckgo.com/html/'\n",
    "    params = {'q': query}\n",
    "    query_url = f\"{base_url}?{urlencode(params)}\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "\n",
    "    response = requests.get(query_url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        result_links = []\n",
    "        \n",
    "        # Find all links in search results\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if '/l/?kh=-1&uddg=' in href:\n",
    "                continue  # Skip unnecessary internal DuckDuckGo redirect links\n",
    "\n",
    "            # Ensure the link is an external, valid result link\n",
    "            if not href.startswith('http'):\n",
    "                continue\n",
    "\n",
    "            result_links.append(href)\n",
    "            \n",
    "            # Stop if we have enough results\n",
    "            if len(result_links) >= num_results:\n",
    "                break\n",
    "        \n",
    "        return result_links\n",
    "    else:\n",
    "        print(f\"Failed to retrieve search results. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "# Example usage\n",
    "query = \"CICIMA research papers\"\n",
    "urls = search_engine_query(query, num_results=5)\n",
    "print(\"Search Result URLs:\")\n",
    "for url in urls:\n",
    "    print(url)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a8fc90c7-eb26-4ce1-9a15-fa300c68c8a2",
   "metadata": {},
   "source": [
    "def scrape_url(base_url):\n",
    "    import os\n",
    "    import time\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    from urllib.parse import urljoin, urlparse\n",
    "    from datetime import datetime\n",
    "    from PyPDF2 import PdfReader\n",
    "    \n",
    "    \n",
    "    # Headers to mimic a browser request\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "    \n",
    "    # Create the main directory for saving the PDFs\n",
    "    main_directory = 'scraper'\n",
    "    domain_name = base_url.split('//')[1].split('/')[0]  # Extract domain name from URL\n",
    "    date_folder = datetime.now().strftime('%Y-%m-%d')  # Current date as folder name\n",
    "    subdirectory = os.path.join(main_directory, domain_name, date_folder)\n",
    "    \n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(subdirectory, exist_ok=True)\n",
    "    \n",
    "    # Helper function to make requests with retries and timeout\n",
    "    def make_request(url, retries=1, timeout=2):\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                response = requests.get(url, headers=headers, timeout=timeout)\n",
    "                return response\n",
    "            except requests.exceptions.ConnectTimeout:\n",
    "                print(f\"Attempt {attempt + 1}: Connection timed out. Retrying...\")\n",
    "                time.sleep(2)  # Wait before retrying\n",
    "        return None  # Return None if all retries fail\n",
    "    \n",
    "    # Helper function to check if PDF contains 'CICIMA' or 'cicima'\n",
    "    def contains_cicima(file_path):\n",
    "        try:\n",
    "            with open(file_path, 'rb') as pdf_file:\n",
    "                reader = PdfReader(pdf_file)\n",
    "                for page in reader.pages:\n",
    "                    text = page.extract_text() or ''\n",
    "                    if 'CICIMA' in text or 'cicima' in text:\n",
    "                        return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Send a GET request to the base URL\n",
    "    response = make_request(base_url)\n",
    "    \n",
    "    if response and response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        pdf_links = []\n",
    "    \n",
    "        # Find all anchor tags that direct to individual pages\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "    \n",
    "            # Filter out invalid links like 'javascript:void(0)'\n",
    "            if href.startswith('javascript:') or not urlparse(href).scheme in ['http', 'https']:\n",
    "                continue\n",
    "    \n",
    "            page_url = urljoin(base_url, href)\n",
    "            \n",
    "            # Send a GET request to the individual page\n",
    "            page_response = make_request(page_url)\n",
    "            time.sleep(2)  # Add delay to avoid rate limiting\n",
    "            \n",
    "            if page_response and page_response.status_code == 200:\n",
    "                page_soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "                \n",
    "                # Find PDF links on the individual page\n",
    "                for pdf_link in page_soup.find_all('a', href=True):\n",
    "                    if pdf_link['href'].endswith('.pdf'):\n",
    "                        full_pdf_url = urljoin(page_url, pdf_link['href'])\n",
    "                        pdf_links.append(full_pdf_url)\n",
    "                        \n",
    "                        # Stop after finding 5 files\n",
    "                        if len(pdf_links) >= 5:\n",
    "                            break\n",
    "            else:\n",
    "                print(f'Failed to retrieve the page: {page_url}. Status code: {page_response.status_code if page_response else \"No response\"}')\n",
    "    \n",
    "            # Stop if 5 PDF links have already been found\n",
    "            if len(pdf_links) >= 5:\n",
    "                break\n",
    "    \n",
    "        # Download and save each PDF\n",
    "        for pdf_url in pdf_links:\n",
    "            time.sleep(2)  # Add delay to avoid hitting rate limits\n",
    "            pdf_response = make_request(pdf_url)\n",
    "    \n",
    "            if pdf_response and pdf_response.status_code == 200:\n",
    "                filename = os.path.basename(pdf_url.split('?')[0])  # Extract filename from URL\n",
    "                pdf_path = os.path.join(subdirectory, filename)\n",
    "                \n",
    "                with open(pdf_path, 'wb') as pdf_file:\n",
    "                    pdf_file.write(pdf_response.content)\n",
    "                \n",
    "                # Check if the PDF contains 'CICIMA' or 'cicima'\n",
    "                if contains_cicima(pdf_path):\n",
    "                    print(f'Downloaded and saved {pdf_path}')\n",
    "                else:\n",
    "                    os.remove(pdf_path)  # Delete the file if it doesn't contain the keyword\n",
    "                    print(f'Deleted {filename} as it did not contain \"CICIMA\" or \"cicima\"')\n",
    "            else:\n",
    "                print(f'Failed to download {pdf_url}. Status code: {pdf_response.status_code if pdf_response else \"No response\"}')\n",
    "    else:\n",
    "        print(f'Failed to retrieve the base webpage. Status code: {response.status_code if response else \"No response\"}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6ceb8631-a799-45f6-b740-3a3b6d788eef",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from datetime import datetime\n",
    "from PyPDF2 import PdfReader\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "def search_engine_query(query, num_results=10, timeout=10):\n",
    "    print(\"Ok search_engine_query \")\n",
    "    \"\"\"\n",
    "    Queries an open search engine (e.g., DuckDuckGo) and returns a list of result URLs.\n",
    "    \n",
    "    Args:\n",
    "    - query (str): The search query string.\n",
    "    - num_results (int): Number of result URLs to retrieve.\n",
    "    - timeout (int): The timeout for the request in seconds.\n",
    "    \n",
    "    Returns:\n",
    "    - List of result URLs.\n",
    "    \"\"\"\n",
    "    base_url = 'https://duckduckgo.com/html/'\n",
    "    params = {'q': query}\n",
    "    query_url = f\"{base_url}?{urlencode(params)}\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(query_url, headers=headers, timeout=timeout)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            result_links = []\n",
    "            \n",
    "            # Find all links in search results\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = link['href']\n",
    "                if '/l/?kh=-1&uddg=' in href:\n",
    "                    continue  # Skip unnecessary internal DuckDuckGo redirect links\n",
    "\n",
    "                # Ensure the link is an external, valid result link\n",
    "                if not href.startswith('http'):\n",
    "                    continue\n",
    "\n",
    "                result_links.append(href)\n",
    "                \n",
    "                # Stop if we have enough results\n",
    "                if len(result_links) >= num_results:\n",
    "                    break\n",
    "            \n",
    "            return result_links\n",
    "        else:\n",
    "            print(f\"Failed to retrieve search results. Status code: {response.status_code}\")\n",
    "            return []\n",
    "    \n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"The request timed out.\")\n",
    "        return []\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "# Example usage\n",
    "query = \"CICIMA research papers\"\n",
    "urls = search_engine_query(query, num_results=5)\n",
    "print(\"Search Result URLs:\")\n",
    "for url in urls:\n",
    "    print(url)\n",
    "    \n",
    "\n",
    "target_strings = [\"ucr\", \"UCR\"]\n",
    "def scrape_url(base_url):\n",
    "    \"\"\"\n",
    "    Scrapes a given URL, downloads PDF files containing 'CICIMA' in their content,\n",
    "    and saves them to a directory structure.\n",
    "    \"\"\"\n",
    "    # Headers to mimic a browser request\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "    \n",
    "    # Create the main directory for saving the PDFs\n",
    "    main_directory = 'scraper'\n",
    "    domain_name = urlparse(base_url).netloc  # Extract domain name from URL\n",
    "    date_folder = datetime.now().strftime('%Y-%m-%d')  # Current date as folder name\n",
    "    subdirectory = os.path.join(main_directory, domain_name, date_folder)\n",
    "    \n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(subdirectory, exist_ok=True)\n",
    "    \n",
    "    # Helper function to make requests with retries and timeout\n",
    "    def make_request(url, retries=1, timeout=2):\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                response = requests.get(url, headers=headers, timeout=timeout)\n",
    "                response.raise_for_status()  # Raise an HTTPError for bad responses (4xx and 5xx)\n",
    "                return response\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Attempt {attempt + 1}: Error occurred: {e}. Retrying...\")\n",
    "                time.sleep(2)  # Wait before retrying\n",
    "        return None  # Return None if all retries fail\n",
    "    \n",
    "    # Helper function to check if PDF contains 'CICIMA' or 'cicima'\n",
    "    def contains_strings(file_path, strings_to_check):\n",
    "        \"\"\"\n",
    "        Checks if any of the strings in `strings_to_check` are present in the PDF content.\n",
    "    \n",
    "        Args:\n",
    "        - file_path (str): Path to the PDF file.\n",
    "        - strings_to_check (list of str): List of strings to search for in the PDF.\n",
    "    \n",
    "        Returns:\n",
    "        - bool: True if any of the strings are found, False otherwise.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'rb') as pdf_file:\n",
    "                reader = PdfReader(pdf_file)\n",
    "                for page in reader.pages:\n",
    "                    text = page.extract_text() or ''\n",
    "                    for search_string in strings_to_check:\n",
    "                        if search_string in text:\n",
    "                            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "    \n",
    "    # Send a GET request to the base URL\n",
    "    response = make_request(base_url)\n",
    "    \n",
    "    if response:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        pdf_links = []\n",
    "    \n",
    "        # Find all anchor tags that direct to individual pages\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "    \n",
    "            # Filter out invalid links like 'javascript:void(0)'\n",
    "            if href.startswith('javascript:') or not urlparse(href).scheme in ['http', 'https']:\n",
    "                continue\n",
    "    \n",
    "            page_url = urljoin(base_url, href)\n",
    "            \n",
    "            # Send a GET request to the individual page\n",
    "            page_response = make_request(page_url)\n",
    "            time.sleep(2)  # Add delay to avoid rate limiting\n",
    "            \n",
    "            if page_response:\n",
    "                page_soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "                \n",
    "                # Find PDF links on the individual page\n",
    "                for pdf_link in page_soup.find_all('a', href=True):\n",
    "                    if pdf_link['href'].endswith('.pdf'):\n",
    "                        full_pdf_url = urljoin(page_url, pdf_link['href'])\n",
    "                        pdf_links.append(full_pdf_url)\n",
    "                        \n",
    "                        # Stop after finding 5 files\n",
    "                        if len(pdf_links) >= 5:\n",
    "                            break\n",
    "            else:\n",
    "                print(f'Failed to retrieve the page: {page_url}')\n",
    "    \n",
    "            # Stop if 5 PDF links have already been found\n",
    "            if len(pdf_links) >= 5:\n",
    "                break\n",
    "    \n",
    "        # Download and save each PDF\n",
    "        for pdf_url in pdf_links:\n",
    "            time.sleep(2)  # Add delay to avoid hitting rate limits\n",
    "            pdf_response = make_request(pdf_url)\n",
    "    \n",
    "            if pdf_response:\n",
    "                filename = os.path.basename(pdf_url.split('?')[0])  # Extract filename from URL\n",
    "                pdf_path = os.path.join(subdirectory, filename)\n",
    "                \n",
    "                with open(pdf_path, 'wb') as pdf_file:\n",
    "                    pdf_file.write(pdf_response.content)\n",
    "                \n",
    "                # Check if the PDF contains 'CICIMA' or 'cicima'\n",
    "                if contains_strings(pdf_path, target_strings):\n",
    "                    print(f'Downloaded and saved {pdf_path}')\n",
    "                else:\n",
    "                    os.remove(pdf_path)  # Delete the file if it doesn't contain the keyword\n",
    "                    print(f'Deleted {filename} as it did not contain \"CICIMA\" or \"cicima\"')\n",
    "            else:\n",
    "                print(f'Failed to download {pdf_url}')\n",
    "    else:\n",
    "        print(f'Failed to retrieve the base webpage: {base_url}')\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Ok 1\")\n",
    "    queries = [\"academic paper materials science ucr\", \"materials science university of costa rica papers\", \"materials engineering UCR\"]\n",
    "    for query in queries:\n",
    "        print(\"Ok loop 2\")\n",
    "        urls = search_engine_query(query, num_results=10)\n",
    "        print(urls)\n",
    "        for url in urls:\n",
    "            scrape_url(url)\n",
    "main()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d4a46be-3560-4e10-80e0-ca497cf85e0b",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from datetime import datetime\n",
    "from PyPDF2 import PdfReader\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "# Configura los proxies de Tor\n",
    "TOR_PROXY = {\n",
    "    \"http\": \"socks5h://127.0.0.1:9050\",\n",
    "    \"https\": \"socks5h://127.0.0.1:9050\"\n",
    "}\n",
    "\n",
    "def search_engine_query(query, num_results=10, timeout=10):\n",
    "    print(\"Using Tor for search_engine_query\")\n",
    "    base_url = 'https://duckduckgo.com/html/'\n",
    "    params = {'q': query}\n",
    "    query_url = f\"{base_url}?{urlencode(params)}\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(query_url, headers=headers, proxies=TOR_PROXY, timeout=timeout)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            result_links = []\n",
    "            \n",
    "            # Find all links in search results\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = link['href']\n",
    "                if '/l/?kh=-1&uddg=' in href:\n",
    "                    continue  # Skip unnecessary internal DuckDuckGo redirect links\n",
    "\n",
    "                # Ensure the link is an external, valid result link\n",
    "                if not href.startswith('http'):\n",
    "                    continue\n",
    "\n",
    "                result_links.append(href)\n",
    "                \n",
    "                # Stop if we have enough results\n",
    "                if len(result_links) >= num_results:\n",
    "                    break\n",
    "            \n",
    "            return result_links\n",
    "        else:\n",
    "            print(f\"Failed to retrieve search results. Status code: {response.status_code}\")\n",
    "            return []\n",
    "    \n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"The request timed out.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "def scrape_url(base_url):\n",
    "    print(f\"Scraping {base_url} using Tor...\")\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "\n",
    "    def make_request(url, retries=3, timeout=10):\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                response = requests.get(url, headers=headers, proxies=TOR_PROXY, timeout=timeout)\n",
    "                response.raise_for_status()\n",
    "                return response\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Attempt {attempt + 1}: Error occurred: {e}. Retrying...\")\n",
    "                time.sleep(2)\n",
    "        return None\n",
    "\n",
    "    # Similar logic as in your original scrape_url function\n",
    "    # Uses make_request for all HTTP requests via Tor\n",
    "    response = make_request(base_url)\n",
    "    if response:\n",
    "        print(f\"Successfully accessed {base_url} through Tor!\")\n",
    "    else:\n",
    "        print(f\"Failed to access {base_url}.\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"CICIMA research papers\"\n",
    "    urls = search_engine_query(query, num_results=5)\n",
    "    for url in urls:\n",
    "        scrape_url(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02c6eddc-61bd-42f1-9bb8-b8650a2442eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled Queries:\n",
      "UCR research on materials engineering field\n",
      "university of costa rica research materials engineering\n",
      "materials science academic papers Costa Rica\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 266\u001b[0m\n\u001b[0;32m    263\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m url \u001b[38;5;129;01min\u001b[39;00m urls:\n\u001b[0;32m    264\u001b[0m             scrape_url(url)\n\u001b[1;32m--> 266\u001b[0m main()\n",
      "Cell \u001b[1;32mIn[15], line 259\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    257\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m    258\u001b[0m     queries \u001b[38;5;241m=\u001b[39m sample_queries(queries_dict, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m--> 259\u001b[0m     modified_queries \u001b[38;5;241m=\u001b[39m modify_queries(queries, num_changes\u001b[38;5;241m=\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m modified_queries:\n\u001b[0;32m    261\u001b[0m         urls \u001b[38;5;241m=\u001b[39m search_engine_query(query, num_results\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 232\u001b[0m, in \u001b[0;36mmodify_queries\u001b[1;34m(queries, num_changes)\u001b[0m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodify_queries\u001b[39m(queries, num_changes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m    231\u001b[0m     modified_queries \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 232\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m query \u001b[38;5;129;01min\u001b[39;00m queries:\n\u001b[0;32m    233\u001b[0m         modified_query \u001b[38;5;241m=\u001b[39m mutate_query(query, num_changes)\n\u001b[0;32m    234\u001b[0m         modified_queries\u001b[38;5;241m.\u001b[39mappend(modified_query)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse, urlencode\n",
    "from datetime import datetime\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "from stem.control import Controller\n",
    "\n",
    "def get_current_ip():\n",
    "    try:\n",
    "        response = requests.get('https://api.ipify.org', proxies=TOR_PROXY, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "        else:\n",
    "            return \"Failed to retrieve IP\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def change_tor_ip():\n",
    "    with Controller.from_port(port=9051) as controller:\n",
    "        controller.authenticate()  # Automatically uses cookie authentication\n",
    "        controller.signal('NEWNYM')\n",
    "        print(\"Tor IP address changed.\")\n",
    "        new_ip = get_current_ip()\n",
    "        print(f\"New IP address: {new_ip}\")\n",
    "        \n",
    "# TOR_PROXY = {\n",
    "#     \"http\": \"socks5h://127.0.0.1:9050\",\n",
    "#     \"https\": \"socks5h://127.0.0.1:9050\"\n",
    "# }\n",
    "\n",
    "TOR_PROXY = {\n",
    "    \"http\": \"socks5h://127.0.0.1:9150\",\n",
    "    \"https\": \"socks5h://127.0.0.1:9150\"\n",
    "}\n",
    "\n",
    "import random\n",
    "\n",
    "def random_headers():\n",
    "    user_agents = [\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/109.0\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; WOW64; rv:45.0) Gecko/20100101 Firefox/45.0\",\n",
    "        \"Mozilla/5.0 (iPhone; CPU iPhone OS 15_2_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.2 Mobile/15E148 Safari/604.1\"\n",
    "    ]\n",
    "    \n",
    "    accept_languages = [\n",
    "        \"en-US,en;q=0.9\",\n",
    "        \"en-GB,en;q=0.8\",\n",
    "        \"es-ES,es;q=0.7,en;q=0.5\",\n",
    "        \"de-DE,de;q=0.9,en;q=0.8\",\n",
    "        \"fr-FR,fr;q=0.9,en;q=0.8\"\n",
    "    ]\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": random.choice(user_agents),\n",
    "        \"Accept-Language\": random.choice(accept_languages),\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "        \"Referer\": random.choice([\n",
    "            \"https://www.google.com/\",\n",
    "            \"https://www.bing.com/\",\n",
    "            \"https://www.yahoo.com/\",\n",
    "            \"https://duckduckgo.com/\"\n",
    "        ]),\n",
    "        \"Connection\": \"keep-alive\"\n",
    "    }\n",
    "    return headers\n",
    "\n",
    "def search_engine_query(query, num_results=10, timeout=10):\n",
    "    change_tor_ip()\n",
    "    time.sleep(2)\n",
    "    #base_url = 'https://duckduckgo.com/html/'\n",
    "    base_url = \"https://duckduckgogg42xjoc72x3sjasowoarfbgcmvfimaftt6twagswzczad.onion\"\n",
    "    params = {'q': query}\n",
    "    query_url = f\"{base_url}?{urlencode(params)}\"\n",
    "    headers = random_headers()\n",
    "    response = requests.get(query_url, headers=headers, proxies=TOR_PROXY, timeout=timeout)\n",
    "    print(response.status_code)  # Debugging step\n",
    "    print(response.text)  # Debugging step\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        result_links = []\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if '/l/?kh=-1&uddg=' in href or not href.startswith('http'):\n",
    "                continue\n",
    "            result_links.append(href)\n",
    "            if len(result_links) >= num_results:\n",
    "                break\n",
    "        return result_links\n",
    "    return []\n",
    "\n",
    "target_strings = [\"ucr\", \"UCR\"]\n",
    "\n",
    "def scrape_url(base_url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    main_directory = 'scraper'\n",
    "    domain_name = urlparse(base_url).netloc\n",
    "    date_folder = datetime.now().strftime('%Y-%m-%d')\n",
    "    subdirectory = os.path.join(main_directory, domain_name, date_folder)\n",
    "    os.makedirs(subdirectory, exist_ok=True)\n",
    "\n",
    "    def make_request(url, retries=3, timeout=10):\n",
    "        for _ in range(retries):\n",
    "            try:\n",
    "                response = requests.get(url, headers=headers, proxies=TOR_PROXY, timeout=timeout)\n",
    "                response.raise_for_status()\n",
    "                return response\n",
    "            except:\n",
    "                time.sleep(10)\n",
    "        return None\n",
    "\n",
    "    def contains_strings(file_path, strings_to_check):\n",
    "        try:\n",
    "            with open(file_path, 'rb') as pdf_file:\n",
    "                reader = PdfReader(pdf_file)\n",
    "                for page in reader.pages:\n",
    "                    text = page.extract_text() or ''\n",
    "                    if any(search_string in text for search_string in strings_to_check):\n",
    "                        return True\n",
    "        except:\n",
    "            pass\n",
    "        return False\n",
    "\n",
    "    response = make_request(base_url)\n",
    "    if response:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        pdf_links = []\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if href.startswith('javascript:') or not urlparse(href).scheme in ['http', 'https']:\n",
    "                continue\n",
    "            page_url = urljoin(base_url, href)\n",
    "            page_response = make_request(page_url)\n",
    "            if page_response:\n",
    "                page_soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "                for pdf_link in page_soup.find_all('a', href=True):\n",
    "                    if pdf_link['href'].endswith('.pdf'):\n",
    "                        full_pdf_url = urljoin(page_url, pdf_link['href'])\n",
    "                        pdf_links.append(full_pdf_url)\n",
    "                        if len(pdf_links) >= 5:\n",
    "                            break\n",
    "            if len(pdf_links) >= 5:\n",
    "                break\n",
    "\n",
    "        for pdf_url in pdf_links:\n",
    "            pdf_response = make_request(pdf_url)\n",
    "            if pdf_response:\n",
    "                filename = os.path.basename(pdf_url.split('?')[0])\n",
    "                pdf_path = os.path.join(subdirectory, filename)\n",
    "                with open(pdf_path, 'wb') as pdf_file:\n",
    "                    pdf_file.write(pdf_response.content)\n",
    "                if contains_strings(pdf_path, target_strings):\n",
    "                    print(f'Downloaded and saved {pdf_path}')\n",
    "                else:\n",
    "                    os.remove(pdf_path)\n",
    "            else:\n",
    "                print(f'Failed to download {pdf_url}')\n",
    "queries_dict = {\n",
    "    1: \"research articles in materials science UCR\",\n",
    "    2: \"papers on materials science university of costa rica\",\n",
    "    3: \"materials engineering research at UCR\",\n",
    "    4: \"university of costa rica materials science publications\",\n",
    "    5: \"UCR academic papers in materials engineering\",\n",
    "    6: \"Costa Rica university materials science research\",\n",
    "    7: \"UCR research on materials engineering field\",\n",
    "    8: \"materials science academic papers Costa Rica\",\n",
    "    9: \"university of costa rica research materials engineering\",\n",
    "    10: \"research papers UCR materials science department\"\n",
    "}\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "# A dictionary that maps each letter to its neighboring keys on a QWERTY keyboard\n",
    "keyboard_neighbors = {\n",
    "    'a': ['q', 'w', 's', 'z', 'x'],\n",
    "    'b': ['v', 'g', 'n'],\n",
    "    'c': ['x', 'f', 'v', 'd'],\n",
    "    'd': ['s', 'e', 'r', 'f', 'c'],\n",
    "    'e': ['w', 'r', 'd', 's'],\n",
    "    'f': ['r', 't', 'g', 'v', 'd'],\n",
    "    'g': ['t', 'h', 'f', 'b', 'v'],\n",
    "    'h': ['y', 'j', 'g', 'n', 'b'],\n",
    "    'i': ['u', 'o', 'k'],\n",
    "    'j': ['u', 'k', 'h'],\n",
    "    'k': ['i', 'j', 'l', 'n'],\n",
    "    'l': ['o', 'k'],\n",
    "    'm': ['n', 'j'],\n",
    "    'n': ['b', 'h', 'm'],\n",
    "    'o': ['i', 'p', 'l'],\n",
    "    'p': ['o', 'l'],\n",
    "    'q': ['w', 'a'],\n",
    "    'r': ['e', 't', 'f', 'd'],\n",
    "    's': ['a', 'w', 'e', 'd', 'z'],\n",
    "    't': ['r', 'y', 'f', 'g'],\n",
    "    'u': ['y', 'i', 'j'],\n",
    "    'v': ['c', 'f', 'g', 'b'],\n",
    "    'w': ['q', 'e', 's', 'a'],\n",
    "    'x': ['z', 'c', 'd', 's'],\n",
    "    'y': ['t', 'u', 'h'],\n",
    "    'z': ['a', 'x']\n",
    "}\n",
    "\n",
    "def mutate_query(query, num_changes=2):\n",
    "    query_list = list(query)  # Convert query to a list of characters\n",
    "    for _ in range(num_changes):\n",
    "        # Choose a random position in the query\n",
    "        idx = random.randint(0, len(query_list) - 1)\n",
    "        char = query_list[idx].lower()  # Get the character at that position\n",
    "        \n",
    "        if char in keyboard_neighbors:\n",
    "            # Pick a random neighboring letter\n",
    "            new_char = random.choice(keyboard_neighbors[char])\n",
    "            \n",
    "            # Randomly decide whether to keep the case or swap it\n",
    "            if random.random() > 0.5:\n",
    "                new_char = new_char.upper() if query_list[idx].isupper() else new_char\n",
    "            \n",
    "            query_list[idx] = new_char\n",
    "    \n",
    "    # Join the list back into a string and return\n",
    "    return ''.join(query_list)\n",
    "\n",
    "def modify_queries(queries, num_changes=2):\n",
    "    modified_queries = []\n",
    "    for query in queries:\n",
    "        modified_query = mutate_query(query, num_changes)\n",
    "        modified_queries.append(modified_query)\n",
    "    return modified_queries\n",
    "\n",
    "# Example usage\n",
    "queries = [\n",
    "    \"research articles in materials science UCR\",\n",
    "    \"papers on materials science university of costa rica\",\n",
    "    \"materials engineering research at UCR\",\n",
    "    \"university of costa rica materials science publications\",\n",
    "    \"UCR academic papers in materials engineering\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "def sample_queries(queries_dict, num_samples=3):\n",
    "    # Select a random sample of queries (num_samples) from the queries_dict\n",
    "    sample = []\n",
    "    while not sample:\n",
    "        sample = random.sample(list(queries_dict.values()), num_samples)\n",
    "    return sample\n",
    "\n",
    "def main():\n",
    "    queries = sample_queries(queries_dict, num_samples=3)\n",
    "    modified_queries = modify_queries(queries, num_changes=random.randint(0, 3))\n",
    "    for query in modified_queries:\n",
    "        urls = search_engine_query(query, num_results=10)\n",
    "        print(f\"{urls=}\")\n",
    "        for url in urls:\n",
    "            scrape_url(url)\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dc1f61f-8027-46e6-bd37-9659191618d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Tor control port successfully!\n"
     ]
    }
   ],
   "source": [
    "from stem.control import Controller\n",
    "\n",
    "try:\n",
    "    with Controller.from_port(port=9051) as controller:\n",
    "        controller.authenticate()\n",
    "        print(\"Connected to Tor control port successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to Tor control port: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492eb0d0-9443-4dc5-be06-44299a36408f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
