{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51927743-a89d-48bc-870f-a4e70f8eb100",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = 'https://scholar.google.es/citations?user=n-YN5poAAAAJ&hl=es'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cf91b7-b323-433f-906d-ea2f90800874",
   "metadata": {},
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# URL of the website to scrape PDFs from\n",
    "url = 'https://scholar.google.com/citations?user=Y8sFm4oAAAAJ&hl=en'\n",
    "\n",
    "# Create the main directory for saving the PDFs\n",
    "main_directory = 'scraper'\n",
    "domain_name = url.split('//')[1].split('/')[0]  # Extract domain name from URL\n",
    "subdirectory = os.path.join(main_directory, domain_name)\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(subdirectory, exist_ok=True)\n",
    "\n",
    "# Send a GET request to the webpage\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    pdf_links = []\n",
    "    \n",
    "    # Find all anchor tags with href ending in .pdf\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        if link['href'].endswith('.pdf'):\n",
    "            full_url = urljoin(url, link['href'])\n",
    "            pdf_links.append(full_url)\n",
    "        \n",
    "        # Stop after finding 5 files\n",
    "        if len(pdf_links) >= 5:\n",
    "            break\n",
    "\n",
    "    # Download and save each PDF\n",
    "    for i, pdf_url in enumerate(pdf_links):\n",
    "        pdf_response = requests.get(pdf_url)\n",
    "        \n",
    "        if pdf_response.status_code == 200:\n",
    "            pdf_path = os.path.join(subdirectory, f'file_{i + 1}.pdf')\n",
    "            with open(pdf_path, 'wb') as pdf_file:\n",
    "                pdf_file.write(pdf_response.content)\n",
    "            print(f'Downloaded {pdf_path}')\n",
    "        else:\n",
    "            print(f'Failed to download {pdf_url}')\n",
    "else:\n",
    "    print(f'Failed to retrieve the webpage. Status code: {response.status_code}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0854a41e-6741-450c-a711-8906a4b204fb",
   "metadata": {},
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# URL of the website to scrape PDFs from\n",
    "url = 'https://scholar.google.com/citations?user=Y8sFm4oAAAAJ&hl=en'\n",
    "\n",
    "# Headers to mimic a browser request\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Create the main directory for saving the PDFs\n",
    "main_directory = 'scraper'\n",
    "domain_name = url.split('//')[1].split('/')[0]  # Extract domain name from URL\n",
    "subdirectory = os.path.join(main_directory, domain_name)\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(subdirectory, exist_ok=True)\n",
    "\n",
    "# Send a GET request to the webpage\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    pdf_links = []\n",
    "\n",
    "    # Find all anchor tags with href ending in .pdf\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        if link['href'].endswith('.pdf'):\n",
    "            full_url = urljoin(url, link['href'])\n",
    "            pdf_links.append(full_url)\n",
    "\n",
    "        # Stop after finding 5 files\n",
    "        if len(pdf_links) >= 5:\n",
    "            break\n",
    "\n",
    "    # Download and save each PDF\n",
    "    for i, pdf_url in enumerate(pdf_links):\n",
    "        time.sleep(2)  # Add delay to avoid hitting rate limits\n",
    "        pdf_response = requests.get(pdf_url, headers=headers)\n",
    "\n",
    "        if pdf_response.status_code == 200:\n",
    "            pdf_path = os.path.join(subdirectory, f'file_{i + 1}.pdf')\n",
    "            with open(pdf_path, 'wb') as pdf_file:\n",
    "                pdf_file.write(pdf_response.content)\n",
    "            print(f'Downloaded {pdf_path}')\n",
    "        else:\n",
    "            print(f'Failed to download {pdf_url}. Status code: {pdf_response.status_code}')\n",
    "else:\n",
    "    print(f'Failed to retrieve the webpage. Status code: {response.status_code}')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "84db32a8-4789-40e0-8605-a2b2a8f4b926",
   "metadata": {},
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# Base URL of the website to scrape\n",
    "base_url = 'https://scholar.google.com/citations?user=Y8sFm4oAAAAJ&hl=en'\n",
    "\n",
    "# Headers to mimic a browser request\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Create the main directory for saving the PDFs\n",
    "main_directory = 'scraper'\n",
    "domain_name = base_url.split('//')[1].split('/')[0]  # Extract domain name from URL\n",
    "subdirectory = os.path.join(main_directory, domain_name)\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(subdirectory, exist_ok=True)\n",
    "\n",
    "# Send a GET request to the base URL\n",
    "response = requests.get(base_url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    pdf_links = []\n",
    "\n",
    "    # Find all anchor tags that direct to individual pages\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        page_url = urljoin(base_url, link['href'])\n",
    "        \n",
    "        # Send a GET request to the individual page\n",
    "        page_response = requests.get(page_url, headers=headers)\n",
    "        time.sleep(2)  # Add delay to avoid rate limiting\n",
    "        \n",
    "        if page_response.status_code == 200:\n",
    "            page_soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "            \n",
    "            # Find PDF links on the individual page\n",
    "            for pdf_link in page_soup.find_all('a', href=True):\n",
    "                if pdf_link['href'].endswith('.pdf'):\n",
    "                    full_pdf_url = urljoin(page_url, pdf_link['href'])\n",
    "                    pdf_links.append(full_pdf_url)\n",
    "                    \n",
    "                    # Stop after finding 5 files\n",
    "                    if len(pdf_links) >= 5:\n",
    "                        break\n",
    "        else:\n",
    "            print(f'Failed to retrieve the page: {page_url}. Status code: {page_response.status_code}')\n",
    "\n",
    "        # Stop if 5 PDF links have already been found\n",
    "        if len(pdf_links) >= 5:\n",
    "            break\n",
    "\n",
    "    # Download and save each PDF\n",
    "    for i, pdf_url in enumerate(pdf_links):\n",
    "        time.sleep(2)  # Add delay to avoid hitting rate limits\n",
    "        pdf_response = requests.get(pdf_url, headers=headers)\n",
    "\n",
    "        if pdf_response.status_code == 200:\n",
    "            pdf_path = os.path.join(subdirectory, f'file_{i + 1}.pdf')\n",
    "            with open(pdf_path, 'wb') as pdf_file:\n",
    "                pdf_file.write(pdf_response.content)\n",
    "            print(f'Downloaded {pdf_path}')\n",
    "        else:\n",
    "            print(f'Failed to download {pdf_url}. Status code: {pdf_response.status_code}')\n",
    "else:\n",
    "    print(f'Failed to retrieve the base webpage. Status code: {response.status_code}')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b2df784-33d3-46b3-889f-05225e900cc7",
   "metadata": {},
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "\n",
    "\n",
    "# Headers to mimic a browser request\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Create the main directory for saving the PDFs\n",
    "main_directory = 'scraper'\n",
    "domain_name = base_url.split('//')[1].split('/')[0]  # Extract domain name from URL\n",
    "subdirectory = os.path.join(main_directory, domain_name)\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(subdirectory, exist_ok=True)\n",
    "\n",
    "# Send a GET request to the base URL\n",
    "response = requests.get(base_url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    pdf_links = []\n",
    "\n",
    "    # Find all anchor tags that direct to individual pages\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link['href']\n",
    "\n",
    "        # Filter out invalid links like 'javascript:void(0)'\n",
    "        if href.startswith('javascript:') or not urlparse(href).scheme in ['http', 'https']:\n",
    "            continue\n",
    "\n",
    "        page_url = urljoin(base_url, href)\n",
    "        \n",
    "        # Send a GET request to the individual page\n",
    "        page_response = requests.get(page_url, headers=headers)\n",
    "        time.sleep(2)  # Add delay to avoid rate limiting\n",
    "        \n",
    "        if page_response.status_code == 200:\n",
    "            page_soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "            \n",
    "            # Find PDF links on the individual page\n",
    "            for pdf_link in page_soup.find_all('a', href=True):\n",
    "                if pdf_link['href'].endswith('.pdf'):\n",
    "                    full_pdf_url = urljoin(page_url, pdf_link['href'])\n",
    "                    pdf_links.append(full_pdf_url)\n",
    "                    \n",
    "                    # Stop after finding 5 files\n",
    "                    if len(pdf_links) >= 5:\n",
    "                        break\n",
    "        else:\n",
    "            print(f'Failed to retrieve the page: {page_url}. Status code: {page_response.status_code}')\n",
    "\n",
    "        # Stop if 5 PDF links have already been found\n",
    "        if len(pdf_links) >= 5:\n",
    "            break\n",
    "\n",
    "    # Download and save each PDF\n",
    "    for i, pdf_url in enumerate(pdf_links):\n",
    "        time.sleep(2)  # Add delay to avoid hitting rate limits\n",
    "        pdf_response = requests.get(pdf_url, headers=headers)\n",
    "\n",
    "        if pdf_response.status_code == 200:\n",
    "            pdf_path = os.path.join(subdirectory, f'file_{i + 1}.pdf')\n",
    "            with open(pdf_path, 'wb') as pdf_file:\n",
    "                pdf_file.write(pdf_response.content)\n",
    "            print(f'Downloaded {pdf_path}')\n",
    "        else:\n",
    "            print(f'Failed to download {pdf_url}. Status code: {pdf_response.status_code}')\n",
    "else:\n",
    "    print(f'Failed to retrieve the base webpage. Status code: {response.status_code}')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6d849138-69fc-4033-8ebf-89625dd4b88e",
   "metadata": {},
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "\n",
    "# Headers to mimic a browser request\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Create the main directory for saving the PDFs\n",
    "main_directory = 'scraper'\n",
    "domain_name = base_url.split('//')[1].split('/')[0]  # Extract domain name from URL\n",
    "subdirectory = os.path.join(main_directory, domain_name)\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(subdirectory, exist_ok=True)\n",
    "\n",
    "# Helper function to make requests with retries and timeout\n",
    "def make_request(url, retries=3, timeout=10):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=timeout)\n",
    "            return response\n",
    "        except requests.exceptions.ConnectTimeout:\n",
    "            print(f\"Attempt {attempt + 1}: Connection timed out. Retrying...\")\n",
    "            time.sleep(2)  # Wait before retrying\n",
    "    return None  # Return None if all retries fail\n",
    "\n",
    "# Send a GET request to the base URL\n",
    "response = make_request(base_url)\n",
    "\n",
    "if response and response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    pdf_links = []\n",
    "\n",
    "    # Find all anchor tags that direct to individual pages\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link['href']\n",
    "\n",
    "        # Filter out invalid links like 'javascript:void(0)'\n",
    "        if href.startswith('javascript:') or not urlparse(href).scheme in ['http', 'https']:\n",
    "            continue\n",
    "\n",
    "        page_url = urljoin(base_url, href)\n",
    "        \n",
    "        # Send a GET request to the individual page\n",
    "        page_response = make_request(page_url)\n",
    "        time.sleep(2)  # Add delay to avoid rate limiting\n",
    "        \n",
    "        if page_response and page_response.status_code == 200:\n",
    "            page_soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "            \n",
    "            # Find PDF links on the individual page\n",
    "            for pdf_link in page_soup.find_all('a', href=True):\n",
    "                if pdf_link['href'].endswith('.pdf'):\n",
    "                    full_pdf_url = urljoin(page_url, pdf_link['href'])\n",
    "                    pdf_links.append(full_pdf_url)\n",
    "                    \n",
    "                    # Stop after finding 5 files\n",
    "                    if len(pdf_links) >= 5:\n",
    "                        break\n",
    "        else:\n",
    "            print(f'Failed to retrieve the page: {page_url}. Status code: {page_response.status_code if page_response else \"No response\"}')\n",
    "\n",
    "        # Stop if 5 PDF links have already been found\n",
    "        if len(pdf_links) >= 5:\n",
    "            break\n",
    "\n",
    "    # Download and save each PDF\n",
    "    for i, pdf_url in enumerate(pdf_links):\n",
    "        time.sleep(2)  # Add delay to avoid hitting rate limits\n",
    "        pdf_response = make_request(pdf_url)\n",
    "\n",
    "        if pdf_response and pdf_response.status_code == 200:\n",
    "            pdf_path = os.path.join(subdirectory, f'file_{i + 1}.pdf')\n",
    "            with open(pdf_path, 'wb') as pdf_file:\n",
    "                pdf_file.write(pdf_response.content)\n",
    "            print(f'Downloaded {pdf_path}')\n",
    "        else:\n",
    "            print(f'Failed to download {pdf_url}. Status code: {pdf_response.status_code if pdf_response else \"No response\"}')\n",
    "else:\n",
    "    print(f'Failed to retrieve the base webpage. Status code: {response.status_code if response else \"No response\"}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043091c7-0a1f-4b0a-9d39-6cad2c278738",
   "metadata": {},
   "source": [
    "Now, adjust the code to use the same filename as the one in the website. Also, to create a subfolder with the corresponding date and to only save files that have CICIMA or cicima in its pdf content.\n",
    "\n",
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3acca66c-c203-4985-a0b6-84b89486300f",
   "metadata": {},
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from datetime import datetime\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "\n",
    "# Headers to mimic a browser request\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "}\n",
    "\n",
    "# Create the main directory for saving the PDFs\n",
    "main_directory = 'scraper'\n",
    "domain_name = base_url.split('//')[1].split('/')[0]  # Extract domain name from URL\n",
    "date_folder = datetime.now().strftime('%Y-%m-%d')  # Current date as folder name\n",
    "subdirectory = os.path.join(main_directory, domain_name, date_folder)\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(subdirectory, exist_ok=True)\n",
    "\n",
    "# Helper function to make requests with retries and timeout\n",
    "def make_request(url, retries=1, timeout=2):\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=timeout)\n",
    "            return response\n",
    "        except requests.exceptions.ConnectTimeout:\n",
    "            print(f\"Attempt {attempt + 1}: Connection timed out. Retrying...\")\n",
    "            time.sleep(2)  # Wait before retrying\n",
    "    return None  # Return None if all retries fail\n",
    "\n",
    "# Helper function to check if PDF contains 'CICIMA' or 'cicima'\n",
    "def contains_cicima(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'rb') as pdf_file:\n",
    "            reader = PdfReader(pdf_file)\n",
    "            for page in reader.pages:\n",
    "                text = page.extract_text() or ''\n",
    "                if 'CICIMA' in text or 'cicima' in text:\n",
    "                    return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file_path}: {e}\")\n",
    "    return False\n",
    "\n",
    "# Send a GET request to the base URL\n",
    "response = make_request(base_url)\n",
    "\n",
    "if response and response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    pdf_links = []\n",
    "\n",
    "    # Find all anchor tags that direct to individual pages\n",
    "    for link in soup.find_all('a', href=True):\n",
    "        href = link['href']\n",
    "\n",
    "        # Filter out invalid links like 'javascript:void(0)'\n",
    "        if href.startswith('javascript:') or not urlparse(href).scheme in ['http', 'https']:\n",
    "            continue\n",
    "\n",
    "        page_url = urljoin(base_url, href)\n",
    "        \n",
    "        # Send a GET request to the individual page\n",
    "        page_response = make_request(page_url)\n",
    "        time.sleep(2)  # Add delay to avoid rate limiting\n",
    "        \n",
    "        if page_response and page_response.status_code == 200:\n",
    "            page_soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "            \n",
    "            # Find PDF links on the individual page\n",
    "            for pdf_link in page_soup.find_all('a', href=True):\n",
    "                if pdf_link['href'].endswith('.pdf'):\n",
    "                    full_pdf_url = urljoin(page_url, pdf_link['href'])\n",
    "                    pdf_links.append(full_pdf_url)\n",
    "                    \n",
    "                    # Stop after finding 5 files\n",
    "                    if len(pdf_links) >= 5:\n",
    "                        break\n",
    "        else:\n",
    "            print(f'Failed to retrieve the page: {page_url}. Status code: {page_response.status_code if page_response else \"No response\"}')\n",
    "\n",
    "        # Stop if 5 PDF links have already been found\n",
    "        if len(pdf_links) >= 5:\n",
    "            break\n",
    "\n",
    "    # Download and save each PDF\n",
    "    for pdf_url in pdf_links:\n",
    "        time.sleep(2)  # Add delay to avoid hitting rate limits\n",
    "        pdf_response = make_request(pdf_url)\n",
    "\n",
    "        if pdf_response and pdf_response.status_code == 200:\n",
    "            filename = os.path.basename(pdf_url.split('?')[0])  # Extract filename from URL\n",
    "            pdf_path = os.path.join(subdirectory, filename)\n",
    "            \n",
    "            with open(pdf_path, 'wb') as pdf_file:\n",
    "                pdf_file.write(pdf_response.content)\n",
    "            \n",
    "            # Check if the PDF contains 'CICIMA' or 'cicima'\n",
    "            if contains_cicima(pdf_path):\n",
    "                print(f'Downloaded and saved {pdf_path}')\n",
    "            else:\n",
    "                os.remove(pdf_path)  # Delete the file if it doesn't contain the keyword\n",
    "                print(f'Deleted {filename} as it did not contain \"CICIMA\" or \"cicima\"')\n",
    "        else:\n",
    "            print(f'Failed to download {pdf_url}. Status code: {pdf_response.status_code if pdf_response else \"No response\"}')\n",
    "else:\n",
    "    print(f'Failed to retrieve the base webpage. Status code: {response.status_code if response else \"No response\"}')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f576dc9f-873f-4ee3-ab3d-e34d72740411",
   "metadata": {},
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "def search_engine_query(query, num_results=10):\n",
    "    \"\"\"\n",
    "    Queries an open search engine (e.g., DuckDuckGo) and returns a list of result URLs.\n",
    "    \n",
    "    Args:\n",
    "    - query (str): The search query string.\n",
    "    - num_results (int): Number of result URLs to retrieve.\n",
    "    \n",
    "    Returns:\n",
    "    - List of result URLs.\n",
    "    \"\"\"\n",
    "    base_url = 'https://duckduckgo.com/html/'\n",
    "    params = {'q': query}\n",
    "    query_url = f\"{base_url}?{urlencode(params)}\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "\n",
    "    response = requests.get(query_url, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        result_links = []\n",
    "        \n",
    "        # Find all links in search results\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if '/l/?kh=-1&uddg=' in href:\n",
    "                continue  # Skip unnecessary internal DuckDuckGo redirect links\n",
    "\n",
    "            # Ensure the link is an external, valid result link\n",
    "            if not href.startswith('http'):\n",
    "                continue\n",
    "\n",
    "            result_links.append(href)\n",
    "            \n",
    "            # Stop if we have enough results\n",
    "            if len(result_links) >= num_results:\n",
    "                break\n",
    "        \n",
    "        return result_links\n",
    "    else:\n",
    "        print(f\"Failed to retrieve search results. Status code: {response.status_code}\")\n",
    "        return []\n",
    "\n",
    "# Example usage\n",
    "query = \"CICIMA research papers\"\n",
    "urls = search_engine_query(query, num_results=5)\n",
    "print(\"Search Result URLs:\")\n",
    "for url in urls:\n",
    "    print(url)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a8fc90c7-eb26-4ce1-9a15-fa300c68c8a2",
   "metadata": {},
   "source": [
    "def scrape_url(base_url):\n",
    "    import os\n",
    "    import time\n",
    "    import requests\n",
    "    from bs4 import BeautifulSoup\n",
    "    from urllib.parse import urljoin, urlparse\n",
    "    from datetime import datetime\n",
    "    from PyPDF2 import PdfReader\n",
    "    \n",
    "    \n",
    "    # Headers to mimic a browser request\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "    \n",
    "    # Create the main directory for saving the PDFs\n",
    "    main_directory = 'scraper'\n",
    "    domain_name = base_url.split('//')[1].split('/')[0]  # Extract domain name from URL\n",
    "    date_folder = datetime.now().strftime('%Y-%m-%d')  # Current date as folder name\n",
    "    subdirectory = os.path.join(main_directory, domain_name, date_folder)\n",
    "    \n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(subdirectory, exist_ok=True)\n",
    "    \n",
    "    # Helper function to make requests with retries and timeout\n",
    "    def make_request(url, retries=1, timeout=2):\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                response = requests.get(url, headers=headers, timeout=timeout)\n",
    "                return response\n",
    "            except requests.exceptions.ConnectTimeout:\n",
    "                print(f\"Attempt {attempt + 1}: Connection timed out. Retrying...\")\n",
    "                time.sleep(2)  # Wait before retrying\n",
    "        return None  # Return None if all retries fail\n",
    "    \n",
    "    # Helper function to check if PDF contains 'CICIMA' or 'cicima'\n",
    "    def contains_cicima(file_path):\n",
    "        try:\n",
    "            with open(file_path, 'rb') as pdf_file:\n",
    "                reader = PdfReader(pdf_file)\n",
    "                for page in reader.pages:\n",
    "                    text = page.extract_text() or ''\n",
    "                    if 'CICIMA' in text or 'cicima' in text:\n",
    "                        return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "        return False\n",
    "    \n",
    "    # Send a GET request to the base URL\n",
    "    response = make_request(base_url)\n",
    "    \n",
    "    if response and response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        pdf_links = []\n",
    "    \n",
    "        # Find all anchor tags that direct to individual pages\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "    \n",
    "            # Filter out invalid links like 'javascript:void(0)'\n",
    "            if href.startswith('javascript:') or not urlparse(href).scheme in ['http', 'https']:\n",
    "                continue\n",
    "    \n",
    "            page_url = urljoin(base_url, href)\n",
    "            \n",
    "            # Send a GET request to the individual page\n",
    "            page_response = make_request(page_url)\n",
    "            time.sleep(2)  # Add delay to avoid rate limiting\n",
    "            \n",
    "            if page_response and page_response.status_code == 200:\n",
    "                page_soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "                \n",
    "                # Find PDF links on the individual page\n",
    "                for pdf_link in page_soup.find_all('a', href=True):\n",
    "                    if pdf_link['href'].endswith('.pdf'):\n",
    "                        full_pdf_url = urljoin(page_url, pdf_link['href'])\n",
    "                        pdf_links.append(full_pdf_url)\n",
    "                        \n",
    "                        # Stop after finding 5 files\n",
    "                        if len(pdf_links) >= 5:\n",
    "                            break\n",
    "            else:\n",
    "                print(f'Failed to retrieve the page: {page_url}. Status code: {page_response.status_code if page_response else \"No response\"}')\n",
    "    \n",
    "            # Stop if 5 PDF links have already been found\n",
    "            if len(pdf_links) >= 5:\n",
    "                break\n",
    "    \n",
    "        # Download and save each PDF\n",
    "        for pdf_url in pdf_links:\n",
    "            time.sleep(2)  # Add delay to avoid hitting rate limits\n",
    "            pdf_response = make_request(pdf_url)\n",
    "    \n",
    "            if pdf_response and pdf_response.status_code == 200:\n",
    "                filename = os.path.basename(pdf_url.split('?')[0])  # Extract filename from URL\n",
    "                pdf_path = os.path.join(subdirectory, filename)\n",
    "                \n",
    "                with open(pdf_path, 'wb') as pdf_file:\n",
    "                    pdf_file.write(pdf_response.content)\n",
    "                \n",
    "                # Check if the PDF contains 'CICIMA' or 'cicima'\n",
    "                if contains_cicima(pdf_path):\n",
    "                    print(f'Downloaded and saved {pdf_path}')\n",
    "                else:\n",
    "                    os.remove(pdf_path)  # Delete the file if it doesn't contain the keyword\n",
    "                    print(f'Deleted {filename} as it did not contain \"CICIMA\" or \"cicima\"')\n",
    "            else:\n",
    "                print(f'Failed to download {pdf_url}. Status code: {pdf_response.status_code if pdf_response else \"No response\"}')\n",
    "    else:\n",
    "        print(f'Failed to retrieve the base webpage. Status code: {response.status_code if response else \"No response\"}')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6ceb8631-a799-45f6-b740-3a3b6d788eef",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from datetime import datetime\n",
    "from PyPDF2 import PdfReader\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "def search_engine_query(query, num_results=10, timeout=10):\n",
    "    print(\"Ok search_engine_query \")\n",
    "    \"\"\"\n",
    "    Queries an open search engine (e.g., DuckDuckGo) and returns a list of result URLs.\n",
    "    \n",
    "    Args:\n",
    "    - query (str): The search query string.\n",
    "    - num_results (int): Number of result URLs to retrieve.\n",
    "    - timeout (int): The timeout for the request in seconds.\n",
    "    \n",
    "    Returns:\n",
    "    - List of result URLs.\n",
    "    \"\"\"\n",
    "    base_url = 'https://duckduckgo.com/html/'\n",
    "    params = {'q': query}\n",
    "    query_url = f\"{base_url}?{urlencode(params)}\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(query_url, headers=headers, timeout=timeout)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            result_links = []\n",
    "            \n",
    "            # Find all links in search results\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = link['href']\n",
    "                if '/l/?kh=-1&uddg=' in href:\n",
    "                    continue  # Skip unnecessary internal DuckDuckGo redirect links\n",
    "\n",
    "                # Ensure the link is an external, valid result link\n",
    "                if not href.startswith('http'):\n",
    "                    continue\n",
    "\n",
    "                result_links.append(href)\n",
    "                \n",
    "                # Stop if we have enough results\n",
    "                if len(result_links) >= num_results:\n",
    "                    break\n",
    "            \n",
    "            return result_links\n",
    "        else:\n",
    "            print(f\"Failed to retrieve search results. Status code: {response.status_code}\")\n",
    "            return []\n",
    "    \n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"The request timed out.\")\n",
    "        return []\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "# Example usage\n",
    "query = \"CICIMA research papers\"\n",
    "urls = search_engine_query(query, num_results=5)\n",
    "print(\"Search Result URLs:\")\n",
    "for url in urls:\n",
    "    print(url)\n",
    "    \n",
    "\n",
    "target_strings = [\"ucr\", \"UCR\"]\n",
    "def scrape_url(base_url):\n",
    "    \"\"\"\n",
    "    Scrapes a given URL, downloads PDF files containing 'CICIMA' in their content,\n",
    "    and saves them to a directory structure.\n",
    "    \"\"\"\n",
    "    # Headers to mimic a browser request\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "    \n",
    "    # Create the main directory for saving the PDFs\n",
    "    main_directory = 'scraper'\n",
    "    domain_name = urlparse(base_url).netloc  # Extract domain name from URL\n",
    "    date_folder = datetime.now().strftime('%Y-%m-%d')  # Current date as folder name\n",
    "    subdirectory = os.path.join(main_directory, domain_name, date_folder)\n",
    "    \n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(subdirectory, exist_ok=True)\n",
    "    \n",
    "    # Helper function to make requests with retries and timeout\n",
    "    def make_request(url, retries=1, timeout=2):\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                response = requests.get(url, headers=headers, timeout=timeout)\n",
    "                response.raise_for_status()  # Raise an HTTPError for bad responses (4xx and 5xx)\n",
    "                return response\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Attempt {attempt + 1}: Error occurred: {e}. Retrying...\")\n",
    "                time.sleep(2)  # Wait before retrying\n",
    "        return None  # Return None if all retries fail\n",
    "    \n",
    "    # Helper function to check if PDF contains 'CICIMA' or 'cicima'\n",
    "    def contains_strings(file_path, strings_to_check):\n",
    "        \"\"\"\n",
    "        Checks if any of the strings in `strings_to_check` are present in the PDF content.\n",
    "    \n",
    "        Args:\n",
    "        - file_path (str): Path to the PDF file.\n",
    "        - strings_to_check (list of str): List of strings to search for in the PDF.\n",
    "    \n",
    "        Returns:\n",
    "        - bool: True if any of the strings are found, False otherwise.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(file_path, 'rb') as pdf_file:\n",
    "                reader = PdfReader(pdf_file)\n",
    "                for page in reader.pages:\n",
    "                    text = page.extract_text() or ''\n",
    "                    for search_string in strings_to_check:\n",
    "                        if search_string in text:\n",
    "                            return True\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "    \n",
    "    # Send a GET request to the base URL\n",
    "    response = make_request(base_url)\n",
    "    \n",
    "    if response:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        pdf_links = []\n",
    "    \n",
    "        # Find all anchor tags that direct to individual pages\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "    \n",
    "            # Filter out invalid links like 'javascript:void(0)'\n",
    "            if href.startswith('javascript:') or not urlparse(href).scheme in ['http', 'https']:\n",
    "                continue\n",
    "    \n",
    "            page_url = urljoin(base_url, href)\n",
    "            \n",
    "            # Send a GET request to the individual page\n",
    "            page_response = make_request(page_url)\n",
    "            time.sleep(2)  # Add delay to avoid rate limiting\n",
    "            \n",
    "            if page_response:\n",
    "                page_soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "                \n",
    "                # Find PDF links on the individual page\n",
    "                for pdf_link in page_soup.find_all('a', href=True):\n",
    "                    if pdf_link['href'].endswith('.pdf'):\n",
    "                        full_pdf_url = urljoin(page_url, pdf_link['href'])\n",
    "                        pdf_links.append(full_pdf_url)\n",
    "                        \n",
    "                        # Stop after finding 5 files\n",
    "                        if len(pdf_links) >= 5:\n",
    "                            break\n",
    "            else:\n",
    "                print(f'Failed to retrieve the page: {page_url}')\n",
    "    \n",
    "            # Stop if 5 PDF links have already been found\n",
    "            if len(pdf_links) >= 5:\n",
    "                break\n",
    "    \n",
    "        # Download and save each PDF\n",
    "        for pdf_url in pdf_links:\n",
    "            time.sleep(2)  # Add delay to avoid hitting rate limits\n",
    "            pdf_response = make_request(pdf_url)\n",
    "    \n",
    "            if pdf_response:\n",
    "                filename = os.path.basename(pdf_url.split('?')[0])  # Extract filename from URL\n",
    "                pdf_path = os.path.join(subdirectory, filename)\n",
    "                \n",
    "                with open(pdf_path, 'wb') as pdf_file:\n",
    "                    pdf_file.write(pdf_response.content)\n",
    "                \n",
    "                # Check if the PDF contains 'CICIMA' or 'cicima'\n",
    "                if contains_strings(pdf_path, target_strings):\n",
    "                    print(f'Downloaded and saved {pdf_path}')\n",
    "                else:\n",
    "                    os.remove(pdf_path)  # Delete the file if it doesn't contain the keyword\n",
    "                    print(f'Deleted {filename} as it did not contain \"CICIMA\" or \"cicima\"')\n",
    "            else:\n",
    "                print(f'Failed to download {pdf_url}')\n",
    "    else:\n",
    "        print(f'Failed to retrieve the base webpage: {base_url}')\n",
    "\n",
    "\n",
    "def main():\n",
    "    print(\"Ok 1\")\n",
    "    queries = [\"academic paper materials science ucr\", \"materials science university of costa rica papers\", \"materials engineering UCR\"]\n",
    "    for query in queries:\n",
    "        print(\"Ok loop 2\")\n",
    "        urls = search_engine_query(query, num_results=10)\n",
    "        print(urls)\n",
    "        for url in urls:\n",
    "            scrape_url(url)\n",
    "main()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2d4a46be-3560-4e10-80e0-ca497cf85e0b",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "from datetime import datetime\n",
    "from PyPDF2 import PdfReader\n",
    "from urllib.parse import urlencode\n",
    "\n",
    "# Configura los proxies de Tor\n",
    "TOR_PROXY = {\n",
    "    \"http\": \"socks5h://127.0.0.1:9050\",\n",
    "    \"https\": \"socks5h://127.0.0.1:9050\"\n",
    "}\n",
    "\n",
    "def search_engine_query(query, num_results=10, timeout=10):\n",
    "    print(\"Using Tor for search_engine_query\")\n",
    "    base_url = 'https://duckduckgo.com/html/'\n",
    "    params = {'q': query}\n",
    "    query_url = f\"{base_url}?{urlencode(params)}\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(query_url, headers=headers, proxies=TOR_PROXY, timeout=timeout)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            result_links = []\n",
    "            \n",
    "            # Find all links in search results\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = link['href']\n",
    "                if '/l/?kh=-1&uddg=' in href:\n",
    "                    continue  # Skip unnecessary internal DuckDuckGo redirect links\n",
    "\n",
    "                # Ensure the link is an external, valid result link\n",
    "                if not href.startswith('http'):\n",
    "                    continue\n",
    "\n",
    "                result_links.append(href)\n",
    "                \n",
    "                # Stop if we have enough results\n",
    "                if len(result_links) >= num_results:\n",
    "                    break\n",
    "            \n",
    "            return result_links\n",
    "        else:\n",
    "            print(f\"Failed to retrieve search results. Status code: {response.status_code}\")\n",
    "            return []\n",
    "    \n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"The request timed out.\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []\n",
    "\n",
    "def scrape_url(base_url):\n",
    "    print(f\"Scraping {base_url} using Tor...\")\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'\n",
    "    }\n",
    "\n",
    "    def make_request(url, retries=3, timeout=10):\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                response = requests.get(url, headers=headers, proxies=TOR_PROXY, timeout=timeout)\n",
    "                response.raise_for_status()\n",
    "                return response\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Attempt {attempt + 1}: Error occurred: {e}. Retrying...\")\n",
    "                time.sleep(2)\n",
    "        return None\n",
    "\n",
    "    # Similar logic as in your original scrape_url function\n",
    "    # Uses make_request for all HTTP requests via Tor\n",
    "    response = make_request(base_url)\n",
    "    if response:\n",
    "        print(f\"Successfully accessed {base_url} through Tor!\")\n",
    "    else:\n",
    "        print(f\"Failed to access {base_url}.\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    query = \"CICIMA research papers\"\n",
    "    urls = search_engine_query(query, num_results=5)\n",
    "    for url in urls:\n",
    "        scrape_url(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9dfbe2f-b3a2-42d2-b441-0de8a07a6994",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Buffer:\n",
    "\n",
    "def random_headers():\n",
    "    user_agents = [\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/109.0\",\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n",
    "        \"Mozilla/5.0 (Windows NT 10.0; WOW64; rv:45.0) Gecko/20100101 Firefox/45.0\",\n",
    "        \"Mozilla/5.0 (iPhone; CPU iPhone OS 15_2_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/15.2 Mobile/15E148 Safari/604.1\"\n",
    "    ]\n",
    "    \n",
    "    accept_languages = [\n",
    "        \"en-US,en;q=0.9\",\n",
    "        \"en-GB,en;q=0.8\",\n",
    "        \"es-ES,es;q=0.7,en;q=0.5\",\n",
    "        \"de-DE,de;q=0.9,en;q=0.8\",\n",
    "        \"fr-FR,fr;q=0.9,en;q=0.8\"\n",
    "    ]\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": random.choice(user_agents),\n",
    "        \"Accept-Language\": random.choice(accept_languages),\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "        \"Referer\": random.choice([\n",
    "            \"https://www.google.com/\",\n",
    "            \"https://www.bing.com/\",\n",
    "            \"https://www.yahoo.com/\",\n",
    "            \"https://duckduckgo.com/\"\n",
    "        ]),\n",
    "        \"Connection\": \"keep-alive\"\n",
    "    }\n",
    "    return headers\n",
    "\n",
    "search_engines = {\n",
    "    \"Google Scholar\": {\n",
    "        \"url\": \"https://scholar.google.com\",\n",
    "        \"description\": \"Covers a wide range of disciplines. Use 'filetype:pdf' to find PDFs.\"\n",
    "    },\n",
    "    \"Microsoft Academic\": {\n",
    "        \"url\": \"https://academic.microsoft.com\",\n",
    "        \"description\": \"Academic papers across various fields. Provides citation details and links to full texts.\"\n",
    "    },\n",
    "    \"PubMed\": {\n",
    "        \"url\": \"https://pubmed.ncbi.nlm.nih.gov\",\n",
    "        \"description\": \"Focused on biomedical and life sciences literature. Often links to PDFs on journal sites.\"\n",
    "    },\n",
    "    \"ResearchGate\": {\n",
    "        \"url\": \"https://www.researchgate.net\",\n",
    "        \"description\": \"Connects researchers and provides access to many PDF articles uploaded by authors.\"\n",
    "    },\n",
    "    \"Semantic Scholar\": {\n",
    "        \"url\": \"https://www.semanticscholar.org\",\n",
    "        \"description\": \"AI-powered search for scientific literature. Often provides links to free PDFs.\"\n",
    "    },\n",
    "    \"BASE (Bielefeld Academic Search Engine)\": {\n",
    "        \"url\": \"https://www.base-search.net\",\n",
    "        \"description\": \"Indexes open access repositories worldwide. Filters available for full-text PDFs.\"\n",
    "    },\n",
    "    \"CORE\": {\n",
    "        \"url\": \"https://core.ac.uk\",\n",
    "        \"description\": \"Aggregates open access research outputs from repositories and journals. PDFs often available directly.\"\n",
    "    },\n",
    "    \"Directory of Open Access Journals (DOAJ)\": {\n",
    "        \"url\": \"https://doaj.org\",\n",
    "        \"description\": \"Comprehensive directory of open access journals. Many articles available in PDF.\"\n",
    "    },\n",
    "    \"ScienceDirect\": {\n",
    "        \"url\": \"https://www.sciencedirect.com\",\n",
    "        \"description\": \"Extensive database of scientific and technical research. PDFs often require subscription.\"\n",
    "    },\n",
    "    \"arXiv\": {\n",
    "        \"url\": \"https://arxiv.org\",\n",
    "        \"description\": \"Preprint repository for physics, mathematics, computer science, and related fields. All articles available as free PDFs.\"\n",
    "    },\n",
    "    \"SSRN (Social Science Research Network)\": {\n",
    "        \"url\": \"https://www.ssrn.com\",\n",
    "        \"description\": \"Focuses on social science and humanities papers. Many articles available as PDFs.\"\n",
    "    },\n",
    "    \"ERIC (Education Resources Information Center)\": {\n",
    "        \"url\": \"https://eric.ed.gov\",\n",
    "        \"description\": \"Provides access to education-related literature, including PDFs of journal articles and reports.\"\n",
    "    },\n",
    "    \"PLOS (Public Library of Science)\": {\n",
    "        \"url\": \"https://plos.org\",\n",
    "        \"description\": \"Open access journals in science and medicine. All articles available as free PDFs.\"\n",
    "    },\n",
    "    \"JSTOR\": {\n",
    "        \"url\": \"https://www.jstor.org\",\n",
    "        \"description\": \"Archives of academic journals, books, and primary sources. PDFs often require subscription.\"\n",
    "    },\n",
    "    \"Open Access Button\": {\n",
    "        \"url\": \"https://openaccessbutton.org\",\n",
    "        \"description\": \"Helps find open access versions of research papers, including PDFs.\"\n",
    "    },\n",
    "    \"Sci-Hub\": {\n",
    "        \"url\": \"https://sci-hub.se\",\n",
    "        \"description\": \"Provides access to a vast collection of research articles in PDF format. Legal status varies by jurisdiction.\"\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_search_engine_url():\n",
    "    search_engines = {\n",
    "        \"Google Scholar\": \"https://scholar.google.com\",\n",
    "        \"Microsoft Academic\": \"https://academic.microsoft.com\",\n",
    "        \"PubMed\": \"https://pubmed.ncbi.nlm.nih.gov\",\n",
    "        \"ResearchGate\": \"https://www.researchgate.net\",\n",
    "        \"Semantic Scholar\": \"https://www.semanticscholar.org\",\n",
    "        \"BASE\": \"https://www.base-search.net\",\n",
    "        \"CORE\": \"https://core.ac.uk\",\n",
    "        \"DOAJ\": \"https://doaj.org\",\n",
    "        \"ScienceDirect\": \"https://www.sciencedirect.com\",\n",
    "        \"arXiv\": \"https://arxiv.org\",\n",
    "        \"SSRN\": \"https://www.ssrn.com\",\n",
    "        \"ERIC\": \"https://eric.ed.gov\",\n",
    "        \"PLOS\": \"https://plos.org\",\n",
    "        \"JSTOR\": \"https://www.jstor.org\",\n",
    "        \"Open Access Button\": \"https://openaccessbutton.org\",\n",
    "        \"Sci-Hub\": \"https://sci-hub.se\"\n",
    "    }\n",
    "    return random.choice(list(search_engines.values()))\n",
    "queries_dict = {\n",
    "    1: \"research articles in materials science UCR PDF\",\n",
    "    2: \"papers PDF on materials science university of costa rica\",\n",
    "    3: \"materials engineering research at UCR PDF\",\n",
    "    4: \"university of costa rica materials science publications PDF\",\n",
    "    5: \"UCR academic papers in materials engineering PDF\",\n",
    "    6: \"PDF Costa Rica university materials science research\",\n",
    "    7: \"UCR research on materials engineering field PDF\",\n",
    "    8: \"materials science academic papers Costa Rica PDF\",\n",
    "    9: \"university of costa rica research materials engineering PDF\",\n",
    "    10: \"research papers UCR materials science department PDF\"\n",
    "}\n",
    "\n",
    "# A dictionary that maps each letter to its neighboring keys on a QWERTY keyboard\n",
    "keyboard_neighbors = {\n",
    "    'a': ['q', 'w', 's', 'z', 'x'],\n",
    "    'b': ['v', 'g', 'n'],\n",
    "    'c': ['x', 'f', 'v', 'd'],\n",
    "    'd': ['s', 'e', 'r', 'f', 'c'],\n",
    "    'e': ['w', 'r', 'd', 's'],\n",
    "    'f': ['r', 't', 'g', 'v', 'd'],\n",
    "    'g': ['t', 'h', 'f', 'b', 'v'],\n",
    "    'h': ['y', 'j', 'g', 'n', 'b'],\n",
    "    'i': ['u', 'o', 'k'],\n",
    "    'j': ['u', 'k', 'h'],\n",
    "    'k': ['i', 'j', 'l', 'n'],\n",
    "    'l': ['o', 'k'],\n",
    "    'm': ['n', 'j'],\n",
    "    'n': ['b', 'h', 'm'],\n",
    "    'o': ['i', 'p', 'l'],\n",
    "    'p': ['o', 'l'],\n",
    "    'q': ['w', 'a'],\n",
    "    'r': ['e', 't', 'f', 'd'],\n",
    "    's': ['a', 'w', 'e', 'd', 'z'],\n",
    "    't': ['r', 'y', 'f', 'g'],\n",
    "    'u': ['y', 'i', 'j'],\n",
    "    'v': ['c', 'f', 'g', 'b'],\n",
    "    'w': ['q', 'e', 's', 'a'],\n",
    "    'x': ['z', 'c', 'd', 's'],\n",
    "    'y': ['t', 'u', 'h'],\n",
    "    'z': ['a', 'x']\n",
    "}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4b82a37f-f368-474a-9b5f-89f7202cb433",
   "metadata": {},
   "source": [
    "from stem.control import Controller\n",
    "\n",
    "try:\n",
    "    with Controller.from_port(port=9051) as controller:\n",
    "        controller.authenticate()\n",
    "        print(\"Connected to Tor control port successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Failed to connect to Tor control port: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "492eb0d0-9443-4dc5-be06-44299a36408f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "import requests\n",
    "\n",
    "# Set up the Chrome WebDriver\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"--headless\")  # run headless for no browser window\n",
    "options.add_argument(\"--disable-gpu\")  # Disable GPU (optional)\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "\n",
    "# Function to find PDF links on a page\n",
    "def find_pdf_links(driver):\n",
    "    pdf_links = []\n",
    "    links = driver.find_elements(By.TAG_NAME, 'a')\n",
    "    for link in links:\n",
    "        href = link.get_attribute('href')\n",
    "        if href and href.endswith('.pdf'):\n",
    "            pdf_links.append(href)\n",
    "    return pdf_links\n",
    "\n",
    "# Function to click all buttons on the page and open every link\n",
    "def click_buttons_and_links(driver, url):\n",
    "    driver.get(url)\n",
    "    time.sleep(2)  # Wait for the page to load\n",
    "\n",
    "    # Find and click all buttons\n",
    "    buttons = driver.find_elements(By.TAG_NAME, 'button')\n",
    "    for button in buttons:\n",
    "        try:\n",
    "            button.click()\n",
    "            time.sleep(1)  # Wait for any changes after clicking the button\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to click button: {e}\")\n",
    "    \n",
    "    # Find and click all links\n",
    "    links = driver.find_elements(By.TAG_NAME, 'a')\n",
    "    for link in links:\n",
    "        try:\n",
    "            href = link.get_attribute('href')\n",
    "            if href:\n",
    "                link.click()\n",
    "                time.sleep(2)  # Allow time for the new page to load\n",
    "                # Check for PDF links on the subpage\n",
    "                pdf_links = find_pdf_links(driver)\n",
    "                if pdf_links:\n",
    "                    print(f\"PDF links found: {pdf_links}\")\n",
    "                # Click buttons on the subpage\n",
    "                subpage_buttons = driver.find_elements(By.TAG_NAME, 'button')\n",
    "                for sub_button in subpage_buttons:\n",
    "                    sub_button.click()\n",
    "                    time.sleep(1)\n",
    "                driver.back()  # Go back to the previous page\n",
    "                time.sleep(1)\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to click link: {e}\")\n",
    "    \n",
    "    # Check for PDF links on the main page\n",
    "    pdf_links = find_pdf_links(driver)\n",
    "    if pdf_links:\n",
    "        print(f\"PDF links found on {url}: {pdf_links}\")\n",
    "\n",
    "# Main function\n",
    "def get_pdfs(url):\n",
    "    # URL to start with\n",
    "    url = 'https://example.com'  # Change this to your target URL\n",
    "    click_buttons_and_links(driver, url)\n",
    "    driver.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02c6eddc-61bd-42f1-9bb8-b8650a2442eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tor IP address changed.\n",
      "New IP address: 185.220.101.66\n",
      "Fetched URLs: ['https://oval.base-search.net/', 'https://api.base-search.net/', 'http://oai.base-search.net/', 'https://openbiblio.social/@base', 'https://www.ub.uni-bielefeld.de/<-de,en>/ub/barrierefreiheit/base/', 'https://orcid.org', 'https://www.ub.uni-bielefeld.de/english', 'http://lucene.apache.org/solr/', 'http://vufind.org/']\n",
      "New URLs: ['https://oval.base-search.net/', 'https://api.base-search.net/', 'http://oai.base-search.net/', 'https://openbiblio.social/@base', 'https://www.ub.uni-bielefeld.de/<-de,en>/ub/barrierefreiheit/base/', 'https://orcid.org', 'https://www.ub.uni-bielefeld.de/english', 'http://lucene.apache.org/solr/', 'http://vufind.org/']\n",
      "Failed to download http://purl.org/net/talks/2019-04-03-gitlab_as_an_institutional_service.pdf\n",
      "Failed to download http://purl.org/net/talks/2016-10-21-static_site_generator_jekyll.pdf\n",
      "Failed to download http://purl.org/net/talks/lockss-memento.pdf\n",
      "Failed to download http://purl.org/net/markdown/slides.pdf\n",
      "Failed to download http://wiki.ub.uni-bielefeld.de//wikifarm/fields/ub_edvmain/uploads/Oeffentlich/opac_2013.pdf\n",
      "HTTPConnectionPool(host='localhost', port=55389): Max retries exceeded with url: /session/c877e03e01cff2c885d97d570320323a/url (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x000001C7CA878620>: Failed to establish a new connection: [WinError 10061] No connection could be made because the target machine actively refused it'))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse, urlencode\n",
    "from datetime import datetime\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "from stem.control import Controller\n",
    "\n",
    "def get_current_ip():\n",
    "    try:\n",
    "        response = requests.get('https://api.ipify.org', proxies=TOR_PROXY, timeout=10)\n",
    "        if response.status_code == 200:\n",
    "            return response.text\n",
    "        else:\n",
    "            return \"Failed to retrieve IP\"\n",
    "    except Exception as e:\n",
    "        return f\"Error: {e}\"\n",
    "\n",
    "def change_tor_ip():\n",
    "    with Controller.from_port(port=9051) as controller:\n",
    "        controller.authenticate()  # Automatically uses cookie authentication\n",
    "        controller.signal('NEWNYM')\n",
    "        print(\"Tor IP address changed.\")\n",
    "        new_ip = get_current_ip()\n",
    "        print(f\"New IP address: {new_ip}\")\n",
    "        \n",
    "# TOR_PROXY = {\n",
    "#     \"http\": \"socks5h://127.0.0.1:9050\",\n",
    "#     \"https\": \"socks5h://127.0.0.1:9050\"\n",
    "# }\n",
    "\n",
    "TOR_PROXY = {\n",
    "    \"http\": \"socks5h://127.0.0.1:9150\",\n",
    "    \"https\": \"socks5h://127.0.0.1:9150\"\n",
    "}\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "def search_engine_query(query, num_results=10, timeout=10):\n",
    "    change_tor_ip()\n",
    "    time.sleep(2)\n",
    "    #base_url = 'https://duckduckgo.com/html/'\n",
    "    base_url = get_search_engine_url()\n",
    "    params = {'q': query}\n",
    "    query_url = f\"{base_url}?{urlencode(params)}\"\n",
    "    headers = random_headers()\n",
    "    response = requests.get(query_url, headers=headers, proxies=TOR_PROXY, timeout=timeout)\n",
    "    #print(response.status_code)  # Debugging step\n",
    "    #print(response.text)  # Debugging step\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        result_links = []\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if '/l/?kh=-1&uddg=' in href or not href.startswith('http'):\n",
    "                continue\n",
    "            result_links.append(href)\n",
    "            if len(result_links) >= num_results:\n",
    "                break\n",
    "        return result_links\n",
    "    return []\n",
    "\n",
    "target_strings = [\"ucr\", \"UCR\"]\n",
    "\n",
    "def scrape_url(base_url):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "    main_directory = 'scraper'\n",
    "    domain_name = urlparse(base_url).netloc\n",
    "    date_folder = datetime.now().strftime('%Y-%m-%d')\n",
    "    subdirectory = os.path.join(main_directory, domain_name, date_folder)\n",
    "    os.makedirs(subdirectory, exist_ok=True)\n",
    "\n",
    "    def make_request(url, retries=3, timeout=10):\n",
    "        for _ in range(retries):\n",
    "            try:\n",
    "                response = requests.get(url, headers=headers, proxies=TOR_PROXY, timeout=timeout)\n",
    "                response.raise_for_status()\n",
    "                return response\n",
    "            except:\n",
    "                time.sleep(10)\n",
    "        return None\n",
    "\n",
    "    def contains_strings(file_path, strings_to_check):\n",
    "        try:\n",
    "            with open(file_path, 'rb') as pdf_file:\n",
    "                reader = PdfReader(pdf_file)\n",
    "                for page in reader.pages:\n",
    "                    text = page.extract_text() or ''\n",
    "                    if any(search_string in text for search_string in strings_to_check):\n",
    "                        return True\n",
    "        except:\n",
    "            pass\n",
    "        return False\n",
    "\n",
    "    response = make_request(base_url)\n",
    "    if response:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        pdf_links = []\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            href = link['href']\n",
    "            if href.startswith('javascript:') or not urlparse(href).scheme in ['http', 'https']:\n",
    "                continue\n",
    "            page_url = urljoin(base_url, href)\n",
    "            page_response = make_request(page_url)\n",
    "            if page_response:\n",
    "                page_soup = BeautifulSoup(page_response.content, 'html.parser')\n",
    "                for pdf_link in page_soup.find_all('a', href=True):\n",
    "                    if pdf_link['href'].endswith('.pdf'):\n",
    "                        full_pdf_url = urljoin(page_url, pdf_link['href'])\n",
    "                        pdf_links.append(full_pdf_url)\n",
    "                        if len(pdf_links) >= 5:\n",
    "                            break\n",
    "            if len(pdf_links) >= 5:\n",
    "                break\n",
    "\n",
    "        for pdf_url in pdf_links:\n",
    "            pdf_response = make_request(pdf_url)\n",
    "            if pdf_response:\n",
    "                filename = os.path.basename(pdf_url.split('?')[0])\n",
    "                pdf_path = os.path.join(subdirectory, filename)\n",
    "                with open(pdf_path, 'wb') as pdf_file:\n",
    "                    pdf_file.write(pdf_response.content)\n",
    "                if contains_strings(pdf_path, target_strings):\n",
    "                    print(f'Downloaded and saved {pdf_path}')\n",
    "                else:\n",
    "                    os.remove(pdf_path)\n",
    "            else:\n",
    "                print(f'Failed to download {pdf_url}')\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "def mutate_query(query, num_changes=2):\n",
    "    query_list = list(query)  # Convert query to a list of characters\n",
    "    for _ in range(num_changes):\n",
    "        # Choose a random position in the query\n",
    "        idx = random.randint(0, len(query_list) - 1)\n",
    "        char = query_list[idx].lower()  # Get the character at that position\n",
    "        \n",
    "        if char in keyboard_neighbors:\n",
    "            # Pick a random neighboring letter\n",
    "            new_char = random.choice(keyboard_neighbors[char])\n",
    "            \n",
    "            # Randomly decide whether to keep the case or swap it\n",
    "            if random.random() > 0.5:\n",
    "                new_char = new_char.upper() if query_list[idx].isupper() else new_char\n",
    "            \n",
    "            query_list[idx] = new_char\n",
    "    \n",
    "    # Join the list back into a string and return\n",
    "    return ''.join(query_list)\n",
    "\n",
    "def modify_queries(queries, num_changes=2):\n",
    "    modified_queries = []\n",
    "    for query in queries:\n",
    "        modified_query = mutate_query(query, num_changes)\n",
    "        modified_queries.append(modified_query)\n",
    "    return modified_queries\n",
    "\n",
    "# Example usage\n",
    "queries = [\n",
    "    \"research articles in materials science UCR\",\n",
    "    \"papers on materials science university of costa rica\",\n",
    "    \"materials engineering research at UCR\",\n",
    "    \"university of costa rica materials science publications\",\n",
    "    \"UCR academic papers in materials engineering\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "def sample_queries(queries_dict, num_samples=3):\n",
    "    # Select a random sample of queries (num_samples) from the queries_dict\n",
    "    sample = []\n",
    "    while not sample:\n",
    "        sample = random.sample(list(queries_dict.values()), num_samples)\n",
    "    return sample\n",
    "\n",
    "def save_new_urls(urls, file_path=\"urls.txt\"):\n",
    "    \"\"\"\n",
    "    Saves new URLs to a file, ensuring no duplicates.\n",
    "    Returns a list of URLs that were newly added.\n",
    "    \"\"\"\n",
    "    # Load existing URLs from the file, if it exists\n",
    "    try:\n",
    "        with open(file_path, \"r\") as file:\n",
    "            existing_urls = set(file.read().splitlines())\n",
    "    except FileNotFoundError:\n",
    "        existing_urls = set()\n",
    "\n",
    "    # Find new URLs (not already in the file)\n",
    "    new_urls = [url for url in urls if url not in existing_urls]\n",
    "\n",
    "    # Append new URLs to the file\n",
    "    with open(file_path, \"a\") as file:\n",
    "        for url in new_urls:\n",
    "            file.write(url + \"\\n\")\n",
    "\n",
    "    return new_urls\n",
    "    \n",
    "def main():\n",
    "    url_counter = 0\n",
    "    target = 100\n",
    "    \n",
    "    while url_counter < target:\n",
    "        time.sleep(random.randint(10, 60*2))\n",
    "        queries = sample_queries(queries_dict, num_samples=3)\n",
    "        modified_queries = modify_queries(queries, num_changes=random.randint(0, 3))\n",
    "        for query in modified_queries:\n",
    "            urls = search_engine_query(query, num_results=10)\n",
    "            url_counter += len(urls)\n",
    "            print(f\"Fetched URLs: {urls}\")\n",
    "        \n",
    "            # Save new URLs to file and get only the newly added ones\n",
    "            new_urls = save_new_urls(urls)\n",
    "            print(f\"New URLs: {new_urls}\")\n",
    "            \n",
    "            # Pass only new URLs to scrape_url\n",
    "            for url in new_urls:\n",
    "                scrape_url(url)\n",
    "                get_pdfs(url)\n",
    "\n",
    "try:\n",
    "    main()\n",
    "except Exception as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971a90e2-ddfe-4956-8c85-9fd39f8c6cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
