{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f814dbfe-e40f-4555-969c-058526eb7f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EstebanSoto\\Jupyter\\escarabajos\\gamma\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Add the current directory to the Python path\n",
    "sys.path.append(current_directory)\n",
    "print(current_directory)\n",
    "\n",
    "from spectraltools import Specimen_Collection, Spectrum, create_path_if_not_exists\n",
    "from metrics import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "import re\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "922f2211-60c3-42b9-9e13-a88edf9da0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This section allows the user to choose their workplace location.\n",
    "This is important if the user has multiple locations and operating systems in which this \n",
    "script is run\"\"\"\n",
    "\n",
    "#select location\n",
    "working_at = \"cicima_desktop\"\n",
    "\n",
    "#Training data is used when we are already certain of species and genera for a particular sample\n",
    "training_data_is_used = False\n",
    "\n",
    "if working_at == \"colaboratory\":\n",
    "  from google.colab import drive\n",
    "  drive.mount(\"/content/drive\")\n",
    "  #base folder\n",
    "  \"\"\"Select the location for your base folder\"\"\"\n",
    "    \n",
    "  base_folder = r\"/content/drive/My Drive/CICIMA/escarabajos_files\"\n",
    "  \n",
    "elif working_at == \"wfh\":\n",
    "\n",
    "    \"\"\"Select the location of your base folder\"\"\"\n",
    "    base_folder = r\"C:\\Users\\esteb\\cicima\\escarabajos\"\n",
    "\n",
    "elif working_at == \"cicima_desktop\":\n",
    "  \n",
    "    \"\"\"Select the location of your base folder\"\"\"\n",
    "    base_folder = r\"C:\\Users\\EstebanSoto\\Jupyter\\escarabajos\"\n",
    "\n",
    "elif working_at == \"cicima_laptop\":\n",
    "    \n",
    "    \"\"\"Select the location of your base folder\"\"\"\n",
    "    base_folder = r\"/home/vinicio/escarabajos\"\n",
    "\n",
    "#define the location of the tables with information about the collections and its parent directory\n",
    "\n",
    "collection_tables_main_path =  os.path.join(base_folder, \"L1050_data\",\"collections\")\n",
    "collection_files_main_path = os.path.join(base_folder, \"L1050_data\")\n",
    "\n",
    "# Define report location\n",
    "report_location = os.path.join(base_folder, \"reports\",\"data_analysis\")\n",
    "\n",
    "#collection_descriptor = r\"CICIMAUCR and ANGSOL\" tododelete\n",
    "\n",
    "#File location and metadata location for collection 1\n",
    "angsol_collection_path = os.path.join(collection_files_main_path,\"ANGSOL\",\"average\") \n",
    "angsol_collection_metadata = os.path.join(collection_tables_main_path,\"CICIMA-beetles-general-inventory - ANGSOL.txt\") \n",
    "\n",
    "#File location and metadata location for collection 2\n",
    "cicimaucr_collection_path = os.path.join(collection_files_main_path,r\"TRA_data_CICIMA_INBUCR\",\"CICIMAUCR\",\"reflectance\")  #listo\n",
    "cicimaucr_collection_2_path = os.path.join(collection_files_main_path,r\"CICIMA-2024-01-REFLECTANCE\",\"average\")\n",
    "cicimaucr_collection_3_path = os.path.join(collection_files_main_path,r\"CICIMA-2024-03-REFLECTANCE\",\"without iris nor lens\",\"average\")\n",
    "cicimaucr_collection_4_path = os.path.join(collection_files_main_path,r\"CICIMA-2024-05-REFLECTANCE\",\"average\")\n",
    "cicima_ucr_metadata = os.path.join(collection_tables_main_path,r\"CICIMA-beetles-general-inventory - CICIMAUCR.txt\") \n",
    "\n",
    "#File location and metadata location for collection 3\n",
    "inbucr_collection_path = os.path.join(collection_files_main_path,r\"INBUCR\",\"average\") #listo\n",
    "inbucr_metadata = os.path.join(collection_tables_main_path,r\"CICIMA-beetles-general-inventory - INBUCR.txt\") \n",
    "\n",
    "#File location and metadata location for collection 4\n",
    "bioucr_collection_path = os.path.join(collection_files_main_path,r\"BIOUCR\",\"average\")  #listo\n",
    "bioucr_metadata = os.path.join(collection_tables_main_path,r\"CICIMA-beetles-general-inventory - BIOUCR.txt\") \n",
    "\n",
    "#agregated data location, here averages and std will be saved when training data and retreived when classifying spectra\n",
    "agregated_data_location = os.path.join(base_folder, \"aggregated_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b69b7d8f-51dd-4b86-b530-0fdf2ff98f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<spectraltools.Spectrum at 0x2096103e2e0>,\n",
       " <spectraltools.Spectrum at 0x2095dc919d0>,\n",
       " <spectraltools.Spectrum at 0x2095dc918e0>,\n",
       " <spectraltools.Spectrum at 0x2095e63a040>,\n",
       " <spectraltools.Spectrum at 0x2096d02db20>,\n",
       " <spectraltools.Spectrum at 0x2096d02df40>,\n",
       " <spectraltools.Spectrum at 0x2096d02d1f0>,\n",
       " <spectraltools.Spectrum at 0x2096d02d670>,\n",
       " <spectraltools.Spectrum at 0x2096d02ddf0>,\n",
       " <spectraltools.Spectrum at 0x2096d02de20>,\n",
       " <spectraltools.Spectrum at 0x2096d037b80>,\n",
       " <spectraltools.Spectrum at 0x2096d037e80>,\n",
       " <spectraltools.Spectrum at 0x2096d037640>,\n",
       " <spectraltools.Spectrum at 0x2096d045610>,\n",
       " <spectraltools.Spectrum at 0x2096d045f70>,\n",
       " <spectraltools.Spectrum at 0x2096d045c70>,\n",
       " <spectraltools.Spectrum at 0x2096d045940>,\n",
       " <spectraltools.Spectrum at 0x2096d045e50>,\n",
       " <spectraltools.Spectrum at 0x2096d037d90>,\n",
       " <spectraltools.Spectrum at 0x2096d045f40>,\n",
       " <spectraltools.Spectrum at 0x2096d037160>,\n",
       " <spectraltools.Spectrum at 0x2096d037340>,\n",
       " <spectraltools.Spectrum at 0x2096d037bb0>,\n",
       " <spectraltools.Spectrum at 0x2096d04d1f0>,\n",
       " <spectraltools.Spectrum at 0x2096d037d00>,\n",
       " <spectraltools.Spectrum at 0x2096d0599a0>,\n",
       " <spectraltools.Spectrum at 0x2096d037850>,\n",
       " <spectraltools.Spectrum at 0x2096d037550>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"In this section we define the collections and add metadata if necessary\"\"\"\n",
    "#Collections\n",
    "angsol_collection = Specimen_Collection(\"ANGSOL\", angsol_collection_path, angsol_collection_metadata, \"HIGH\")\n",
    "angsol_collection.set_description(\"ANGSOL collection has specimens that belong to Angel Sol√≠s. The confidence that we have about specimen identification is high.\")\n",
    "\n",
    "cicimaucr_collection = Specimen_Collection(\"CICIMAUCR1\", cicimaucr_collection_path, cicima_ucr_metadata, \"HIGH\")\n",
    "cicimaucr_collection.set_description(\"\"\"Elytra measurements for\"\"\")\n",
    "cicimaucr_collection_2 = Specimen_Collection(\"CICIMAUCR2\", cicimaucr_collection_2_path, cicima_ucr_metadata, \"HIGH\")\n",
    "cicimaucr_collection_3 = Specimen_Collection(\"CICIMAUCR3\", cicimaucr_collection_3_path, cicima_ucr_metadata, \"HIGH\")\n",
    "cicimaucr_collection_3.set_description(\"\"\"The most part of CICIMA specimens belongs to this collection. Full body measurements.\"\"\")\n",
    "\n",
    "cicimaucr_collection_4 = Specimen_Collection(\"CICIMAUCR4\", cicimaucr_collection_4_path, cicima_ucr_metadata, \"HIGH\")\n",
    "cicimaucr_collection_4.set_description(\"\"\"This collection has 3 kalinini specimens which were not used in training. \n",
    "                                        These are intended to be used as test subjects\"\"\")\n",
    "\n",
    "inbucr_collection = Specimen_Collection(\"INBUCR\", inbucr_collection_path, inbucr_metadata, \"MID\")\n",
    "bioucr_collection = Specimen_Collection(\"BIOUCR\", bioucr_collection_path, bioucr_metadata, \"LOW\")\n",
    "\n",
    "collection_list = [\n",
    "                    #angsol_collection,\n",
    "                    cicimaucr_collection,\n",
    "                    cicimaucr_collection_2,\n",
    "                    cicimaucr_collection_3,\n",
    "                    #inbucr_collection,\n",
    "                    #bioucr_collection,\n",
    "                    ]\n",
    "collection_names_set = set([collection.name for collection in collection_list])\n",
    "collection_names = \" \".join( sorted(collection_names_set))\n",
    "\n",
    "prediction_list = [\n",
    "                    #angsol_collection,\n",
    "                    #cicimaucr_collection,\n",
    "                    #cicimaucr_collection_2,\n",
    "                    #cicimaucr_collection_3,\n",
    "                    cicimaucr_collection_4,\n",
    "                    #inbucr_collection,\n",
    "                    #bioucr_collection,\n",
    "                    ]\n",
    "prediction_collection_names_set = set([collection.name for collection in collection_list])\n",
    "prediction_collection_names = \" \".join( sorted(collection_names_set))\n",
    "\n",
    "\n",
    "#print(collection_names)\n",
    "#date\n",
    "from datetime import datetime\n",
    "current_date = datetime.now().date()\n",
    "\n",
    "def get_filtered_spectra(collection_list):\n",
    "\n",
    "    all_spectra = []\n",
    "    \n",
    "    for collection in collection_list:\n",
    "        all_spectra += collection.get_spectra()\n",
    "\n",
    "    all_spectra = [item for item in all_spectra if item.get_species() in [\"kalinini\", \"resplendens\", \"cupreomarginata\"]]\n",
    "    return all_spectra\n",
    "    \n",
    "def get_spectra(collection_list):\n",
    "\n",
    "    all_spectra = []\n",
    "    \n",
    "    for collection in collection_list:\n",
    "        all_spectra += collection.get_spectra()\n",
    "\n",
    "    return all_spectra  \n",
    "    \n",
    "training_spectra = get_filtered_spectra(collection_list)\n",
    "prediction_spectra  = get_spectra(prediction_list) \n",
    "\n",
    "for spectrum in prediction_spectra:\n",
    "    print(spectrum.get_species())\n",
    "\n",
    "training_spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ca2d18c-6d96-4c8c-92ed-d12ae453aa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_list(lst, filler=0):\n",
    "    metrics = lst[1]\n",
    "    \n",
    "    for metric in metrics:\n",
    "        #print(metric)\n",
    "        \n",
    "        max_length = max([len(list) for list in metrics])\n",
    "        \n",
    "        #print(max_length)\n",
    "        \n",
    "        padded_list = []\n",
    "\n",
    "        for element in metrics:\n",
    "\n",
    "            padded_sublist = element\n",
    "            \n",
    "            padded_sublist += [filler] * (max_length - len(padded_sublist))\n",
    "            padded_list.append(padded_sublist)\n",
    "            #print(padded_sublist)\n",
    "    final_list = [lst[0], (padded_list), lst[2]]\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fccf47a7-9c20-471a-bab5-1f5556c28589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3a45db8-0a26-430d-9fdc-c638f18bf2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define species list\n",
    "species_list = [\"kalinini\",\"resplendens\", \"cupreomarginata\"]\n",
    "### Training data \n",
    "scalar_metrics = [Gamma_Arbitrary_Limits,Gamma_First_Two_Peaks,Gamma_Area_Under_Curve_Naive, Gamma_Area_Under_Curve_First_Min_Cut]\n",
    "#scalar\n",
    "gamma_arbitrary_limits_data = feature_and_label_extractor(Gamma_Arbitrary_Limits, training_spectra)\n",
    "gamma_first_two_peaks_data = feature_and_label_extractor(Gamma_First_Two_Peaks, training_spectra)\n",
    "gamma_area_under_curve_data = feature_and_label_extractor(Gamma_Area_Under_Curve_Naive, training_spectra)\n",
    "gamma_area_under_curve_first_min_cut_data = feature_and_label_extractor(Gamma_Area_Under_Curve_First_Min_Cut, training_spectra)\n",
    "\n",
    "#vectorial\n",
    "vectorial_metrics = [Wavelength_Vector, Maximum_Points, Minimum_Points, Maximum_Points_Normalized, Minimum_Points_Normalized, Critical_Points ]\n",
    "\n",
    "gamma_vector_relative_reflectance_data = feature_and_label_extractor(Gamma_Vector_Relative_Reflectance, training_spectra)\n",
    "wavelength_vector_data = feature_and_label_extractor(Wavelength_Vector, training_spectra)\n",
    "critical_points_data = feature_and_label_extractor(Critical_Points, training_spectra)\n",
    "\n",
    "maximum_points_data = feature_and_label_extractor(Maximum_Points, training_spectra)\n",
    "minimum_points_data = feature_and_label_extractor(Minimum_Points, training_spectra)\n",
    "maximum_points_normalized_data =feature_and_label_extractor(Maximum_Points_Normalized, training_spectra)\n",
    "minimum_points_normalized_data =feature_and_label_extractor(Minimum_Points_Normalized, training_spectra)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1c8158fc-31bc-497f-a63a-aec9da99fb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_aggregate(data):\n",
    "    \n",
    "    length = len(data[0])\n",
    "\n",
    "    data_points = []\n",
    "\n",
    "    #for each specimen\n",
    "    \n",
    "    for i in range(0, length): \n",
    "        code = data[0][i]\n",
    "        vector = data[1][i]\n",
    "        species =data[2][i]\n",
    "\n",
    "        data_point = {}\n",
    "        data_point[\"code\"] = code\n",
    "        data_point[\"vector\"] = vector\n",
    "        data_point[\"species\"] = species\n",
    "\n",
    "        data_points.append(data_point)\n",
    "\n",
    "    #Now, for each species \n",
    "    aggregates = {}\n",
    "    \n",
    "    for species in [\"kalinini\", \"resplendens\", \"cupreomarginata\"]:\n",
    "        \n",
    "        specimens = [x for x in data_points if x[\"species\"] == species]\n",
    "\n",
    "        #extract vectors\n",
    "        vectors = [element[\"vector\"] for element in specimens]\n",
    "\n",
    "        #first entry\n",
    "        vector_first_entries =  [element[0] for element in vectors]\n",
    "        vector_second_entries =  [element[1] for element in vectors]\n",
    "        \n",
    "        #get max length of \n",
    "        #print(type(vector_first_entries[0]))\n",
    "        if not isinstance((vector_first_entries[0]), np.float64):\n",
    "            max_length_x = max([len(x) for x in vector_first_entries])\n",
    "            max_length_y = max([len(x) for x in vector_second_entries])\n",
    "        else:\n",
    "            max_length_x = 1\n",
    "            max_length_y = 1\n",
    "        \n",
    "        #get number of vectors\n",
    "        number_of_specimens = len(specimens)\n",
    "        \n",
    "        #add zeroes\n",
    "        #for first entry\n",
    "        \n",
    "        new_subset_vectors = []\n",
    "        \n",
    "        new_vector_first_entry = []\n",
    "        for first_entry_i in vector_first_entries:\n",
    "            #for the first and second entry\n",
    "            if isinstance(first_entry_i, np.float64):\n",
    "                length_first_entry = 1\n",
    "                number_of_zeroes = max_length_x - length_first_entry\n",
    "                extend_vector = np.array([0]*number_of_zeroes)\n",
    "                #print(extend_vector)\n",
    "                #print(first_entry_i)\n",
    "                first_entry_i = np.concatenate(([first_entry_i], extend_vector))\n",
    "                new_vector_first_entry.append(first_entry_i)\n",
    "            else:\n",
    "                length_first_entry = len(first_entry_i)\n",
    "                number_of_zeroes = max_length_x - length_first_entry\n",
    "                extend_vector = np.array([0]*number_of_zeroes)\n",
    "                #print(extend_vector)\n",
    "                #print(first_entry_i)\n",
    "                first_entry_i = np.concatenate((first_entry_i, extend_vector))\n",
    "                new_vector_first_entry.append(first_entry_i)\n",
    "\n",
    "        new_vector_second_entry =  []\n",
    "        \n",
    "        for second_entry_i in vector_second_entries:\n",
    "            #for the first and second entry\n",
    "            if isinstance(second_entry_i, np.float64):\n",
    "                length_second_entry = 1\n",
    "                number_of_zeroes = max_length_x - length_second_entry\n",
    "                extend_vector = np.array([0]*number_of_zeroes)\n",
    "                second_entry_i = np.concatenate(([second_entry_i], extend_vector))\n",
    "                new_vector_second_entry.append(second_entry_i)\n",
    "            else:\n",
    "                length_second_entry = len(second_entry_i)\n",
    "                number_of_zeroes = max_length_x - length_second_entry\n",
    "                extend_vector = np.array([0]*number_of_zeroes)\n",
    "                second_entry_i = np.concatenate((second_entry_i, extend_vector))\n",
    "                new_vector_second_entry.append(second_entry_i)\n",
    "        #print(f\"{new_vector_first_entry=}\")\n",
    "        #print(f\"{new_vector_second_entry=}\")\n",
    "\n",
    "        #now calculate averages \n",
    "\n",
    "        x_averages = []\n",
    "        x_std = []\n",
    "        for i in range(max_length_x): \n",
    "            vector_i = []\n",
    "\n",
    "            for n in range(0, number_of_specimens):\n",
    "                value_n = new_vector_first_entry[n][i]\n",
    "                if not( (value_n < 0.1) & (value_n > -0.1) ) : #if value is not zero\n",
    "                    vector_i.append(value_n)\n",
    "            #then get the total of elements, convert it into a numpy array , calculate the average. \n",
    "            x_averages.append(np.mean(np.array(vector_i)))\n",
    "            x_std.append(np.std(np.array(vector_i)))\n",
    "            \n",
    "        y_averages =[]\n",
    "        y_std = []\n",
    "        \n",
    "        for i in range(max_length_y): \n",
    "            vector_x = []\n",
    "            for n in range(0, number_of_specimens):\n",
    "                value_n = new_vector_second_entry[n][i]\n",
    "                if not( (value_n < 0.1) & (value_n > -0.1) ) : #if value is not zero\n",
    "                    vector_x.append(value_n)\n",
    "            #then get the total of elements, convert it into a numpy array , calculate the average. \n",
    "            y_averages.append(np.mean(np.array(vector_x)))\n",
    "            y_std.append(np.std(np.array(vector_x)))\n",
    "        \n",
    "        info = np.array( [x_averages, y_averages, x_std, y_std]).T\n",
    "        df = pd.DataFrame(info, columns= [f\"{species}_x_avg\", f\"{species}_y_avg\", f\"{species}_x_std\", f\"{species}_y_std\"])\n",
    "        aggregates[species] = df\n",
    "        #print(aggregates)\n",
    "    df_2 = pd.DataFrame([])\n",
    "    for element in aggregates:\n",
    "        df = (aggregates[element])\n",
    "        df_2 = pd.concat([df, df_2], axis=1)\n",
    "        #print(df_2)\n",
    "    return df_2\n",
    "    \n",
    "\n",
    "def save_vector_aggregate(metric_class, spectra, agregated_data_location):\n",
    "    #get metric values for spectra\n",
    "    data = feature_and_label_extractor(metric_class, spectra)\n",
    "    #create vector aggregate df\n",
    "    df = vector_aggregate(data)  \n",
    "    #create path location\n",
    "    path_location = os.path.join(agregated_data_location, \"metric_avg_std\")\n",
    "    create_path_if_not_exists(path_location)\n",
    "    path_and_filename = os.path.join( path_location, f'{metric_class.get_name()}')\n",
    "    #save to csv\n",
    "    df.to_csv( path_and_filename, index=False, sep = \"\\t\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a4aec18-1158-4502-a4b2-bf5b538a564c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'C:\\Users\\EstebanSoto\\Jupyter\\escarabajos\\aggregated_data\\metric_avg_std' already exists.\n",
      "Directory 'C:\\Users\\EstebanSoto\\Jupyter\\escarabajos\\aggregated_data\\metric_avg_std' already exists.\n",
      "Directory 'C:\\Users\\EstebanSoto\\Jupyter\\escarabajos\\aggregated_data\\metric_avg_std' already exists.\n",
      "Directory 'C:\\Users\\EstebanSoto\\Jupyter\\escarabajos\\aggregated_data\\metric_avg_std' already exists.\n",
      "Directory 'C:\\Users\\EstebanSoto\\Jupyter\\escarabajos\\aggregated_data\\metric_avg_std' already exists.\n",
      "Directory 'C:\\Users\\EstebanSoto\\Jupyter\\escarabajos\\aggregated_data\\metric_avg_std' already exists.\n"
     ]
    }
   ],
   "source": [
    "for metric in vectorial_metrics: \n",
    "    save_vector_aggregate(metric, training_spectra, agregated_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9ba06b25-64e0-49cf-83fe-65fec67eee74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'C:\\Users\\EstebanSoto\\Jupyter\\escarabajos\\aggregated_data\\metric_avg_std' already exists.\n",
      "Directory 'C:\\Users\\EstebanSoto\\Jupyter\\escarabajos\\aggregated_data\\metric_avg_std' already exists.\n",
      "Directory 'C:\\Users\\EstebanSoto\\Jupyter\\escarabajos\\aggregated_data\\metric_avg_std' already exists.\n",
      "Directory 'C:\\Users\\EstebanSoto\\Jupyter\\escarabajos\\aggregated_data\\metric_avg_std' already exists.\n",
      "Directory 'C:\\Users\\EstebanSoto\\Jupyter\\escarabajos\\aggregated_data\\metric_avg_std' already exists.\n"
     ]
    }
   ],
   "source": [
    "#Save averages and std for training data\n",
    "metrics_list = [Gamma_Arbitrary_Limits, Gamma_First_Two_Peaks, Gamma_Area_Under_Curve_Naive, Gamma_Area_Under_Curve_First_Min_Cut, \n",
    "                Gamma_Area_Under_Curve_First_Min_Cut]\n",
    "\n",
    "for metric in metrics_list:\n",
    "    save_aggregated_data(metric,training_spectra,agregated_data_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aef85ed7-7afc-459d-ba90-9ff83f0e318c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Critical_Points', 'Gamma_Arbitrary_Limits.txt', 'gamma_area_under_curve_cut_first_minimum.txt', 'Gamma_Area_Under_Curve_First_Min_Cut.txt', 'Gamma_Area_Under_Curve_Naive.txt', 'Gamma_First_Two_Peaks.txt', 'Maximum_Points', 'Maximum_Points_Normalized', 'Minimum_Points', 'Minimum_Points_Normalized', 'Wavelength_Vector']\n"
     ]
    }
   ],
   "source": [
    "#read averages and std\n",
    "avg_std_location = os.path.join(agregated_data_location, \"metric_avg_std\")\n",
    "dfs = read_aggregated_data(avg_std_location)\n",
    "#dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a84c93-2744-482d-94e0-3a4f775f7e2c",
   "metadata": {},
   "source": [
    "#get metrics keys\n",
    "metric_keys = (dfs.keys())\n",
    "print(metric_keys)\n",
    "#get avg and std dataframe\n",
    "avg_std_df_x = dfs[\"Maximum_Points\"]\n",
    "#print(avg_df)\n",
    "species_list = [\"kalinini\", \"resplendens\", \"cupreomarginata\"]\n",
    "spectrum_peaks = np.array(prediction_spectra[1].get_maxima())\n",
    "n = 5\n",
    "#print(spectrum)\n",
    "similarity_index(spectrum_peaks, avg_std_df_x, species_list, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "11d9a3e4-0901-41b1-b67e-1a08e29d7950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(df):\n",
    "    #print(f\"{species =} \\n {df=}\" )\n",
    "    \"\"\"Accuracy: Percentage of correct guesses. \"\"\"\n",
    "    #filter all predictions of that particular species\n",
    "    \n",
    "    total_of_predictions = df[\"prediction\"].count()\n",
    "    #print(f\"{total_of_predictions=}\" )\n",
    "    #Now count the amount of actual species\n",
    "    correct_predictions = df[df[\"species\"] == df[\"prediction\"]][\"species\"].count()\n",
    "    #print(f\"{correct_predictions=}\" )\n",
    "    #print(f\"{total_of_predictions=}\" )\n",
    "    accuracy = correct_predictions/total_of_predictions*100\n",
    "    #print(f\"{accuracy=}\" )\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6cb78919-d3db-440d-bda6-dd2ffcbc1aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision(df, species):\n",
    "    #print(f\"{species =} \\n {df=}\" )\n",
    "    \"\"\"Precision: For a given prediction, what percentage is actually for that species. \"\"\"\n",
    "    #filter all predictions of that particular species\n",
    "    filtered_df = df[df[\"prediction\"] == species]\n",
    "    total_of_predictions = filtered_df[\"prediction\"].count()\n",
    "    #print(f\"{total_of_predictions=}\" )\n",
    "    #Now count the amount of actual species\n",
    "    \n",
    "    actual_species = filtered_df[filtered_df[\"species\"] == species][\"species\"].count()\n",
    "    precision = actual_species/total_of_predictions*100\n",
    "    #print(f\"{actual_species=}\" )\n",
    "    #print(f\"{total_of_predictions=}\" )\n",
    "    \n",
    "    #print(f\"{precision=}\" )\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3276d2d2-3bdf-48f6-8b8f-95088968fd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall(df, species):\n",
    "    #print(f\"{species =} \\n {df=}\" )\n",
    "    \"\"\"recall: For a given species, what percentage was correctly characterized. \"\"\"\n",
    "    #filter all lines of a particular species\n",
    "    filtered_df = df[df[\"species\"] == species]\n",
    "    total_of_species = filtered_df[\"species\"].count()\n",
    "    #print(f\"{total_of_species=}\" )\n",
    "    #Count the amount of correct predictions\n",
    "    correct_predictions = filtered_df[filtered_df[\"prediction\"] == species][\"prediction\"].count()\n",
    "    recall = correct_predictions/total_of_species*100\n",
    "    #print(f\"{recall=}\" )\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40e80b45-ff5c-4ee4-9291-bd9b9e7fd527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_index(spectrum, metric, aggregate_data_df , species_list, n):\n",
    "    avg_std_df = aggregate_data_df\n",
    "    spectrum_peaks = metric(spectrum).get_metric_value()\n",
    "    \n",
    "    def distance(x, x0, sigma_0):\n",
    "        \n",
    "        x = float(x)\n",
    "        x0 = float(x0[0])\n",
    "        sigma_0 = float(sigma_0[0])\n",
    "\n",
    "        #numerator = (np.abs(x - x0))\n",
    "        #denominator = float(sigma_0 + np.abs(x - x0) +0.1)\n",
    "        numerator = ((x - x0)**2)\n",
    "        denominator = 1\n",
    "        \n",
    "        try:\n",
    "            index = numerator/denominator\n",
    "        except:\n",
    "            return 0\n",
    "        return index\n",
    "        #def get_gamma_factor(self, spectrum):\n",
    "    \n",
    "    #get first n peaks of the gamma\n",
    "    x = spectrum_peaks[0][0:n]\n",
    "    y = spectrum_peaks[1][0:n]\n",
    "\n",
    "    #for each species\n",
    "    similarity_index_ret = {} #define return dict\n",
    "    \n",
    "    for spec in species_list:\n",
    "        #print(spec)\n",
    "        \n",
    "        #print(averages_df.columns)\n",
    "        x_avg_column_names = [name for name in (avg_std_df.columns) if (\"avg\" in name) and (spec in name) and (\"x\" in name) ]\n",
    "        y_avg_column_names = [name for name in (avg_std_df.columns) if (\"avg\" in name) and (spec in name) and (\"y\" in name) ]\n",
    "        #print(x_avg_column_names)\n",
    "\n",
    "        x_std_column_names = [name for name in avg_std_df.columns if (\"std\" in name) and (spec in name) and (\"x\" in name)]\n",
    "        y_std_column_names = [name for name in avg_std_df.columns if (\"std\" in name) and (spec in name) and (\"y\" in name)]\n",
    "        #print(x_std_column_names)\n",
    "\n",
    "        #load species average values\n",
    "\n",
    "        x_avg = (avg_std_df[x_avg_column_names].dropna().values[0:n])\n",
    "        y_avg = avg_std_df[y_avg_column_names].dropna().values[0:n]\n",
    "\n",
    "        #load species std values:\n",
    "        x_std = avg_std_df[x_std_column_names].dropna().values[0:n]\n",
    "        y_std = avg_std_df[y_std_column_names].dropna().values[0:n]\n",
    "\n",
    "        #for each peak, calculate the distances to the averages x values\n",
    "        similarity_index_x_species = 0.0\n",
    "        similarity_index_y_species = 0.0\n",
    "        similarity_index_z_species = 0.0\n",
    "        #calculate similarity_index for \n",
    "        \n",
    "      \n",
    "        #print(x, x_avg, x_std)\n",
    "        for n_i ,n_0, sigma_n_0 in zip(x, x_avg, x_std):\n",
    "            similarity_index_x_species += distance(n_i, n_0, sigma_n_0)\n",
    "            #similarity_index_x_species += 1/distance(n_i, n_0, sigma_n_0)\n",
    "            \n",
    "            \n",
    "        #calculate similarity_index for y\n",
    "        for n_i ,n_0, sigma_n_0 in zip(y, y_avg, y_std):\n",
    "            similarity_index_y_species += distance(n_i, n_0, sigma_n_0)\n",
    "            #similarity_index_y_species+= 1/distance(n_i, n_0, sigma_n_0)\n",
    "\n",
    "        #calculate similarity_index for z**2 = x**2 + y**2\n",
    "        for x_i, x_0, y_i, y_0 in zip(x, x_avg, y, y_avg):\n",
    "            similarity_index_z_species += ((x_i-x_0)**2 + (y_i-y_0)**2)\n",
    "            #similarity_index_y_species+= 1/distance(n_i, n_0, sigma_n_0)\n",
    "        \n",
    "        #arithmetic average\n",
    "        similarity_index_x_species = similarity_index_x_species*(1/ float(n) )\n",
    "        similarity_index_y_species = similarity_index_y_species*(1/ float(n) )\n",
    "        similarity_index_z_species = similarity_index_z_species*(1/ float(n) )\n",
    "        \n",
    "        #geometric average\n",
    "        #similarity_index_x_species = similarity_index_x_species**(1/ float(n) )\n",
    "        #similarity_index_y_species = similarity_index_y_species**(1/ float(n) )\n",
    "        \n",
    "        #Calculate harmonic average\n",
    "        #similarity_index_x_species = 1/similarity_index_x_species\n",
    "        #similarity_index_y_species = 1/similarity_index_y_species\n",
    "\n",
    "        #add to dictionary\n",
    "        similarity_index_ret[spec] =[similarity_index_x_species, similarity_index_y_species,similarity_index_z_species]\n",
    "  \n",
    "    return similarity_index_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "314f1aec-ef55-4d21-bf11-dead38e9eff2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'avg_std_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m spectrum_i \u001b[38;5;129;01min\u001b[39;00m training_spectra:\n\u001b[1;32m----> 2\u001b[0m     similarity_index_df_xy \u001b[38;5;241m=\u001b[39m similarity_index(spectrum \u001b[38;5;241m=\u001b[39m spectrum_i, metric \u001b[38;5;241m=\u001b[39m Critical_Points, aggregate_data_df \u001b[38;5;241m=\u001b[39m \u001b[43mavg_std_df\u001b[49m , species_list \u001b[38;5;241m=\u001b[39m species_list, n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'avg_std_df' is not defined"
     ]
    }
   ],
   "source": [
    "for spectrum_i in training_spectra:\n",
    "    similarity_index_df_xy = similarity_index(spectrum = spectrum_i, metric = Critical_Points, aggregate_data_df = avg_std_df , species_list = species_list, n = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "be8cbf28-4422-4095-af2a-5402c3d8608d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorial_manual_classifier_1(spectrum, avg_std_df, metric_class, species_list, type_of_sim_index = 1):\n",
    "    #get metrics keys\n",
    "    keys = (avg_std_df.keys())\n",
    "    #print(keys)\n",
    "    \n",
    "    metric_name = metric_class.get_name()\n",
    "    #print(metric_name)\n",
    "    #get avg and std dataframe\n",
    "\n",
    "    #print(metric_name)\n",
    "    avg_std_df_metric = avg_std_df[metric_name]\n",
    "    #print(avg_std_df)\n",
    "    \n",
    "    #print(avg_df)\"\n",
    "    spectrum_peaks = np.array(metric_class(spectrum).metric_value)\n",
    "    #print(spectrum_peaks)\n",
    "    n = 5\n",
    "    #print(spectrum)\n",
    "    sim_index = similarity_index(spectrum_peaks, avg_std_df_metric, species_list, n)\n",
    "\n",
    "    for element in sim_index:\n",
    "        if type_of_sim_index == 0:\n",
    "            sim_index[element] = sim_index[element][0] \n",
    "        elif type_of_sim_index == 1:\n",
    "            sim_index[element] = sim_index[element][1] \n",
    "        elif type_of_sim_index == 2:\n",
    "            sim_index[element] = sim_index[element][1] *sim_index[element][0]\n",
    "        elif type_of_sim_index == 3:\n",
    "            sim_index[element] = sim_index[element][2]\n",
    "\n",
    "    min_key = min(sim_index, key=sim_index.get)\n",
    "    return min_key\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "db8cc282-278a-49c5-a45a-a99036cdb6d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Critical_Points', 'Gamma_Arbitrary_Limits.txt', 'gamma_area_under_curve_cut_first_minimum.txt', 'Gamma_Area_Under_Curve_First_Min_Cut.txt', 'Gamma_Area_Under_Curve_Naive.txt', 'Gamma_First_Two_Peaks.txt', 'Maximum_Points', 'Maximum_Points_Normalized', 'Minimum_Points', 'Minimum_Points_Normalized', 'Wavelength_Vector']\n"
     ]
    }
   ],
   "source": [
    "def get_stats_for_metric(metric_list, spectra, avg_std_df, type_of_sim_index = 1 ):\n",
    "    real_species = ([item.species for item in spectra]) #List of the correct species\n",
    "\n",
    "    return_df = pd.DataFrame([])\n",
    "    \n",
    "    for metric_class in metric_list:\n",
    "        #get metric name\n",
    "        metric_name = metric_class.get_name()\n",
    "        #print(metric_name)\n",
    "        #get metric predictions\n",
    "        \n",
    "        predictions = [vectorial_manual_classifier_1(item, avg_std_df, metric_class, species_list,type_of_sim_index) for item in spectra]\n",
    "        #print(f\"{predictions=}\")\n",
    "        #predictions = [item for item in spectra ]\n",
    "        \n",
    "        #create dataframe with predictions and real species \n",
    "        species_and_predictions_df = pd.DataFrame(np.array([real_species, predictions]).T, columns=[\"species\",\"prediction\"] ) \n",
    "        \n",
    "        stats = {}\n",
    "        stats[\"metric\"] = metric_name\n",
    "        #calculate accuracy\n",
    "        accuracy = calculate_accuracy(species_and_predictions_df)\n",
    "        stats[\"accuracy\"] = accuracy\n",
    "        \n",
    "        #for each species, get column names and recall and precision\n",
    "        for species in species_list:\n",
    "            stats[f\"{species}_recall\"] =calculate_recall(species_and_predictions_df, species)\n",
    "            stats[f\"{species}_precision\"] =calculate_precision(species_and_predictions_df, species)\n",
    "        #save stats\n",
    "        #create empty dataframe\n",
    "        stats_df = pd.DataFrame([stats]) \n",
    "        #print(stats_df)\n",
    "        return_df = pd.concat([return_df,stats_df]) \n",
    "    #print(return_df)\n",
    "    return return_df\n",
    "avg_std_df= read_aggregated_data(avg_std_location)\n",
    "met_list = [Critical_Points, Maximum_Points, Maximum_Points_Normalized, Minimum_Points, Minimum_Points_Normalized]\n",
    "#met_list = [Critical_Points]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3112c8a3-7d00-46d1-9e2e-7b5149ea73ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EstebanSoto\\AppData\\Local\\Temp\\ipykernel_23824\\4184302347.py:11: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  precision = actual_species/total_of_predictions*100\n",
      "C:\\Users\\EstebanSoto\\AppData\\Local\\Temp\\ipykernel_23824\\4184302347.py:11: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  precision = actual_species/total_of_predictions*100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>kalinini_recall</th>\n",
       "      <th>kalinini_precision</th>\n",
       "      <th>resplendens_recall</th>\n",
       "      <th>resplendens_precision</th>\n",
       "      <th>cupreomarginata_recall</th>\n",
       "      <th>cupreomarginata_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Critical_Points</td>\n",
       "      <td>64.285714</td>\n",
       "      <td>77.777778</td>\n",
       "      <td>58.333333</td>\n",
       "      <td>54.545455</td>\n",
       "      <td>75.00</td>\n",
       "      <td>62.5</td>\n",
       "      <td>62.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Maximum_Points</td>\n",
       "      <td>46.428571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>63.636364</td>\n",
       "      <td>43.75</td>\n",
       "      <td>75.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Maximum_Points_Normalized</td>\n",
       "      <td>46.428571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>63.636364</td>\n",
       "      <td>43.75</td>\n",
       "      <td>75.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Minimum_Points</td>\n",
       "      <td>42.857143</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>45.454545</td>\n",
       "      <td>50.00</td>\n",
       "      <td>37.5</td>\n",
       "      <td>37.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Minimum_Points_Normalized</td>\n",
       "      <td>42.857143</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>45.454545</td>\n",
       "      <td>50.00</td>\n",
       "      <td>37.5</td>\n",
       "      <td>37.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      metric   accuracy  kalinini_recall  kalinini_precision  \\\n",
       "0            Critical_Points  64.285714        77.777778           58.333333   \n",
       "0             Maximum_Points  46.428571         0.000000                 NaN   \n",
       "0  Maximum_Points_Normalized  46.428571         0.000000                 NaN   \n",
       "0             Minimum_Points  42.857143        44.444444           40.000000   \n",
       "0  Minimum_Points_Normalized  42.857143        44.444444           40.000000   \n",
       "\n",
       "   resplendens_recall  resplendens_precision  cupreomarginata_recall  \\\n",
       "0           54.545455                  75.00                    62.5   \n",
       "0           63.636364                  43.75                    75.0   \n",
       "0           63.636364                  43.75                    75.0   \n",
       "0           45.454545                  50.00                    37.5   \n",
       "0           45.454545                  50.00                    37.5   \n",
       "\n",
       "   cupreomarginata_precision  \n",
       "0                       62.5  \n",
       "0                       50.0  \n",
       "0                       50.0  \n",
       "0                       37.5  \n",
       "0                       37.5  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(get_stats_for_metric(met_list, training_spectra, avg_std_df , type_of_sim_index = 0 ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6a54e2f-38ba-4944-a178-8111102a98df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>kalinini_recall</th>\n",
       "      <th>kalinini_precision</th>\n",
       "      <th>resplendens_recall</th>\n",
       "      <th>resplendens_precision</th>\n",
       "      <th>cupreomarginata_recall</th>\n",
       "      <th>cupreomarginata_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Critical_Points</td>\n",
       "      <td>67.857143</td>\n",
       "      <td>55.555556</td>\n",
       "      <td>83.333333</td>\n",
       "      <td>72.727273</td>\n",
       "      <td>61.538462</td>\n",
       "      <td>75.0</td>\n",
       "      <td>66.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Maximum_Points</td>\n",
       "      <td>67.857143</td>\n",
       "      <td>55.555556</td>\n",
       "      <td>71.428571</td>\n",
       "      <td>72.727273</td>\n",
       "      <td>61.538462</td>\n",
       "      <td>75.0</td>\n",
       "      <td>75.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Maximum_Points_Normalized</td>\n",
       "      <td>57.142857</td>\n",
       "      <td>88.888889</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>18.181818</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>75.0</td>\n",
       "      <td>46.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Minimum_Points</td>\n",
       "      <td>53.571429</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>63.636364</td>\n",
       "      <td>58.333333</td>\n",
       "      <td>62.5</td>\n",
       "      <td>45.454545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Minimum_Points_Normalized</td>\n",
       "      <td>35.714286</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>36.363636</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>75.0</td>\n",
       "      <td>35.294118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      metric   accuracy  kalinini_recall  kalinini_precision  \\\n",
       "0            Critical_Points  67.857143        55.555556           83.333333   \n",
       "0             Maximum_Points  67.857143        55.555556           71.428571   \n",
       "0  Maximum_Points_Normalized  57.142857        88.888889           80.000000   \n",
       "0             Minimum_Points  53.571429        33.333333           60.000000   \n",
       "0  Minimum_Points_Normalized  35.714286         0.000000            0.000000   \n",
       "\n",
       "   resplendens_recall  resplendens_precision  cupreomarginata_recall  \\\n",
       "0           72.727273              61.538462                    75.0   \n",
       "0           72.727273              61.538462                    75.0   \n",
       "0           18.181818              40.000000                    75.0   \n",
       "0           63.636364              58.333333                    62.5   \n",
       "0           36.363636              50.000000                    75.0   \n",
       "\n",
       "   cupreomarginata_precision  \n",
       "0                  66.666667  \n",
       "0                  75.000000  \n",
       "0                  46.153846  \n",
       "0                  45.454545  \n",
       "0                  35.294118  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(get_stats_for_metric(met_list, training_spectra, avg_std_df , type_of_sim_index = 1 ))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1dce7994-ea31-4b70-8b15-00e461013f08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>kalinini_recall</th>\n",
       "      <th>kalinini_precision</th>\n",
       "      <th>resplendens_recall</th>\n",
       "      <th>resplendens_precision</th>\n",
       "      <th>cupreomarginata_recall</th>\n",
       "      <th>cupreomarginata_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Critical_Points</td>\n",
       "      <td>78.571429</td>\n",
       "      <td>88.888889</td>\n",
       "      <td>88.888889</td>\n",
       "      <td>72.727273</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>75.0</td>\n",
       "      <td>66.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Maximum_Points</td>\n",
       "      <td>57.142857</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>63.636364</td>\n",
       "      <td>53.846154</td>\n",
       "      <td>75.0</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Maximum_Points_Normalized</td>\n",
       "      <td>64.285714</td>\n",
       "      <td>55.555556</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>63.636364</td>\n",
       "      <td>63.636364</td>\n",
       "      <td>75.0</td>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Minimum_Points</td>\n",
       "      <td>39.285714</td>\n",
       "      <td>22.222222</td>\n",
       "      <td>28.571429</td>\n",
       "      <td>45.454545</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>50.0</td>\n",
       "      <td>36.363636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Minimum_Points_Normalized</td>\n",
       "      <td>39.285714</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>36.363636</td>\n",
       "      <td>36.363636</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>37.5</td>\n",
       "      <td>37.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      metric   accuracy  kalinini_recall  kalinini_precision  \\\n",
       "0            Critical_Points  78.571429        88.888889           88.888889   \n",
       "0             Maximum_Points  57.142857        33.333333          100.000000   \n",
       "0  Maximum_Points_Normalized  64.285714        55.555556          100.000000   \n",
       "0             Minimum_Points  39.285714        22.222222           28.571429   \n",
       "0  Minimum_Points_Normalized  39.285714        44.444444           36.363636   \n",
       "\n",
       "   resplendens_recall  resplendens_precision  cupreomarginata_recall  \\\n",
       "0           72.727273              80.000000                    75.0   \n",
       "0           63.636364              53.846154                    75.0   \n",
       "0           63.636364              63.636364                    75.0   \n",
       "0           45.454545              50.000000                    50.0   \n",
       "0           36.363636              44.444444                    37.5   \n",
       "\n",
       "   cupreomarginata_precision  \n",
       "0                  66.666667  \n",
       "0                  50.000000  \n",
       "0                  50.000000  \n",
       "0                  36.363636  \n",
       "0                  37.500000  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(get_stats_for_metric(met_list, training_spectra, avg_std_df , type_of_sim_index = 2 ) ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbea8c4f-fc18-4899-afed-6034143e621b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EstebanSoto\\AppData\\Local\\Temp\\ipykernel_23824\\4184302347.py:11: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  precision = actual_species/total_of_predictions*100\n",
      "C:\\Users\\EstebanSoto\\AppData\\Local\\Temp\\ipykernel_23824\\4184302347.py:11: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  precision = actual_species/total_of_predictions*100\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>metric</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>kalinini_recall</th>\n",
       "      <th>kalinini_precision</th>\n",
       "      <th>resplendens_recall</th>\n",
       "      <th>resplendens_precision</th>\n",
       "      <th>cupreomarginata_recall</th>\n",
       "      <th>cupreomarginata_precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Critical_Points</td>\n",
       "      <td>67.857143</td>\n",
       "      <td>88.888889</td>\n",
       "      <td>61.538462</td>\n",
       "      <td>54.545455</td>\n",
       "      <td>85.714286</td>\n",
       "      <td>62.5</td>\n",
       "      <td>62.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Maximum_Points</td>\n",
       "      <td>46.428571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>63.636364</td>\n",
       "      <td>43.750000</td>\n",
       "      <td>75.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Maximum_Points_Normalized</td>\n",
       "      <td>46.428571</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>63.636364</td>\n",
       "      <td>43.750000</td>\n",
       "      <td>75.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Minimum_Points</td>\n",
       "      <td>42.857143</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>45.454545</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>37.5</td>\n",
       "      <td>37.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Minimum_Points_Normalized</td>\n",
       "      <td>42.857143</td>\n",
       "      <td>44.444444</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>45.454545</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>37.5</td>\n",
       "      <td>37.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      metric   accuracy  kalinini_recall  kalinini_precision  \\\n",
       "0            Critical_Points  67.857143        88.888889           61.538462   \n",
       "0             Maximum_Points  46.428571         0.000000                 NaN   \n",
       "0  Maximum_Points_Normalized  46.428571         0.000000                 NaN   \n",
       "0             Minimum_Points  42.857143        44.444444           40.000000   \n",
       "0  Minimum_Points_Normalized  42.857143        44.444444           40.000000   \n",
       "\n",
       "   resplendens_recall  resplendens_precision  cupreomarginata_recall  \\\n",
       "0           54.545455              85.714286                    62.5   \n",
       "0           63.636364              43.750000                    75.0   \n",
       "0           63.636364              43.750000                    75.0   \n",
       "0           45.454545              50.000000                    37.5   \n",
       "0           45.454545              50.000000                    37.5   \n",
       "\n",
       "   cupreomarginata_precision  \n",
       "0                       62.5  \n",
       "0                       50.0  \n",
       "0                       50.0  \n",
       "0                       37.5  \n",
       "0                       37.5  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(get_stats_for_metric(met_list, training_spectra, avg_std_df , type_of_sim_index = 3 ) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bbf964-9252-4df4-805a-f88fd8067104",
   "metadata": {},
   "source": [
    "## Metric: Maximum_Points_Normalized\n",
    "### Just wavelength\n",
    "#### first metric:  ((x - x0)**2) arith avg\n",
    "accuracy=46.42857142857143\n",
    "kalinini\n",
    "recall=0.0\n",
    "precision=nan\n",
    "\n",
    "\n",
    "resplendens\n",
    "recall=63.63636363636363\n",
    "precision=43.75\n",
    "\n",
    "\n",
    "cupreomarginata\n",
    "recall=75.0\n",
    "precision=50.0\n",
    "\n",
    "\n",
    "### Just reflectance\n",
    "#### first metric:  ((x - x0)**2) arith avg\n",
    "accuracy=57.14285714285714\n",
    "kalinini\n",
    "recall=88.88888888888889\n",
    "precision=80.0\n",
    "\n",
    "\n",
    "resplendens\n",
    "recall=18.181818181818183\n",
    "precision=40.0\n",
    "\n",
    "\n",
    "cupreomarginata\n",
    "recall=75.0\n",
    "precision=46.15384615384615\n",
    "#### second metric: ((x - x0)**2) geo avg\n",
    "accuracy=32.142857142857146\n",
    "kalinini\n",
    "recall=100.0\n",
    "precision=32.142857142857146\n",
    "\n",
    "\n",
    "resplendens\n",
    "recall=0.0\n",
    "precision=nan\n",
    "\n",
    "\n",
    "cupreomarginata\n",
    "recall=0.0\n",
    "precision=nan\n",
    "#### third metric:  numerator = (np.abs(x - x0)) geometric avg\n",
    "        denominator = float(sigma_0 + np.abs(x - x0) +0.1)\n",
    "\n",
    "accuracy=32.142857142857146\n",
    "kalinini\n",
    "recall=100.0\n",
    "precision=32.142857142857146\n",
    "\n",
    "\n",
    "resplendens\n",
    "recall=0.0\n",
    "precision=nan\n",
    "\n",
    "\n",
    "cupreomarginata\n",
    "recall=0.0\n",
    "precision=nan\n",
    "\n",
    "\n",
    "### Multipliying wavelength and reflectance\n",
    "#### first metric:  \n",
    "numerator = float(np.abs(x - x0))\n",
    "denominator = float(sigma_0 + np.abs(x - x0) +1)\n",
    "\n",
    "accuracy=50.0\n",
    "kalinini\n",
    "recall=0.0\n",
    "precision=nan\n",
    "\n",
    "\n",
    "resplendens\n",
    "recall=90.9090909090909\n",
    "precision=43.47826086956522\n",
    "\n",
    "\n",
    "cupreomarginata\n",
    "recall=50.0\n",
    "precision=80.0\n",
    "\n",
    "\n",
    "#### second metric ((x - x0)**2) arith avg (mejores)\n",
    "\n",
    "accuracy=64.28571428571429\n",
    "kalinini\n",
    "recall=55.55555555555556\n",
    "precision=100.0\n",
    "\n",
    "\n",
    "resplendens\n",
    "recall=63.63636363636363\n",
    "precision=63.63636363636363\n",
    "\n",
    "\n",
    "cupreomarginata\n",
    "recall=75.0\n",
    "precision=50.0\n",
    "\n",
    "\n",
    "\n",
    "#### third metric ((x - x0)**2) geometric avg\n",
    "\n",
    "kalinini\n",
    "accuracy=53.57142857142857\n",
    "recall=22.22222222222222\n",
    "precision=66.66666666666666\n",
    "resplendens\n",
    "accuracy=53.57142857142857\n",
    "recall=72.72727272727273\n",
    "precision=47.05882352941176\n",
    "cupreomarginata\n",
    "accuracy=53.57142857142857\n",
    "recall=62.5\n",
    "precision=62.5\n",
    "\n",
    "#### fourth metric abs((x - x0)) arith avg\n",
    "\n",
    "kalinini\n",
    "accuracy=53.57142857142857\n",
    "recall=22.22222222222222\n",
    "precision=66.66666666666666\n",
    "resplendens\n",
    "accuracy=53.57142857142857\n",
    "recall=72.72727272727273\n",
    "precision=47.05882352941176\n",
    "cupreomarginata\n",
    "accuracy=53.57142857142857\n",
    "recall=62.5\n",
    "precision=62.5\n",
    "\n",
    "### Conclusions: \n",
    "(x-x0)**2 and arith avg gives the best accuracies. Now reflectance allows us to detect kalinini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "42be5642-9c7f-4ab6-a80e-bf266f97035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction data\n",
    "\n",
    "prediction_gamma_arbitrary_limits_data = feature_and_label_extractor(Gamma_Arbitrary_Limits, prediction_spectra)\n",
    "prediction_gamma_first_two_peaks_data = feature_and_label_extractor(Gamma_First_Two_Peaks, prediction_spectra)\n",
    "prediction_gamma_area_under_curve_data = feature_and_label_extractor(Gamma_Area_Under_Curve_Naive, prediction_spectra)\n",
    "prediction_gamma_area_under_curve_first_min_cut_data = feature_and_label_extractor(Gamma_Area_Under_Curve_First_Min_Cut, prediction_spectra)\n",
    "prediction_gamma_vector_relative_reflectance_data = feature_and_label_extractor(Gamma_Vector_Relative_Reflectance, prediction_spectra)\n",
    "prediction_critical_points_data = feature_and_label_extractor(Critical_Points, prediction_spectra)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a690e936-3ba5-4db4-90d2-201fe5796470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['CICIMAUCR0104',\n",
       "  'CICIMAUCR0105',\n",
       "  'CICIMAUCR0158',\n",
       "  'CICIMAUCR0001',\n",
       "  'CICIMAUCR0006',\n",
       "  'CICIMAUCR0008',\n",
       "  'CICIMAUCR0009',\n",
       "  'CICIMAUCR0012',\n",
       "  'CICIMAUCR0013',\n",
       "  'CICIMAUCR0014',\n",
       "  'CICIMAUCR0015',\n",
       "  'CICIMAUCR0018',\n",
       "  'CICIMAUCR0019',\n",
       "  'CICIMAUCR0020',\n",
       "  'CICIMAUCR0021',\n",
       "  'CICIMAUCR0070',\n",
       "  'CICIMAUCR0071',\n",
       "  'CICIMAUCR0097',\n",
       "  'CICIMAUCR0098',\n",
       "  'CICIMAUCR0100',\n",
       "  'CICIMAUCR0101',\n",
       "  'CICIMAUCR0104',\n",
       "  'CICIMAUCR0108',\n",
       "  'CICIMAUCR0112',\n",
       "  'CICIMAUCR0113',\n",
       "  'CICIMAUCR0116',\n",
       "  'CICIMAUCR0141',\n",
       "  'CICIMAUCR0158'],\n",
       " [[586.0, 755.0, 1385.0, 0, 0, 0, 0, 0, 0],\n",
       "  [586.0, 776.0, 1396.0, 1668.0, 0, 0, 0, 0, 0],\n",
       "  [572.0, 742.0, 1113.0, 1213.0, 0, 0, 0, 0, 0],\n",
       "  [567.0, 765.0, 1123.0, 1300.0, 1649.0, 1848.0, 0, 0, 0],\n",
       "  [550.0, 772.0, 858.0, 1119.0, 1303.0, 1654.0, 1858.0, 0, 0],\n",
       "  [573.0, 742.0, 858.0, 1118.0, 1304.0, 0, 0, 0, 0],\n",
       "  [577.0, 797.0, 858.0, 1119.0, 1304.0, 1654.0, 1860.0, 0, 0],\n",
       "  [546.0, 804.0, 1119.0, 1298.0, 1658.0, 1753.0, 1860.0, 0, 0],\n",
       "  [568.0, 684.0, 809.0, 870.0, 1119.0, 1303.0, 1552.0, 1656.0, 1860.0],\n",
       "  [577.0, 858.0, 917.0, 1117.0, 1301.0, 1656.0, 1857.0, 0, 0],\n",
       "  [578.0, 692.0, 795.0, 858.0, 1119.0, 1304.0, 1555.0, 1654.0, 1858.0],\n",
       "  [569.0, 894.0, 1112.0, 1295.0, 1656.0, 1858.0, 0, 0, 0],\n",
       "  [566.0, 894.0, 1118.0, 1301.0, 1655.0, 1754.0, 1857.0, 0, 0],\n",
       "  [585.0, 784.0, 858.0, 1107.0, 1302.0, 1656.0, 1753.0, 1858.0, 0],\n",
       "  [577.0, 803.0, 858.0, 1121.0, 1303.0, 1654.0, 1858.0, 0, 0],\n",
       "  [584.0, 900.0, 1108.0, 1297.0, 1648.0, 1847.0, 0, 0, 0],\n",
       "  [572.0, 741.0, 881.0, 1122.0, 1298.0, 1641.0, 1697.0, 1847.0, 0],\n",
       "  [596.0, 803.0, 858.0, 1129.0, 1306.0, 1652.0, 1854.0, 0, 0],\n",
       "  [568.0, 858.0, 1117.0, 1295.0, 1657.0, 1857.0, 0, 0, 0],\n",
       "  [570.0, 887.0, 1118.0, 1302.0, 1651.0, 1857.0, 0, 0, 0],\n",
       "  [585.0, 741.0, 1117.0, 1304.0, 0, 0, 0, 0, 0],\n",
       "  [493.0, 586.0, 756.0, 881.0, 1123.0, 1305.0, 1654.0, 1857.0, 0],\n",
       "  [582.0, 690.0, 820.0, 1119.0, 1302.0, 1653.0, 1857.0, 0, 0],\n",
       "  [579.0, 800.0, 1119.0, 1304.0, 1557.0, 1655.0, 1860.0, 0, 0],\n",
       "  [587.0, 769.0, 858.0, 1118.0, 1305.0, 0, 0, 0, 0],\n",
       "  [581.0, 790.0, 859.0, 1122.0, 1299.0, 1653.0, 1856.0, 0, 0],\n",
       "  [580.0, 773.0, 858.0, 1129.0, 1302.0, 1647.0, 1698.0, 1849.0, 0],\n",
       "  [566.0, 825.0, 890.0, 1119.0, 1296.0, 1651.0, 1848.0, 0, 0]],\n",
       " ['resplendens',\n",
       "  'kalinini',\n",
       "  'cupreomarginata',\n",
       "  'kalinini',\n",
       "  'kalinini',\n",
       "  'kalinini',\n",
       "  'kalinini',\n",
       "  'cupreomarginata',\n",
       "  'resplendens',\n",
       "  'cupreomarginata',\n",
       "  'resplendens',\n",
       "  'cupreomarginata',\n",
       "  'cupreomarginata',\n",
       "  'resplendens',\n",
       "  'resplendens',\n",
       "  'resplendens',\n",
       "  'resplendens',\n",
       "  'kalinini',\n",
       "  'cupreomarginata',\n",
       "  'cupreomarginata',\n",
       "  'resplendens',\n",
       "  'resplendens',\n",
       "  'resplendens',\n",
       "  'resplendens',\n",
       "  'kalinini',\n",
       "  'kalinini',\n",
       "  'kalinini',\n",
       "  'cupreomarginata']]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(critical_points_data)\n",
    "pad_list(critical_points_data, filler = np.array([0,0]))\n",
    "pad_list(gamma_vector_relative_reflectance_data)\n",
    "data = pad_list(wavelength_vector_data)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "68b5cce2-8547-4314-b054-22f7bbc29acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version_info(major=3, minor=9, micro=18, releaselevel='final', serial=0)\n",
      "Loaded the plot_curve function.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.version_info)\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# The following lines adjust the granularity of reporting. \n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = \"{:.1f}\".format\n",
    "\n",
    "# The following line improves formatting when ouputting NumPy arrays.\n",
    "np.set_printoptions(linewidth = 200)\n",
    "\n",
    "def replace_strings_3(lst):\n",
    "    mapping = {\"kalinini\": 0, \"resplendens\": 1, \"cupreomarginata\": 2}\n",
    "    return [mapping.get(item, item) for item in lst]\n",
    "\n",
    "def replace_species_with_categorical(df):\n",
    "    \n",
    "    df.loc[df[\"species\"]==\"kalinini\",\"species\"] = 0\n",
    "    df.loc[df[\"species\"]==\"resplendens\", \"species\"] = 1\n",
    "    df.loc[df[\"species\"]==\"cupreomarginata\", \"species\"] = 2\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def plot_curve(epochs, hist, list_of_metrics):\n",
    "  \"\"\"Plot a curve of one or more classification metrics vs. epoch.\"\"\"  \n",
    "  # list_of_metrics should be one of the names shown in:\n",
    "  # https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#define_the_model_and_metrics  \n",
    "\n",
    "  plt.figure()\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(\"Value\")\n",
    "\n",
    "  for m in list_of_metrics:\n",
    "    x = hist[m]\n",
    "    plt.plot(epochs[1:], x[1:], label=m)\n",
    "\n",
    "  plt.legend()\n",
    "\n",
    "print(\"Loaded the plot_curve function.\")\n",
    "\n",
    "def transpose_list(lst):\n",
    "    return list(zip(*lst))\n",
    "    \n",
    "def get_nth_feature(data, n):\n",
    "    feature_vector = [data[0], [x[n] for x in data[1]] , data[2]] \n",
    "    return feature_vector\n",
    "def scatter_plot_2_variables(df_1, df_2):\n",
    "\n",
    "    joint_df = pd.merge(df_1, df_2, on=[\"code\", \"species\"], how=\"inner\")\n",
    "    column_list = joint_df.columns.tolist()\n",
    "    print(column_list)\n",
    "    column_list = [x for x in column_list if x not in [\"code\", \"species\"] ]\n",
    "    print(column_list)\n",
    "    plt.figure()\n",
    "    sns.scatterplot(joint_df, x=column_list[0], y =column_list[1], hue=\"species\")\n",
    "    plt.show()\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def scatter_plot_3_variables(df_1, df_2, df_3):\n",
    "    \n",
    "    joint_df = pd.merge(df_1, df_2, on=[\"code\", \"species\"], how=\"inner\")\n",
    "    joint_df = pd.merge(joint_df, df_3, on=[\"code\", \"species\"], how=\"inner\")\n",
    "    \n",
    "    column_list = joint_df.columns.tolist()\n",
    "    print(column_list)\n",
    "    column_list = [x for x in column_list if x not in [\"code\", \"species\"] ]\n",
    "    print(column_list)\n",
    "    \n",
    "    x = joint_df[column_list[0]]\n",
    "    y = joint_df[column_list[1]]\n",
    "    z = joint_df[column_list[2]]\n",
    "    species = joint_df[\"species\"]\n",
    "    \n",
    "    # Create color map\n",
    "    colors = {'kalinini': 'r', 'resplendens': 'g', 'cupreomarginata': 'b'}\n",
    "    \n",
    "    # Create figure and 3D axis\n",
    "    fig = plt.figure(figsize=(15, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Plot points with color based on the fourth dimension\n",
    "    for category in set(species):\n",
    "        indices = species == category\n",
    "        ax.scatter(x[indices], y[indices], z[indices], c=colors[category], label=category, marker='o')\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xlabel(f'{column_list[0]}')\n",
    "    ax.set_ylabel(f'{column_list[1]}')\n",
    "    ax.set_zlabel(f'{column_list[2]}')\n",
    "    #ax.set_title('3D Scatter plot with species based on Fourth Dimension')\n",
    "    \n",
    "    # Add legend\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5f559b3c-dca4-4724-9ce1-98d23f7df0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    g_arbitrary_limits species  g_first_2_peaks  g_area_und_curve  \\\n",
      "33                 1.0       2              1.1               2.5   \n",
      "13                 2.8       2              1.1               1.1   \n",
      "50                 0.9       2              0.9               2.6   \n",
      "26                 1.0       2              1.0               2.4   \n",
      "12                 2.8       2              0.9               2.6   \n",
      "..                 ...     ...              ...               ...   \n",
      "48                 0.9       2              0.9               1.1   \n",
      "25                 1.1       2              1.0               2.2   \n",
      "11                 2.8       2              0.9               2.6   \n",
      "32                 1.1       2              1.0               2.0   \n",
      "49                 0.9       2              0.9               1.1   \n",
      "\n",
      "    g_area_und_curve_first_min  \n",
      "33                         2.8  \n",
      "13                         1.8  \n",
      "50                         1.8  \n",
      "26                         2.9  \n",
      "12                         1.7  \n",
      "..                         ...  \n",
      "48                         1.8  \n",
      "25                         2.7  \n",
      "11                         1.8  \n",
      "32                         1.7  \n",
      "49                         1.7  \n",
      "\n",
      "[16 rows x 5 columns]\n",
      "    g_arbitrary_limits  g_first_2_peaks  g_area_und_curve  \\\n",
      "19                 1.4              1.3               1.6   \n",
      "8                  1.8              1.3               1.6   \n",
      "17                 1.3              1.3               2.1   \n",
      "47                 1.4              1.3               2.0   \n",
      "46                 1.4              1.4               1.9   \n",
      "31                 1.5              1.3               2.0   \n",
      "\n",
      "    g_area_und_curve_first_min  \n",
      "19                         2.1  \n",
      "8                          4.0  \n",
      "17                         2.2  \n",
      "47                         1.9  \n",
      "46                         3.2  \n",
      "31                         1.8  \n",
      "[[1.06126356 2.5783343 ]\n",
      " [2.4370413  1.39520883]\n",
      " [1.06179656 2.84571654]\n",
      " [1.05697194 1.82391218]\n",
      " [1.29853839 2.07831437]\n",
      " [0.8562896  1.82391218]\n",
      " [1.06126356 1.39520883]\n",
      " [1.04246223 2.86395875]\n",
      " [1.00165641 1.45960999]\n",
      " [0.8562896  1.68607591]\n",
      " [1.05697194 1.82391218]\n",
      " [1.06126356 1.39520883]\n",
      " [0.95861261 2.37758417]\n",
      " [1.08973413 1.60353085]\n",
      " [1.28946201 4.02252466]\n",
      " [1.34874782 2.21187809]\n",
      " [2.4370413  2.5783343 ]\n",
      " [2.4370413  2.5783343 ]\n",
      " [2.4370413  2.5783343 ]\n",
      " [1.0326184  1.51943642]\n",
      " [1.26214043 1.87880782]\n",
      " [1.40828734 3.19563013]\n",
      " [1.05697194 1.82391218]\n",
      " [1.05697194 1.82391218]\n",
      " [0.8562896  1.82391218]\n",
      " [1.05697194 1.68607591]\n",
      " [0.84577209 0.39854101]\n",
      " [1.31857708 1.83457996]\n",
      " [1.05697194 1.68607591]\n",
      " [0.8562896  1.82391218]\n",
      " [1.03487736 2.72176018]\n",
      " [0.8562896  1.82391218]\n",
      " [1.00783275 1.72575088]\n",
      " [0.8562896  1.68607591]\n",
      " [1.06126356 2.5783343 ]\n",
      " [1.41054007 3.50842406]]\n",
      "[[0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "#Training data. Define dataframees\n",
    "\n",
    "#Use scalar metrics\n",
    "gal_df = pd.DataFrame(transpose_list(gamma_arbitrary_limits_data), columns =[\"code\", \"g_arbitrary_limits\", \"species\"]) \n",
    "gftp_df = pd.DataFrame(transpose_list(gamma_first_two_peaks_data), columns =[\"code\", \"g_first_2_peaks\", \"species\"])\n",
    "gaucfmc_df = pd.DataFrame(transpose_list(gamma_area_under_curve_first_min_cut_data), columns =[\"code\", \"g_area_und_curve_first_min\", \"species\"]) \n",
    "gauc_df = pd.DataFrame(transpose_list(gamma_area_under_curve_data), columns =[\"code\", \"g_area_und_curve\", \"species\"])\n",
    "similarity_index_xy_df = pd.DataFrame(transpose_list())\n",
    "\n",
    "#merge scalar metrics into a single dataframe\n",
    "complete_df = pd.merge(gal_df, gftp_df, on=[\"species\",\"code\"], how=\"inner\")\n",
    "complete_df = pd.merge(complete_df, gauc_df, on=[\"species\",\"code\"], how=\"inner\")\n",
    "complete_df = pd.merge(complete_df, gaucfmc_df, on=[\"species\",\"code\"], how=\"inner\")\n",
    "complete_df.drop(columns=['code'], inplace=True)\n",
    "\n",
    "complete_df = replace_species_with_categorical(complete_df)\n",
    "#complete_df.drop(columns=['species'], inplace=True)\n",
    "complete_df\n",
    "\n",
    "#shuffle the df\n",
    "shuffled_df = complete_df.sample(frac=1, random_state=42)  # Random_state for reproducibility\n",
    "\n",
    "# Define the fraction of data to be used for training\n",
    "train_fraction = 0.66  # For example, 80% for training, 20% for testing\n",
    "\n",
    "# Calculate the number of rows for the training set\n",
    "train_size = int(train_fraction * len(complete_df))\n",
    "\n",
    "# Split the shuffled DataFrame into train and test sets\n",
    "train_data = shuffled_df.iloc[:train_size]\n",
    "test_data = shuffled_df.iloc[train_size:]\n",
    "\n",
    "# Split the shuffled DataFrame into features and labels\n",
    "train_data_features = train_data.drop(columns=['species'], inplace=False)\n",
    "train_data_labels =  train_data.drop(columns=['g_arbitrary_limits','g_first_2_peaks','g_area_und_curve_first_min','g_area_und_curve'], inplace=False)\n",
    "test_data_features =  test_data.drop(columns=['species'], inplace=False)\n",
    "test_data_labels =  test_data.drop(columns=['g_arbitrary_limits','g_first_2_peaks','g_area_und_curve_first_min','g_area_und_curve'], inplace=False)\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Assuming y_train and y_test are your integer labels\n",
    "train_data_labels_one_hot = to_categorical(train_data_labels, num_classes=3)\n",
    "test_data_labels_one_hot = to_categorical(test_data_labels, num_classes=3)\n",
    "\n",
    "print(train_data[train_data_labels[\"species\"]==2])\n",
    "print(train_data_features[train_data_labels[\"species\"]==0])\n",
    "#print(len(train_data))\n",
    "#print(len(test_data))\n",
    "#print(test_features)\n",
    "#print(test_labels)\n",
    "\n",
    "# Select 2\n",
    "train_data_features = np.array(train_data_features.drop(columns=['g_arbitrary_limits',\"g_area_und_curve\"], inplace=False).values)\n",
    "test_data_features =  np.array(test_data_features.drop(columns=['g_arbitrary_limits',\"g_area_und_curve\"], inplace=False).values)\n",
    "\n",
    "print(train_data_features)\n",
    "#print(train_data_labels[train_data_labels[\"species\"]==0])\n",
    "print(train_data_labels_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9af49d08-188d-4066-bb7d-c63d02c42f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_3(my_learning_rate):\n",
    "  \"\"\"Create and compile a deep neural net.\"\"\"\n",
    "  \n",
    "  # All models in this course are sequential.\n",
    "  model = tf.keras.models.Sequential()\n",
    "\n",
    "  # The features are stored in a two-dimensional 28X28 array. \n",
    "  # Flatten that two-dimensional array into a one-dimensional \n",
    "  # 784-element array.\n",
    "  model.add(tf.keras.layers.Flatten(input_shape=(2,)))\n",
    "\n",
    "  # Define the first hidden layer.   \n",
    "  model.add(tf.keras.layers.Dense(units=8, activation='relu'))\n",
    "\n",
    "  # Define the first hidden layer.   \n",
    "  model.add(tf.keras.layers.Dense(units=4, activation='relu'))\n",
    "\n",
    "  # Define the first hidden layer.   \n",
    "  model.add(tf.keras.layers.Dense(units=4, activation='relu'))\n",
    "  \n",
    "  # Define a dropout regularization layer. \n",
    "  model.add(tf.keras.layers.Dropout(rate=0.1))\n",
    "\n",
    "  # Define the output layer. The units parameter is set to 10 because\n",
    "  # the model must choose among 10 possible output values (representing\n",
    "  # the digits from 0 to 9, inclusive).\n",
    "  #\n",
    "  # Don't change this layer.\n",
    "  model.add(tf.keras.layers.Dense(units=3, activation='softmax'))     \n",
    "                           \n",
    "  # Construct the layers into a model that TensorFlow can execute.  \n",
    "  # Notice that the loss function for multi-class classification\n",
    "  # is different than the loss function for binary classification.  \n",
    "  #model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=my_learning_rate),loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
    "  model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "  return model    \n",
    "\n",
    "\n",
    "def train_model(model, train_features, train_label, epochs,\n",
    "                batch_size=None, validation_split=0.1):\n",
    "  \"\"\"Train the model by feeding it data.\"\"\"\n",
    "\n",
    "  history = model.fit(x=train_features, y=train_label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=True)\n",
    "                      #validation_split=validation_split)\n",
    " \n",
    "  # To track the progression of training, gather a snapshot\n",
    "  # of the model's metrics at each epoch. \n",
    "  epochs = history.epoch\n",
    "  hist = pd.DataFrame(history.history)\n",
    "\n",
    "  return epochs, hist    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43828edf-9500-4fde-a32c-50cc773f3d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0767 - accuracy: 0.4444\n",
      "Epoch 2/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0736 - accuracy: 0.4444\n",
      "Epoch 3/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0686 - accuracy: 0.4444\n",
      "Epoch 4/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0670 - accuracy: 0.4444\n",
      "Epoch 5/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0750 - accuracy: 0.4444\n",
      "Epoch 6/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0626 - accuracy: 0.4444\n",
      "Epoch 7/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0694 - accuracy: 0.4722\n",
      "Epoch 8/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0672 - accuracy: 0.4167\n",
      "Epoch 9/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0772 - accuracy: 0.4167\n",
      "Epoch 10/160\n",
      "12/12 [==============================] - 0s 968us/step - loss: 1.0618 - accuracy: 0.4444\n",
      "Epoch 11/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0604 - accuracy: 0.4444\n",
      "Epoch 12/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0590 - accuracy: 0.4722\n",
      "Epoch 13/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0609 - accuracy: 0.4444\n",
      "Epoch 14/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0401 - accuracy: 0.4444\n",
      "Epoch 15/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0593 - accuracy: 0.5000\n",
      "Epoch 16/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0471 - accuracy: 0.4444\n",
      "Epoch 17/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0475 - accuracy: 0.5000\n",
      "Epoch 18/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0394 - accuracy: 0.4444\n",
      "Epoch 19/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0451 - accuracy: 0.4444\n",
      "Epoch 20/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0390 - accuracy: 0.4444\n",
      "Epoch 21/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0517 - accuracy: 0.4167\n",
      "Epoch 22/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0350 - accuracy: 0.4444\n",
      "Epoch 23/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0425 - accuracy: 0.4722\n",
      "Epoch 24/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0295 - accuracy: 0.4444\n",
      "Epoch 25/160\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.0391 - accuracy: 0.4167\n",
      "Epoch 26/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0492 - accuracy: 0.4444\n",
      "Epoch 27/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0363 - accuracy: 0.4722\n",
      "Epoch 28/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0278 - accuracy: 0.4722\n",
      "Epoch 29/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0352 - accuracy: 0.4722\n",
      "Epoch 30/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0355 - accuracy: 0.4167\n",
      "Epoch 31/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0379 - accuracy: 0.4722\n",
      "Epoch 32/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0217 - accuracy: 0.4444\n",
      "Epoch 33/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0214 - accuracy: 0.4167\n",
      "Epoch 34/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0195 - accuracy: 0.4444\n",
      "Epoch 35/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0350 - accuracy: 0.4722\n",
      "Epoch 36/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0293 - accuracy: 0.4167\n",
      "Epoch 37/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0152 - accuracy: 0.5278\n",
      "Epoch 38/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0282 - accuracy: 0.5000\n",
      "Epoch 39/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0373 - accuracy: 0.5000\n",
      "Epoch 40/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0332 - accuracy: 0.5000\n",
      "Epoch 41/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0113 - accuracy: 0.6111\n",
      "Epoch 42/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0250 - accuracy: 0.5556\n",
      "Epoch 43/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0177 - accuracy: 0.5278\n",
      "Epoch 44/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0067 - accuracy: 0.5833\n",
      "Epoch 45/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0077 - accuracy: 0.6111\n",
      "Epoch 46/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0306 - accuracy: 0.5278\n",
      "Epoch 47/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0013 - accuracy: 0.5556\n",
      "Epoch 48/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0210 - accuracy: 0.5556\n",
      "Epoch 49/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0116 - accuracy: 0.5278\n",
      "Epoch 50/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0009 - accuracy: 0.5833\n",
      "Epoch 51/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9891 - accuracy: 0.5833\n",
      "Epoch 52/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0064 - accuracy: 0.6111\n",
      "Epoch 53/160\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 1.0026 - accuracy: 0.5278\n",
      "Epoch 54/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0195 - accuracy: 0.5278\n",
      "Epoch 55/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0194 - accuracy: 0.5278\n",
      "Epoch 56/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9810 - accuracy: 0.6389\n",
      "Epoch 57/160\n",
      "12/12 [==============================] - 0s 1000us/step - loss: 0.9825 - accuracy: 0.6389\n",
      "Epoch 58/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9924 - accuracy: 0.5833\n",
      "Epoch 59/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0031 - accuracy: 0.5833\n",
      "Epoch 60/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0128 - accuracy: 0.5833\n",
      "Epoch 61/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0111 - accuracy: 0.6111\n",
      "Epoch 62/160\n",
      "12/12 [==============================] - 0s 928us/step - loss: 0.9806 - accuracy: 0.6389\n",
      "Epoch 63/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9812 - accuracy: 0.6667\n",
      "Epoch 64/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9801 - accuracy: 0.6111\n",
      "Epoch 65/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9771 - accuracy: 0.6389\n",
      "Epoch 66/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9660 - accuracy: 0.6944\n",
      "Epoch 67/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9674 - accuracy: 0.6667\n",
      "Epoch 68/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0050 - accuracy: 0.5556\n",
      "Epoch 69/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9782 - accuracy: 0.6111\n",
      "Epoch 70/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9740 - accuracy: 0.6389\n",
      "Epoch 71/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 1.0105 - accuracy: 0.6111\n",
      "Epoch 72/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9778 - accuracy: 0.6944\n",
      "Epoch 73/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9706 - accuracy: 0.6667\n",
      "Epoch 74/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9738 - accuracy: 0.6667\n",
      "Epoch 75/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9865 - accuracy: 0.6389\n",
      "Epoch 76/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9535 - accuracy: 0.6944\n",
      "Epoch 77/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9814 - accuracy: 0.6667\n",
      "Epoch 78/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9800 - accuracy: 0.6667\n",
      "Epoch 79/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9607 - accuracy: 0.6667\n",
      "Epoch 80/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9209 - accuracy: 0.7222\n",
      "Epoch 81/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9362 - accuracy: 0.7222\n",
      "Epoch 82/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9398 - accuracy: 0.7222\n",
      "Epoch 83/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9386 - accuracy: 0.6389\n",
      "Epoch 84/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9504 - accuracy: 0.5833\n",
      "Epoch 85/160\n",
      "12/12 [==============================] - 0s 1000us/step - loss: 0.9603 - accuracy: 0.6111\n",
      "Epoch 86/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8885 - accuracy: 0.8056\n",
      "Epoch 87/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9464 - accuracy: 0.6667\n",
      "Epoch 88/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9401 - accuracy: 0.6389\n",
      "Epoch 89/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9313 - accuracy: 0.7222\n",
      "Epoch 90/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8972 - accuracy: 0.6944\n",
      "Epoch 91/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9262 - accuracy: 0.6944\n",
      "Epoch 92/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9383 - accuracy: 0.6667\n",
      "Epoch 93/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9324 - accuracy: 0.5833\n",
      "Epoch 94/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9274 - accuracy: 0.6389\n",
      "Epoch 95/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9411 - accuracy: 0.5833\n",
      "Epoch 96/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9416 - accuracy: 0.6667\n",
      "Epoch 97/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9051 - accuracy: 0.6944\n",
      "Epoch 98/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9115 - accuracy: 0.6944\n",
      "Epoch 99/160\n",
      "12/12 [==============================] - 0s 2ms/step - loss: 0.8876 - accuracy: 0.6944\n",
      "Epoch 100/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9077 - accuracy: 0.6944\n",
      "Epoch 101/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9137 - accuracy: 0.7222\n",
      "Epoch 102/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8810 - accuracy: 0.7222\n",
      "Epoch 103/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8815 - accuracy: 0.6667\n",
      "Epoch 104/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9011 - accuracy: 0.6944\n",
      "Epoch 105/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9237 - accuracy: 0.6389\n",
      "Epoch 106/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8727 - accuracy: 0.7222\n",
      "Epoch 107/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8991 - accuracy: 0.7500\n",
      "Epoch 108/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9136 - accuracy: 0.7222\n",
      "Epoch 109/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9111 - accuracy: 0.6667\n",
      "Epoch 110/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8714 - accuracy: 0.6944\n",
      "Epoch 111/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8575 - accuracy: 0.6667\n",
      "Epoch 112/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8791 - accuracy: 0.6944\n",
      "Epoch 113/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9055 - accuracy: 0.6389\n",
      "Epoch 114/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9165 - accuracy: 0.5833\n",
      "Epoch 115/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8838 - accuracy: 0.6667\n",
      "Epoch 116/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8553 - accuracy: 0.7222\n",
      "Epoch 117/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8681 - accuracy: 0.7222\n",
      "Epoch 118/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8814 - accuracy: 0.6944\n",
      "Epoch 119/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8418 - accuracy: 0.7222\n",
      "Epoch 120/160\n",
      "12/12 [==============================] - 0s 939us/step - loss: 0.8637 - accuracy: 0.6667\n",
      "Epoch 121/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8687 - accuracy: 0.6389\n",
      "Epoch 122/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8360 - accuracy: 0.6944\n",
      "Epoch 123/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8757 - accuracy: 0.6389\n",
      "Epoch 124/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9050 - accuracy: 0.6389\n",
      "Epoch 125/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8172 - accuracy: 0.7500\n",
      "Epoch 126/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8683 - accuracy: 0.6389\n",
      "Epoch 127/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8280 - accuracy: 0.6944\n",
      "Epoch 128/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8403 - accuracy: 0.7500\n",
      "Epoch 129/160\n",
      "12/12 [==============================] - 0s 977us/step - loss: 0.8896 - accuracy: 0.6944\n",
      "Epoch 130/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8391 - accuracy: 0.6944\n",
      "Epoch 131/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8247 - accuracy: 0.7222\n",
      "Epoch 132/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8507 - accuracy: 0.6944\n",
      "Epoch 133/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8025 - accuracy: 0.7222\n",
      "Epoch 134/160\n",
      "12/12 [==============================] - 0s 3ms/step - loss: 0.8699 - accuracy: 0.6389\n",
      "Epoch 135/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8832 - accuracy: 0.6944\n",
      "Epoch 136/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9067 - accuracy: 0.6111\n",
      "Epoch 137/160\n",
      "12/12 [==============================] - 0s 1000us/step - loss: 0.8004 - accuracy: 0.7222\n",
      "Epoch 138/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8913 - accuracy: 0.6111\n",
      "Epoch 139/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8050 - accuracy: 0.7222\n",
      "Epoch 140/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8260 - accuracy: 0.6944\n",
      "Epoch 141/160\n",
      "12/12 [==============================] - 0s 965us/step - loss: 0.8084 - accuracy: 0.6944\n",
      "Epoch 142/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8406 - accuracy: 0.6389\n",
      "Epoch 143/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8353 - accuracy: 0.6667\n",
      "Epoch 144/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.9055 - accuracy: 0.6389\n",
      "Epoch 145/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8811 - accuracy: 0.6389\n",
      "Epoch 146/160\n",
      "12/12 [==============================] - 0s 1000us/step - loss: 0.8385 - accuracy: 0.6389\n",
      "Epoch 147/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8194 - accuracy: 0.7222\n",
      "Epoch 148/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8403 - accuracy: 0.6389\n",
      "Epoch 149/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8706 - accuracy: 0.6389\n",
      "Epoch 150/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8411 - accuracy: 0.7500\n",
      "Epoch 151/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8389 - accuracy: 0.6944\n",
      "Epoch 152/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8114 - accuracy: 0.6944\n",
      "Epoch 153/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8124 - accuracy: 0.7500\n",
      "Epoch 154/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8496 - accuracy: 0.6111\n",
      "Epoch 155/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.7999 - accuracy: 0.6667\n",
      "Epoch 156/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8157 - accuracy: 0.6944\n",
      "Epoch 157/160\n",
      "12/12 [==============================] - 0s 1000us/step - loss: 0.8528 - accuracy: 0.6944\n",
      "Epoch 158/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8071 - accuracy: 0.6944\n",
      "Epoch 159/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8129 - accuracy: 0.6944\n",
      "Epoch 160/160\n",
      "12/12 [==============================] - 0s 1ms/step - loss: 0.8069 - accuracy: 0.6944\n",
      "\n",
      " Evaluate the new model against the test set:\n",
      "7/7 [==============================] - 0s 1ms/step - loss: 0.7313 - accuracy: 0.7000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7313053011894226, 0.699999988079071]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.007\n",
    "epochs = 160\n",
    "batch_size = 3\n",
    "validation_split = 0.05\n",
    "\n",
    "# Establish the model's topography.\n",
    "my_model = create_model_3(learning_rate)\n",
    "\n",
    "# Train the model on the normalized training set.\n",
    "#epochs, hist = train_model(my_model, x_train, y_train, \n",
    "                           #epochs, batch_size, validation_split)\n",
    "epochs, hist = train_model(my_model, train_data_features, train_data_labels_one_hot, \n",
    "                           epochs, batch_size, validation_split)\n",
    "\n",
    "# Plot a graph of the metric vs. epochs.\n",
    "list_of_metrics_to_plot = ['accuracy']\n",
    "plot_curve(epochs, hist, list_of_metrics_to_plot)\n",
    "\n",
    "# Evaluate against the test set.\n",
    "print(\"\\n Evaluate the new model against the test set:\")\n",
    "my_model.evaluate(x=test_data_features, y=test_data_labels_one_hot, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f3c3ca-d4c4-4a0e-972b-e82897d912b5",
   "metadata": {},
   "source": [
    "### Notas: \n",
    "Mejores resultados usando el valor de batch size  4 o 3, acc 0.7\n",
    " Para 5, acc baja a 0.6\n",
    " Learning rate, buenos resultados en: 0.008-0.009. Acc 0.7\n",
    "aprendizaje inestable: Cambiamos learning rate a 0.007 y epochs a 600\n",
    " Pasar regularizaci√≥n de 0 a 0.1 hace que el aprendizaje sea m√°s suave\n",
    " Pasar de 4,4 a 8,4 hace el aprendizaje mas suave\n",
    " Pasar de 4,4 a 8,6 hace el aprendizaje menos efectivo\n",
    " Pasar de 4,4 a 8,4,4 hace el aprendizaje mas suave\n",
    "aumentar el validation split de 0.1 a 0.3 reduce el acc a 0.3 0.6 0.6\n",
    "bajar el validation split 0.1 a 0.05 reduce el acc a 0.5 0.6 0.7\n",
    "dejar el val. split en 0.1 deja acc en 0.5 0.6 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2312c97-d8ab-4e53-af25-279981393f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model \n",
    "\n",
    "# Define the file path where you want to save your model\n",
    "model_file_path = 'trained_model_4_var.h5'\n",
    "\n",
    "# Save the model\n",
    "my_model.save(model_file_path)\n",
    "\n",
    "print(\"Model saved successfully at:\", model_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e39f580-834b-4d0d-8c2c-333c24046e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "model_file_path = 'trained_model_4_var.h5'\n",
    "loaded_model2 = tf.keras.models.load_model(model_file_path)\n",
    "\n",
    "#load prediction data\n",
    "\n",
    "#Define dataframe\n",
    "pred_gal_df = pd.DataFrame(transpose_list(prediction_gamma_arbitrary_limits_data), columns =[\"code\", \"g_arbitrary_limits\", \"real_species\"]) \n",
    "pred_gftp_df = pd.DataFrame(transpose_list(prediction_gamma_first_two_peaks_data), columns =[\"code\", \"g_first_2_peaks\", \"real_species\"])\n",
    "pred_gauc_df = pd.DataFrame(transpose_list(prediction_gamma_area_under_curve_data), columns =[\"code\", \"g_area_und_curve\", \"real_species\"])\n",
    "pred_gaucfmc_df = pd.DataFrame(transpose_list(prediction_gamma_area_under_curve_first_min_cut_data), columns =[\"code\", \"g_area_und_curve_first_min\", \"real_species\"]) \n",
    "\n",
    "\n",
    "#drop species\n",
    "\n",
    "drop_pred_gal_df = pred_gal_df.copy()\n",
    "drop_pred_gal_df.drop(columns=[ \"real_species\"], inplace=True)\n",
    "\n",
    "drop_pred_gftp_df = pred_gftp_df.copy()\n",
    "drop_pred_gftp_df.drop(columns=[ \"real_species\"], inplace=True)\n",
    "\n",
    "drop_pred_gauc_df= pred_gauc_df.copy()\n",
    "drop_pred_gauc_df.drop(columns=[ \"real_species\"], inplace=True)\n",
    "\n",
    "drop_pred_gaucfmc_df=pred_gaucfmc_df.copy()\n",
    "drop_pred_gaucfmc_df.drop(columns=[ \"real_species\"], inplace=True)\n",
    "\n",
    "\n",
    "#merge on code\n",
    "pred_complete_df = pd.merge(drop_pred_gal_df, drop_pred_gftp_df, on=[\"code\"], how=\"inner\")\n",
    "pred_complete_df = pd.merge(pred_complete_df, drop_pred_gauc_df, on=[\"code\"], how=\"inner\")\n",
    "pred_complete_df = pd.merge(pred_complete_df, drop_pred_gaucfmc_df, on=[\"code\"], how=\"inner\")\n",
    "\n",
    "no_code_pred_complete_df= pred_complete_df.copy()\n",
    "no_code_pred_complete_df.drop(columns=[ \"code\"], inplace=True)\n",
    "pred_complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcff6cbd-be4d-4894-9ef5-03af5c2c90fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 2\n",
    "no_code_pred_complete_df = no_code_pred_complete_df.drop(columns=['g_arbitrary_limits',\"g_area_und_curve\"], inplace=False)\n",
    "\n",
    "print(no_code_pred_complete_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e43d65-8ede-46f7-bc98-4849897ee62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prediction_features = pd.DataFrame({name:np.array(value) for name, value in no_code_pred_complete_df.items()})\n",
    "\n",
    "print(prediction_features)\n",
    "#print(pred_complete_df)\n",
    "#convert prediction_features to tensor\n",
    "for element in prediction_features:\n",
    "    prediction_features[element] = tf.convert_to_tensor(np.array(prediction_features[element]), dtype=tf.int64) \n",
    "\n",
    "\n",
    "# Predict using the loaded model\n",
    "predictions = loaded_model2.predict(prediction_features)\n",
    "\n",
    "# Print the predictions\n",
    "#print(predictions)\n",
    "prediction_df = pd.DataFrame(predictions, columns=[\"kalinini\", \"resplendens\"])\n",
    "prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f171b28b-aef4-4a6a-a1fe-28499358f1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_prediction_data = pd.merge(pred_gal_df, pred_gftp_df , on=[\"code\",\"real_species\"], how=\"inner\")\n",
    "merged_prediction_data = pd.merge(merged_prediction_data, pred_gauc_df , on=[\"code\",\"real_species\"], how=\"inner\")\n",
    "merged_prediction_data = pd.merge(merged_prediction_data, pred_gaucfmc_df , on=[\"code\",\"real_species\"], how=\"inner\")\n",
    "merged_prediction_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f76c03-8a45-4bb3-b1f7-d7a4bd8ee84a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e3ee75-6dc2-4b03-98cd-1c35389a4afd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da0e246-ecd9-4fb7-99c6-6b72801c7e10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146bcf98-0ab6-471a-970b-30f0620140cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0750d20a-f226-4be6-a2d4-a0e0f2e20125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c63b38-ab00-497e-a06a-ad5ec691969f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
