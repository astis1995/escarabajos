{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f814dbfe-e40f-4555-969c-058526eb7f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EstebanSoto\\Jupyter\\escarabajos\\gamma\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Add the current directory to the Python path\n",
    "sys.path.append(current_directory)\n",
    "print(current_directory)\n",
    "\n",
    "from spectraltools import Specimen_Collection, Spectrum, create_path_if_not_exists\n",
    "from metrics import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "import re\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "922f2211-60c3-42b9-9e13-a88edf9da0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This section allows the user to choose their workplace location.\n",
    "This is important if the user has multiple locations and operating systems in which this \n",
    "script is run\"\"\"\n",
    "\n",
    "#select location\n",
    "working_at = \"cicima_desktop\"\n",
    "\n",
    "#Training data is used when we are already certain of species and genera for a particular sample\n",
    "training_data_is_used = False\n",
    "\n",
    "if working_at == \"colaboratory\":\n",
    "  from google.colab import drive\n",
    "  drive.mount(\"/content/drive\")\n",
    "  #base folder\n",
    "  \"\"\"Select the location for your base folder\"\"\"\n",
    "    \n",
    "  base_folder = r\"/content/drive/My Drive/CICIMA/escarabajos_files\"\n",
    "  \n",
    "elif working_at == \"wfh\":\n",
    "\n",
    "    \"\"\"Select the location of your base folder\"\"\"\n",
    "    base_folder = r\"C:\\Users\\esteb\\cicima\\escarabajos\"\n",
    "\n",
    "elif working_at == \"cicima_desktop\":\n",
    "  \n",
    "    \"\"\"Select the location of your base folder\"\"\"\n",
    "    base_folder = r\"C:\\Users\\EstebanSoto\\Jupyter\\escarabajos\"\n",
    "\n",
    "elif working_at == \"cicima_laptop\":\n",
    "    \n",
    "    \"\"\"Select the location of your base folder\"\"\"\n",
    "    base_folder = r\"/home/vinicio/escarabajos\"\n",
    "\n",
    "#define the location of the tables with information about the collections and its parent directory\n",
    "\n",
    "collection_tables_main_path =  os.path.join(base_folder, \"L1050_data\",\"collections\")\n",
    "collection_files_main_path = os.path.join(base_folder, \"L1050_data\")\n",
    "\n",
    "# Define report location\n",
    "report_location = os.path.join(base_folder, \"reports\",\"data_analysis\")\n",
    "\n",
    "#collection_descriptor = r\"CICIMAUCR and ANGSOL\" tododelete\n",
    "\n",
    "#File location and metadata location for collection 1\n",
    "angsol_collection_path = os.path.join(collection_files_main_path,\"ANGSOL\",\"average\") \n",
    "angsol_collection_metadata = os.path.join(collection_tables_main_path,\"CICIMA-beetles-general-inventory - ANGSOL.txt\") \n",
    "\n",
    "#File location and metadata location for collection 2\n",
    "cicimaucr_collection_path = os.path.join(collection_files_main_path,r\"TRA_data_CICIMA_INBUCR\",\"CICIMAUCR\",\"reflectance\")  #listo\n",
    "cicimaucr_collection_2_path = os.path.join(collection_files_main_path,r\"CICIMA-2024-01-REFLECTANCE\",\"average\")\n",
    "cicimaucr_collection_3_path = os.path.join(collection_files_main_path,r\"CICIMA-2024-03-REFLECTANCE\",\"without iris nor lens\",\"average\")\n",
    "cicimaucr_collection_4_path = os.path.join(collection_files_main_path,r\"CICIMA-2024-05-REFLECTANCE\",\"average\")\n",
    "cicima_ucr_metadata = os.path.join(collection_tables_main_path,r\"CICIMA-beetles-general-inventory - CICIMAUCR.txt\") \n",
    "\n",
    "#File location and metadata location for collection 3\n",
    "inbucr_collection_path = os.path.join(collection_files_main_path,r\"INBUCR\",\"average\") #listo\n",
    "inbucr_metadata = os.path.join(collection_tables_main_path,r\"CICIMA-beetles-general-inventory - INBUCR.txt\") \n",
    "\n",
    "#File location and metadata location for collection 4\n",
    "bioucr_collection_path = os.path.join(collection_files_main_path,r\"BIOUCR\",\"average\")  #listo\n",
    "bioucr_metadata = os.path.join(collection_tables_main_path,r\"CICIMA-beetles-general-inventory - BIOUCR.txt\") \n",
    "\n",
    "#agregated data location, here averages and std will be saved when training data and retreived when classifying spectra\n",
    "agregated_data_location = os.path.join(base_folder, \"aggregated_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b69b7d8f-51dd-4b86-b530-0fdf2ff98f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<spectraltools.Spectrum at 0x22a3f171d30>,\n",
       " <spectraltools.Spectrum at 0x22a33eb6ee0>,\n",
       " <spectraltools.Spectrum at 0x22a3f171880>,\n",
       " <spectraltools.Spectrum at 0x22a3f171910>,\n",
       " <spectraltools.Spectrum at 0x22a40207460>,\n",
       " <spectraltools.Spectrum at 0x22a40207cd0>,\n",
       " <spectraltools.Spectrum at 0x22a40207d30>,\n",
       " <spectraltools.Spectrum at 0x22a40207190>,\n",
       " <spectraltools.Spectrum at 0x22a40207b20>,\n",
       " <spectraltools.Spectrum at 0x22a4020e5b0>,\n",
       " <spectraltools.Spectrum at 0x22a4020ef70>,\n",
       " <spectraltools.Spectrum at 0x22a4020ef10>,\n",
       " <spectraltools.Spectrum at 0x22a40217e50>,\n",
       " <spectraltools.Spectrum at 0x22a40217280>,\n",
       " <spectraltools.Spectrum at 0x22a40217c10>,\n",
       " <spectraltools.Spectrum at 0x22a40217040>,\n",
       " <spectraltools.Spectrum at 0x22a40217e20>,\n",
       " <spectraltools.Spectrum at 0x22a4020ed90>,\n",
       " <spectraltools.Spectrum at 0x22a40217ee0>,\n",
       " <spectraltools.Spectrum at 0x22a40217670>,\n",
       " <spectraltools.Spectrum at 0x22a40225280>,\n",
       " <spectraltools.Spectrum at 0x22a40225ac0>,\n",
       " <spectraltools.Spectrum at 0x22a4020eb50>,\n",
       " <spectraltools.Spectrum at 0x22a40225fa0>,\n",
       " <spectraltools.Spectrum at 0x22a4022ff10>,\n",
       " <spectraltools.Spectrum at 0x22a402258e0>,\n",
       " <spectraltools.Spectrum at 0x22a402251f0>,\n",
       " <spectraltools.Spectrum at 0x22a4020efa0>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"In this section we define the collections and add metadata if necessary\"\"\"\n",
    "#Collections\n",
    "angsol_collection = Specimen_Collection(\"ANGSOL\", angsol_collection_path, angsol_collection_metadata, \"HIGH\")\n",
    "angsol_collection.set_description(\"ANGSOL collection has specimens that belong to Angel Sol√≠s. The confidence that we have about specimen identification is high.\")\n",
    "\n",
    "cicimaucr_collection = Specimen_Collection(\"CICIMAUCR1\", cicimaucr_collection_path, cicima_ucr_metadata, \"HIGH\")\n",
    "cicimaucr_collection.set_description(\"\"\"Elytra measurements for\"\"\")\n",
    "cicimaucr_collection_2 = Specimen_Collection(\"CICIMAUCR2\", cicimaucr_collection_2_path, cicima_ucr_metadata, \"HIGH\")\n",
    "cicimaucr_collection_3 = Specimen_Collection(\"CICIMAUCR3\", cicimaucr_collection_3_path, cicima_ucr_metadata, \"HIGH\")\n",
    "cicimaucr_collection_3.set_description(\"\"\"The most part of CICIMA specimens belongs to this collection. Full body measurements.\"\"\")\n",
    "\n",
    "cicimaucr_collection_4 = Specimen_Collection(\"CICIMAUCR4\", cicimaucr_collection_4_path, cicima_ucr_metadata, \"HIGH\")\n",
    "cicimaucr_collection_4.set_description(\"\"\"This collection has 3 kalinini specimens which were not used in training. \n",
    "                                        These are intended to be used as test subjects\"\"\")\n",
    "\n",
    "inbucr_collection = Specimen_Collection(\"INBUCR\", inbucr_collection_path, inbucr_metadata, \"MID\")\n",
    "bioucr_collection = Specimen_Collection(\"BIOUCR\", bioucr_collection_path, bioucr_metadata, \"LOW\")\n",
    "\n",
    "collection_list = [\n",
    "                    #angsol_collection,\n",
    "                    cicimaucr_collection,\n",
    "                    cicimaucr_collection_2,\n",
    "                    cicimaucr_collection_3,\n",
    "                    #inbucr_collection,\n",
    "                    #bioucr_collection,\n",
    "                    ]\n",
    "collection_names_set = set([collection.name for collection in collection_list])\n",
    "collection_names = \" \".join( sorted(collection_names_set))\n",
    "\n",
    "prediction_list = [\n",
    "                    #angsol_collection,\n",
    "                    #cicimaucr_collection,\n",
    "                    #cicimaucr_collection_2,\n",
    "                    #cicimaucr_collection_3,\n",
    "                    cicimaucr_collection_4,\n",
    "                    #inbucr_collection,\n",
    "                    #bioucr_collection,\n",
    "                    ]\n",
    "prediction_collection_names_set = set([collection.name for collection in collection_list])\n",
    "prediction_collection_names = \" \".join( sorted(collection_names_set))\n",
    "\n",
    "\n",
    "#print(collection_names)\n",
    "#date\n",
    "from datetime import datetime\n",
    "current_date = datetime.now().date()\n",
    "\n",
    "def get_filtered_spectra(collection_list):\n",
    "\n",
    "    all_spectra = []\n",
    "    \n",
    "    for collection in collection_list:\n",
    "        all_spectra += collection.get_spectra()\n",
    "\n",
    "    all_spectra = [item for item in all_spectra if item.get_species() in [\"kalinini\", \"resplendens\", \"cupreomarginata\"]]\n",
    "    return all_spectra\n",
    "    \n",
    "def get_spectra(collection_list):\n",
    "\n",
    "    all_spectra = []\n",
    "    \n",
    "    for collection in collection_list:\n",
    "        all_spectra += collection.get_spectra()\n",
    "\n",
    "    return all_spectra  \n",
    "    \n",
    "training_spectra = get_filtered_spectra(collection_list)\n",
    "prediction_spectra  = get_spectra(prediction_list) \n",
    "\n",
    "for spectrum in prediction_spectra:\n",
    "    print(spectrum.get_species())\n",
    "\n",
    "training_spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ca2d18c-6d96-4c8c-92ed-d12ae453aa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_list(lst, filler=0):\n",
    "    metrics = lst[1]\n",
    "    \n",
    "    for metric in metrics:\n",
    "        #print(metric)\n",
    "        \n",
    "        max_length = max([len(list) for list in metrics])\n",
    "        \n",
    "        #print(max_length)\n",
    "        \n",
    "        padded_list = []\n",
    "\n",
    "        for element in metrics:\n",
    "\n",
    "            padded_sublist = element\n",
    "            \n",
    "            padded_sublist += [filler] * (max_length - len(padded_sublist))\n",
    "            padded_list.append(padded_sublist)\n",
    "            #print(padded_sublist)\n",
    "    final_list = [lst[0], (padded_list), lst[2]]\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fccf47a7-9c20-471a-bab5-1f5556c28589",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Gamma_Arbitrary_Limits' object has no attribute 'metric_value'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensoflow\\lib\\site-packages\\IPython\\core\\formatters.py:708\u001b[0m, in \u001b[0;36mPlainTextFormatter.__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    701\u001b[0m stream \u001b[38;5;241m=\u001b[39m StringIO()\n\u001b[0;32m    702\u001b[0m printer \u001b[38;5;241m=\u001b[39m pretty\u001b[38;5;241m.\u001b[39mRepresentationPrinter(stream, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnewline,\n\u001b[0;32m    704\u001b[0m     max_seq_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_length,\n\u001b[0;32m    705\u001b[0m     singleton_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msingleton_printers,\n\u001b[0;32m    706\u001b[0m     type_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtype_printers,\n\u001b[0;32m    707\u001b[0m     deferred_pprinters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeferred_printers)\n\u001b[1;32m--> 708\u001b[0m \u001b[43mprinter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpretty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m printer\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m stream\u001b[38;5;241m.\u001b[39mgetvalue()\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensoflow\\lib\\site-packages\\IPython\\lib\\pretty.py:410\u001b[0m, in \u001b[0;36mRepresentationPrinter.pretty\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    407\u001b[0m                         \u001b[38;5;28;01mreturn\u001b[39;00m meth(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[0;32m    408\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mobject\u001b[39m \\\n\u001b[0;32m    409\u001b[0m                         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__dict__\u001b[39m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__repr__\u001b[39m\u001b[38;5;124m'\u001b[39m)):\n\u001b[1;32m--> 410\u001b[0m                     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_repr_pprint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcycle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _default_pprint(obj, \u001b[38;5;28mself\u001b[39m, cycle)\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\tensoflow\\lib\\site-packages\\IPython\\lib\\pretty.py:778\u001b[0m, in \u001b[0;36m_repr_pprint\u001b[1;34m(obj, p, cycle)\u001b[0m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"A pprint that just redirects to the normal repr function.\"\"\"\u001b[39;00m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;66;03m# Find newlines and replace them with p.break_()\u001b[39;00m\n\u001b[1;32m--> 778\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mrepr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m lines \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msplitlines()\n\u001b[0;32m    780\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgroup():\n",
      "File \u001b[1;32m~\\Jupyter\\escarabajos\\gamma\\metrics.py:75\u001b[0m, in \u001b[0;36mGamma_Arbitrary_Limits.__repr__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__repr__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGamma arbitrary limits, value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_metric_value()\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspectrum\u001b[38;5;241m.\u001b[39mgenus\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspectrum\u001b[38;5;241m.\u001b[39mspecies\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. File: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspectrum\u001b[38;5;241m.\u001b[39mfilename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32m~\\Jupyter\\escarabajos\\gamma\\metrics.py:18\u001b[0m, in \u001b[0;36mMetric.get_metric_value\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_metric_value\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetric_value\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Gamma_Arbitrary_Limits' object has no attribute 'metric_value'"
     ]
    }
   ],
   "source": [
    "import metrics\n",
    "spectrum = training_spectra[0]\n",
    "metric_1 = Gamma_Arbitrary_Limits(spectrum)\n",
    "metric_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3a45db8-0a26-430d-9fdc-c638f18bf2a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Gamma_Arbitrary_Limits' object has no attribute 'metric_value'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m scalar_metrics \u001b[38;5;241m=\u001b[39m [Gamma_Arbitrary_Limits,Gamma_First_Two_Peaks,Gamma_Area_Under_Curve_Naive, Gamma_Area_Under_Curve_First_Min_Cut]\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#scalar\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m gamma_arbitrary_limits_data \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_and_label_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGamma_Arbitrary_Limits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining_spectra\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m gamma_first_two_peaks_data \u001b[38;5;241m=\u001b[39m feature_and_label_extractor(Gamma_First_Two_Peaks, training_spectra)\n\u001b[0;32m      8\u001b[0m gamma_area_under_curve_data \u001b[38;5;241m=\u001b[39m feature_and_label_extractor(Gamma_Area_Under_Curve_Naive, training_spectra)\n",
      "File \u001b[1;32m~\\Jupyter\\escarabajos\\gamma\\metrics.py:89\u001b[0m, in \u001b[0;36mfeature_and_label_extractor\u001b[1;34m(Metric, spectra)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m spectrum \u001b[38;5;129;01min\u001b[39;00m spectra:\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;66;03m#spectrum.plot()\u001b[39;00m\n\u001b[0;32m     88\u001b[0m     metric \u001b[38;5;241m=\u001b[39m Metric(spectrum)\n\u001b[1;32m---> 89\u001b[0m     feature \u001b[38;5;241m=\u001b[39m \u001b[43mmetric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_metric_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m     label \u001b[38;5;241m=\u001b[39m spectrum\u001b[38;5;241m.\u001b[39mget_species()\n\u001b[0;32m     91\u001b[0m     code \u001b[38;5;241m=\u001b[39m spectrum\u001b[38;5;241m.\u001b[39mcode\n",
      "File \u001b[1;32m~\\Jupyter\\escarabajos\\gamma\\metrics.py:18\u001b[0m, in \u001b[0;36mMetric.get_metric_value\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_metric_value\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetric_value\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Gamma_Arbitrary_Limits' object has no attribute 'metric_value'"
     ]
    }
   ],
   "source": [
    "### Define species list\n",
    "species_list = [\"kalinini\",\"resplendens\", \"cupreomarginata\"]\n",
    "### Training data \n",
    "scalar_metrics = [Gamma_Arbitrary_Limits,Gamma_First_Two_Peaks,Gamma_Area_Under_Curve_Naive, Gamma_Area_Under_Curve_First_Min_Cut]\n",
    "#scalar\n",
    "gamma_arbitrary_limits_data = feature_and_label_extractor(Gamma_Arbitrary_Limits, training_spectra)\n",
    "gamma_first_two_peaks_data = feature_and_label_extractor(Gamma_First_Two_Peaks, training_spectra)\n",
    "gamma_area_under_curve_data = feature_and_label_extractor(Gamma_Area_Under_Curve_Naive, training_spectra)\n",
    "gamma_area_under_curve_first_min_cut_data = feature_and_label_extractor(Gamma_Area_Under_Curve_First_Min_Cut, training_spectra)\n",
    "\n",
    "#vectorial\n",
    "vectorial_metrics = [Wavelength_Vector, Maximum_Points, Minimum_Points, Maximum_Points_Normalized, Minimum_Points_Normalized, Critical_Points ]\n",
    "\n",
    "gamma_vector_relative_reflectance_data = feature_and_label_extractor(Gamma_Vector_Relative_Reflectance, training_spectra)\n",
    "wavelength_vector_data = feature_and_label_extractor(Wavelength_Vector, training_spectra)\n",
    "critical_points_data = feature_and_label_extractor(Critical_Points, training_spectra)\n",
    "\n",
    "maximum_points_data = feature_and_label_extractor(Maximum_Points, training_spectra)\n",
    "minimum_points_data = feature_and_label_extractor(Minimum_Points, training_spectra)\n",
    "maximum_points_normalized_data =feature_and_label_extractor(Maximum_Points_Normalized, training_spectra)\n",
    "minimum_points_normalized_data =feature_and_label_extractor(Minimum_Points_Normalized, training_spectra)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8158fc-31bc-497f-a63a-aec9da99fb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_aggregate(data):\n",
    "    \n",
    "    length = len(data[0])\n",
    "\n",
    "    data_points = []\n",
    "\n",
    "    #for each specimen\n",
    "    \n",
    "    for i in range(0, length): \n",
    "        code = data[0][i]\n",
    "        vector = data[1][i]\n",
    "        species =data[2][i]\n",
    "\n",
    "        data_point = {}\n",
    "        data_point[\"code\"] = code\n",
    "        data_point[\"vector\"] = vector\n",
    "        data_point[\"species\"] = species\n",
    "\n",
    "        data_points.append(data_point)\n",
    "\n",
    "    #Now, for each species \n",
    "    aggregates = {}\n",
    "    \n",
    "    for species in [\"kalinini\", \"resplendens\", \"cupreomarginata\"]:\n",
    "        \n",
    "        specimens = [x for x in data_points if x[\"species\"] == species]\n",
    "\n",
    "        #extract vectors\n",
    "        vectors = [element[\"vector\"] for element in specimens]\n",
    "\n",
    "        #first entry\n",
    "        vector_first_entries =  [element[0] for element in vectors]\n",
    "        vector_second_entries =  [element[1] for element in vectors]\n",
    "        \n",
    "        #get max length of \n",
    "        #print(type(vector_first_entries[0]))\n",
    "        if not isinstance((vector_first_entries[0]), np.float64):\n",
    "            max_length_x = max([len(x) for x in vector_first_entries])\n",
    "            max_length_y = max([len(x) for x in vector_second_entries])\n",
    "        else:\n",
    "            max_length_x = 1\n",
    "            max_length_y = 1\n",
    "        \n",
    "        #get number of vectors\n",
    "        number_of_specimens = len(specimens)\n",
    "        \n",
    "        #add zeroes\n",
    "        #for first entry\n",
    "        \n",
    "        new_subset_vectors = []\n",
    "        \n",
    "        new_vector_first_entry = []\n",
    "        for first_entry_i in vector_first_entries:\n",
    "            #for the first and second entry\n",
    "            if isinstance(first_entry_i, np.float64):\n",
    "                length_first_entry = 1\n",
    "                number_of_zeroes = max_length_x - length_first_entry\n",
    "                extend_vector = np.array([0]*number_of_zeroes)\n",
    "                #print(extend_vector)\n",
    "                #print(first_entry_i)\n",
    "                first_entry_i = np.concatenate(([first_entry_i], extend_vector))\n",
    "                new_vector_first_entry.append(first_entry_i)\n",
    "            else:\n",
    "                length_first_entry = len(first_entry_i)\n",
    "                number_of_zeroes = max_length_x - length_first_entry\n",
    "                extend_vector = np.array([0]*number_of_zeroes)\n",
    "                #print(extend_vector)\n",
    "                #print(first_entry_i)\n",
    "                first_entry_i = np.concatenate((first_entry_i, extend_vector))\n",
    "                new_vector_first_entry.append(first_entry_i)\n",
    "\n",
    "        new_vector_second_entry =  []\n",
    "        \n",
    "        for second_entry_i in vector_second_entries:\n",
    "            #for the first and second entry\n",
    "            if isinstance(second_entry_i, np.float64):\n",
    "                length_second_entry = 1\n",
    "                number_of_zeroes = max_length_x - length_second_entry\n",
    "                extend_vector = np.array([0]*number_of_zeroes)\n",
    "                second_entry_i = np.concatenate(([second_entry_i], extend_vector))\n",
    "                new_vector_second_entry.append(second_entry_i)\n",
    "            else:\n",
    "                length_second_entry = len(second_entry_i)\n",
    "                number_of_zeroes = max_length_x - length_second_entry\n",
    "                extend_vector = np.array([0]*number_of_zeroes)\n",
    "                second_entry_i = np.concatenate((second_entry_i, extend_vector))\n",
    "                new_vector_second_entry.append(second_entry_i)\n",
    "        #print(f\"{new_vector_first_entry=}\")\n",
    "        #print(f\"{new_vector_second_entry=}\")\n",
    "\n",
    "        #now calculate averages \n",
    "\n",
    "        x_averages = []\n",
    "        x_std = []\n",
    "        for i in range(max_length_x): \n",
    "            vector_i = []\n",
    "\n",
    "            for n in range(0, number_of_specimens):\n",
    "                value_n = new_vector_first_entry[n][i]\n",
    "                if not( (value_n < 0.1) & (value_n > -0.1) ) : #if value is not zero\n",
    "                    vector_i.append(value_n)\n",
    "            #then get the total of elements, convert it into a numpy array , calculate the average. \n",
    "            x_averages.append(np.mean(np.array(vector_i)))\n",
    "            x_std.append(np.std(np.array(vector_i)))\n",
    "            \n",
    "        y_averages =[]\n",
    "        y_std = []\n",
    "        \n",
    "        for i in range(max_length_y): \n",
    "            vector_x = []\n",
    "            for n in range(0, number_of_specimens):\n",
    "                value_n = new_vector_second_entry[n][i]\n",
    "                if not( (value_n < 0.1) & (value_n > -0.1) ) : #if value is not zero\n",
    "                    vector_x.append(value_n)\n",
    "            #then get the total of elements, convert it into a numpy array , calculate the average. \n",
    "            y_averages.append(np.mean(np.array(vector_x)))\n",
    "            y_std.append(np.std(np.array(vector_x)))\n",
    "        \n",
    "        info = np.array( [x_averages, y_averages, x_std, y_std]).T\n",
    "        df = pd.DataFrame(info, columns= [f\"{species}_x_avg\", f\"{species}_y_avg\", f\"{species}_x_std\", f\"{species}_y_std\"])\n",
    "        aggregates[species] = df\n",
    "        #print(aggregates)\n",
    "    df_2 = pd.DataFrame([])\n",
    "    for element in aggregates:\n",
    "        df = (aggregates[element])\n",
    "        df_2 = pd.concat([df, df_2], axis=1)\n",
    "        #print(df_2)\n",
    "    return df_2\n",
    "    \n",
    "\n",
    "def save_vector_aggregate(metric_class, spectra, agregated_data_location):\n",
    "    #get metric values for spectra\n",
    "    data = feature_and_label_extractor(metric_class, spectra)\n",
    "    #create vector aggregate df\n",
    "    df = vector_aggregate(data)  \n",
    "    #create path location\n",
    "    path_location = os.path.join(agregated_data_location, \"metric_avg_std\")\n",
    "    create_path_if_not_exists(path_location)\n",
    "    path_and_filename = os.path.join( path_location, f'{metric_class.get_name()}')\n",
    "    #save to csv\n",
    "    df.to_csv( path_and_filename, index=False, sep = \"\\t\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4aec18-1158-4502-a4b2-bf5b538a564c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in vectorial_metrics: \n",
    "    save_vector_aggregate(metric, training_spectra, agregated_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba06b25-64e0-49cf-83fe-65fec67eee74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save averages and std for training data\n",
    "metrics_list = [Gamma_Arbitrary_Limits, Gamma_First_Two_Peaks, Gamma_Area_Under_Curve_Naive, Gamma_Area_Under_Curve_First_Min_Cut, \n",
    "                Gamma_Area_Under_Curve_First_Min_Cut]\n",
    "\n",
    "for metric in metrics_list:\n",
    "    save_aggregated_data(metric,training_spectra,agregated_data_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef85ed7-7afc-459d-ba90-9ff83f0e318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read averages and std\n",
    "avg_std_location = os.path.join(agregated_data_location, \"metric_avg_std\")\n",
    "aggregate_data_df = read_aggregated_data(avg_std_location)\n",
    "#dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a84c93-2744-482d-94e0-3a4f775f7e2c",
   "metadata": {},
   "source": [
    "#get metrics keys\n",
    "metric_keys = (dfs.keys())\n",
    "print(metric_keys)\n",
    "#get avg and std dataframe\n",
    "avg_std_df_x = dfs[\"Maximum_Points\"]\n",
    "#print(avg_df)\n",
    "species_list = [\"kalinini\", \"resplendens\", \"cupreomarginata\"]\n",
    "spectrum_peaks = np.array(prediction_spectra[1].get_maxima())\n",
    "n = 5\n",
    "#print(spectrum)\n",
    "similarity_index(spectrum_peaks, avg_std_df_x, species_list, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d9a3e4-0901-41b1-b67e-1a08e29d7950",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(df):\n",
    "    #print(f\"{species =} \\n {df=}\" )\n",
    "    \"\"\"Accuracy: Percentage of correct guesses. \"\"\"\n",
    "    #filter all predictions of that particular species\n",
    "    \n",
    "    total_of_predictions = df[\"prediction\"].count()\n",
    "    #print(f\"{total_of_predictions=}\" )\n",
    "    #Now count the amount of actual species\n",
    "    correct_predictions = df[df[\"species\"] == df[\"prediction\"]][\"species\"].count()\n",
    "    #print(f\"{correct_predictions=}\" )\n",
    "    #print(f\"{total_of_predictions=}\" )\n",
    "    accuracy = correct_predictions/total_of_predictions*100\n",
    "    #print(f\"{accuracy=}\" )\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb78919-d3db-440d-bda6-dd2ffcbc1aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision(df, species):\n",
    "    #print(f\"{species =} \\n {df=}\" )\n",
    "    \"\"\"Precision: For a given prediction, what percentage is actually for that species. \"\"\"\n",
    "    #filter all predictions of that particular species\n",
    "    filtered_df = df[df[\"prediction\"] == species]\n",
    "    total_of_predictions = filtered_df[\"prediction\"].count()\n",
    "    #print(f\"{total_of_predictions=}\" )\n",
    "    #Now count the amount of actual species\n",
    "    \n",
    "    actual_species = filtered_df[filtered_df[\"species\"] == species][\"species\"].count()\n",
    "    precision = actual_species/total_of_predictions*100\n",
    "    #print(f\"{actual_species=}\" )\n",
    "    #print(f\"{total_of_predictions=}\" )\n",
    "    \n",
    "    #print(f\"{precision=}\" )\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3276d2d2-3bdf-48f6-8b8f-95088968fd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_recall(df, species):\n",
    "    #print(f\"{species =} \\n {df=}\" )\n",
    "    \"\"\"recall: For a given species, what percentage was correctly characterized. \"\"\"\n",
    "    #filter all lines of a particular species\n",
    "    filtered_df = df[df[\"species\"] == species]\n",
    "    total_of_species = filtered_df[\"species\"].count()\n",
    "    #print(f\"{total_of_species=}\" )\n",
    "    #Count the amount of correct predictions\n",
    "    correct_predictions = filtered_df[filtered_df[\"prediction\"] == species][\"prediction\"].count()\n",
    "    recall = correct_predictions/total_of_species*100\n",
    "    #print(f\"{recall=}\" )\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e80b45-ff5c-4ee4-9291-bd9b9e7fd527",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_index(spectrum, metric, aggregate_data_df , species_list, n):\n",
    "    avg_std_df = aggregate_data_df\n",
    "    spectrum_peaks = metric(spectrum).get_metric_value()\n",
    "    \n",
    "    def distance(x, x0, sigma_0):\n",
    "        \n",
    "        x = float(x)\n",
    "        x0 = float(x0[0])\n",
    "        sigma_0 = float(sigma_0[0])\n",
    "\n",
    "        #numerator = (np.abs(x - x0))\n",
    "        #denominator = float(sigma_0 + np.abs(x - x0) +0.1)\n",
    "        numerator = ((x - x0)**2)\n",
    "        denominator = 1\n",
    "        \n",
    "        try:\n",
    "            index = numerator/denominator\n",
    "        except:\n",
    "            return 0\n",
    "        return index\n",
    "        #def get_gamma_factor(self, spectrum):\n",
    "    \n",
    "    #get first n peaks of the gamma\n",
    "    x = spectrum_peaks[0][0:n]\n",
    "    y = spectrum_peaks[1][0:n]\n",
    "\n",
    "    #for each species\n",
    "    similarity_index_ret = {} #define return dict\n",
    "    \n",
    "    for spec in species_list:\n",
    "        #print(spec)\n",
    "        \n",
    "        #print(averages_df.columns)\n",
    "        x_avg_column_names = [name for name in (avg_std_df.columns) if (\"avg\" in name) and (spec in name) and (\"x\" in name) ]\n",
    "        y_avg_column_names = [name for name in (avg_std_df.columns) if (\"avg\" in name) and (spec in name) and (\"y\" in name) ]\n",
    "        #print(x_avg_column_names)\n",
    "\n",
    "        x_std_column_names = [name for name in avg_std_df.columns if (\"std\" in name) and (spec in name) and (\"x\" in name)]\n",
    "        y_std_column_names = [name for name in avg_std_df.columns if (\"std\" in name) and (spec in name) and (\"y\" in name)]\n",
    "        #print(x_std_column_names)\n",
    "\n",
    "        #load species average values\n",
    "\n",
    "        x_avg = (avg_std_df[x_avg_column_names].dropna().values[0:n])\n",
    "        y_avg = avg_std_df[y_avg_column_names].dropna().values[0:n]\n",
    "\n",
    "        #load species std values:\n",
    "        x_std = avg_std_df[x_std_column_names].dropna().values[0:n]\n",
    "        y_std = avg_std_df[y_std_column_names].dropna().values[0:n]\n",
    "\n",
    "        #for each peak, calculate the distances to the averages x values\n",
    "        similarity_index_x_species = 0.0\n",
    "        similarity_index_y_species = 0.0\n",
    "        similarity_index_z_species = 0.0\n",
    "        #calculate similarity_index for \n",
    "        \n",
    "      \n",
    "        #print(x, x_avg, x_std)\n",
    "        for n_i ,n_0, sigma_n_0 in zip(x, x_avg, x_std):\n",
    "            similarity_index_x_species += distance(n_i, n_0, sigma_n_0)\n",
    "            #similarity_index_x_species += 1/distance(n_i, n_0, sigma_n_0)\n",
    "            \n",
    "            \n",
    "        #calculate similarity_index for y\n",
    "        for n_i ,n_0, sigma_n_0 in zip(y, y_avg, y_std):\n",
    "            similarity_index_y_species += distance(n_i, n_0, sigma_n_0)\n",
    "            #similarity_index_y_species+= 1/distance(n_i, n_0, sigma_n_0)\n",
    "\n",
    "        #calculate similarity_index for z**2 = x**2 + y**2\n",
    "        for x_i, x_0, y_i, y_0 in zip(x, x_avg, y, y_avg):\n",
    "            similarity_index_z_species += ((x_i-x_0)**2 + (y_i-y_0)**2)\n",
    "            #similarity_index_y_species+= 1/distance(n_i, n_0, sigma_n_0)\n",
    "        \n",
    "        #arithmetic average\n",
    "        similarity_index_x_species = similarity_index_x_species*(1/ float(n) )\n",
    "        similarity_index_y_species = similarity_index_y_species*(1/ float(n) )\n",
    "        similarity_index_z_species = similarity_index_z_species*(1/ float(n) )\n",
    "        \n",
    "        #geometric average\n",
    "        #similarity_index_x_species = similarity_index_x_species**(1/ float(n) )\n",
    "        #similarity_index_y_species = similarity_index_y_species**(1/ float(n) )\n",
    "        \n",
    "        #Calculate harmonic average\n",
    "        #similarity_index_x_species = 1/similarity_index_x_species\n",
    "        #similarity_index_y_species = 1/similarity_index_y_species\n",
    "\n",
    "        #add to dictionary\n",
    "        similarity_index_ret[spec] =[similarity_index_x_species, similarity_index_y_species,similarity_index_z_species]\n",
    "  \n",
    "    return similarity_index_ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314f1aec-ef55-4d21-bf11-dead38e9eff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for spectrum_i in training_spectra:\n",
    "    similarity_index_df_xy = similarity_index(spectrum = spectrum_i, metric = Critical_Points, aggregate_data_df = aggregate_data_df , species_list = species_list, n = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8cbf28-4422-4095-af2a-5402c3d8608d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorial_manual_classifier_1(spectrum, avg_std_df, metric_class, species_list, type_of_sim_index = 1):\n",
    "    #get metrics keys\n",
    "    keys = (avg_std_df.keys())\n",
    "    #print(keys)\n",
    "    \n",
    "    metric_name = metric_class.get_name()\n",
    "    #print(metric_name)\n",
    "    #get avg and std dataframe\n",
    "\n",
    "    #print(metric_name)\n",
    "    avg_std_df_metric = avg_std_df[metric_name]\n",
    "    #print(avg_std_df)\n",
    "    \n",
    "    #print(avg_df)\"\n",
    "    spectrum_peaks = np.array(metric_class(spectrum).metric_value)\n",
    "    #print(spectrum_peaks)\n",
    "    n = 5\n",
    "    #print(spectrum)\n",
    "    sim_index = similarity_index(spectrum_peaks, avg_std_df_metric, species_list, n)\n",
    "\n",
    "    for element in sim_index:\n",
    "        if type_of_sim_index == 0:\n",
    "            sim_index[element] = sim_index[element][0] \n",
    "        elif type_of_sim_index == 1:\n",
    "            sim_index[element] = sim_index[element][1] \n",
    "        elif type_of_sim_index == 2:\n",
    "            sim_index[element] = sim_index[element][1] *sim_index[element][0]\n",
    "        elif type_of_sim_index == 3:\n",
    "            sim_index[element] = sim_index[element][2]\n",
    "\n",
    "    min_key = min(sim_index, key=sim_index.get)\n",
    "    return min_key\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8cc282-278a-49c5-a45a-a99036cdb6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stats_for_metric(metric_list, spectra, avg_std_df, type_of_sim_index = 1 ):\n",
    "    real_species = ([item.species for item in spectra]) #List of the correct species\n",
    "\n",
    "    return_df = pd.DataFrame([])\n",
    "    \n",
    "    for metric_class in metric_list:\n",
    "        #get metric name\n",
    "        metric_name = metric_class.get_name()\n",
    "        #print(metric_name)\n",
    "        #get metric predictions\n",
    "        \n",
    "        predictions = [vectorial_manual_classifier_1(item, avg_std_df, metric_class, species_list,type_of_sim_index) for item in spectra]\n",
    "        #print(f\"{predictions=}\")\n",
    "        #predictions = [item for item in spectra ]\n",
    "        \n",
    "        #create dataframe with predictions and real species \n",
    "        species_and_predictions_df = pd.DataFrame(np.array([real_species, predictions]).T, columns=[\"species\",\"prediction\"] ) \n",
    "        \n",
    "        stats = {}\n",
    "        stats[\"metric\"] = metric_name\n",
    "        #calculate accuracy\n",
    "        accuracy = calculate_accuracy(species_and_predictions_df)\n",
    "        stats[\"accuracy\"] = accuracy\n",
    "        \n",
    "        #for each species, get column names and recall and precision\n",
    "        for species in species_list:\n",
    "            stats[f\"{species}_recall\"] =calculate_recall(species_and_predictions_df, species)\n",
    "            stats[f\"{species}_precision\"] =calculate_precision(species_and_predictions_df, species)\n",
    "        #save stats\n",
    "        #create empty dataframe\n",
    "        stats_df = pd.DataFrame([stats]) \n",
    "        #print(stats_df)\n",
    "        return_df = pd.concat([return_df,stats_df]) \n",
    "    #print(return_df)\n",
    "    return return_df\n",
    "avg_std_df= read_aggregated_data(avg_std_location)\n",
    "met_list = [Critical_Points, Maximum_Points, Maximum_Points_Normalized, Minimum_Points, Minimum_Points_Normalized]\n",
    "#met_list = [Critical_Points]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3112c8a3-7d00-46d1-9e2e-7b5149ea73ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "(get_stats_for_metric(met_list, training_spectra, avg_std_df , type_of_sim_index = 0 ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a54e2f-38ba-4944-a178-8111102a98df",
   "metadata": {},
   "outputs": [],
   "source": [
    "(get_stats_for_metric(met_list, training_spectra, avg_std_df , type_of_sim_index = 1 ))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dce7994-ea31-4b70-8b15-00e461013f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "(get_stats_for_metric(met_list, training_spectra, avg_std_df , type_of_sim_index = 2 ) ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbea8c4f-fc18-4899-afed-6034143e621b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(get_stats_for_metric(met_list, training_spectra, avg_std_df , type_of_sim_index = 3 ) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57bbf964-9252-4df4-805a-f88fd8067104",
   "metadata": {},
   "source": [
    "## Metric: Maximum_Points_Normalized\n",
    "### Just wavelength\n",
    "#### first metric:  ((x - x0)**2) arith avg\n",
    "accuracy=46.42857142857143\n",
    "kalinini\n",
    "recall=0.0\n",
    "precision=nan\n",
    "\n",
    "\n",
    "resplendens\n",
    "recall=63.63636363636363\n",
    "precision=43.75\n",
    "\n",
    "\n",
    "cupreomarginata\n",
    "recall=75.0\n",
    "precision=50.0\n",
    "\n",
    "\n",
    "### Just reflectance\n",
    "#### first metric:  ((x - x0)**2) arith avg\n",
    "accuracy=57.14285714285714\n",
    "kalinini\n",
    "recall=88.88888888888889\n",
    "precision=80.0\n",
    "\n",
    "\n",
    "resplendens\n",
    "recall=18.181818181818183\n",
    "precision=40.0\n",
    "\n",
    "\n",
    "cupreomarginata\n",
    "recall=75.0\n",
    "precision=46.15384615384615\n",
    "#### second metric: ((x - x0)**2) geo avg\n",
    "accuracy=32.142857142857146\n",
    "kalinini\n",
    "recall=100.0\n",
    "precision=32.142857142857146\n",
    "\n",
    "\n",
    "resplendens\n",
    "recall=0.0\n",
    "precision=nan\n",
    "\n",
    "\n",
    "cupreomarginata\n",
    "recall=0.0\n",
    "precision=nan\n",
    "#### third metric:  numerator = (np.abs(x - x0)) geometric avg\n",
    "        denominator = float(sigma_0 + np.abs(x - x0) +0.1)\n",
    "\n",
    "accuracy=32.142857142857146\n",
    "kalinini\n",
    "recall=100.0\n",
    "precision=32.142857142857146\n",
    "\n",
    "\n",
    "resplendens\n",
    "recall=0.0\n",
    "precision=nan\n",
    "\n",
    "\n",
    "cupreomarginata\n",
    "recall=0.0\n",
    "precision=nan\n",
    "\n",
    "\n",
    "### Multipliying wavelength and reflectance\n",
    "#### first metric:  \n",
    "numerator = float(np.abs(x - x0))\n",
    "denominator = float(sigma_0 + np.abs(x - x0) +1)\n",
    "\n",
    "accuracy=50.0\n",
    "kalinini\n",
    "recall=0.0\n",
    "precision=nan\n",
    "\n",
    "\n",
    "resplendens\n",
    "recall=90.9090909090909\n",
    "precision=43.47826086956522\n",
    "\n",
    "\n",
    "cupreomarginata\n",
    "recall=50.0\n",
    "precision=80.0\n",
    "\n",
    "\n",
    "#### second metric ((x - x0)**2) arith avg (mejores)\n",
    "\n",
    "accuracy=64.28571428571429\n",
    "kalinini\n",
    "recall=55.55555555555556\n",
    "precision=100.0\n",
    "\n",
    "\n",
    "resplendens\n",
    "recall=63.63636363636363\n",
    "precision=63.63636363636363\n",
    "\n",
    "\n",
    "cupreomarginata\n",
    "recall=75.0\n",
    "precision=50.0\n",
    "\n",
    "\n",
    "\n",
    "#### third metric ((x - x0)**2) geometric avg\n",
    "\n",
    "kalinini\n",
    "accuracy=53.57142857142857\n",
    "recall=22.22222222222222\n",
    "precision=66.66666666666666\n",
    "resplendens\n",
    "accuracy=53.57142857142857\n",
    "recall=72.72727272727273\n",
    "precision=47.05882352941176\n",
    "cupreomarginata\n",
    "accuracy=53.57142857142857\n",
    "recall=62.5\n",
    "precision=62.5\n",
    "\n",
    "#### fourth metric abs((x - x0)) arith avg\n",
    "\n",
    "kalinini\n",
    "accuracy=53.57142857142857\n",
    "recall=22.22222222222222\n",
    "precision=66.66666666666666\n",
    "resplendens\n",
    "accuracy=53.57142857142857\n",
    "recall=72.72727272727273\n",
    "precision=47.05882352941176\n",
    "cupreomarginata\n",
    "accuracy=53.57142857142857\n",
    "recall=62.5\n",
    "precision=62.5\n",
    "\n",
    "### Conclusions: \n",
    "(x-x0)**2 and arith avg gives the best accuracies. Now reflectance allows us to detect kalinini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42be5642-9c7f-4ab6-a80e-bf266f97035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction data\n",
    "\n",
    "prediction_gamma_arbitrary_limits_data = feature_and_label_extractor(Gamma_Arbitrary_Limits, prediction_spectra)\n",
    "prediction_gamma_first_two_peaks_data = feature_and_label_extractor(Gamma_First_Two_Peaks, prediction_spectra)\n",
    "prediction_gamma_area_under_curve_data = feature_and_label_extractor(Gamma_Area_Under_Curve_Naive, prediction_spectra)\n",
    "prediction_gamma_area_under_curve_first_min_cut_data = feature_and_label_extractor(Gamma_Area_Under_Curve_First_Min_Cut, prediction_spectra)\n",
    "prediction_gamma_vector_relative_reflectance_data = feature_and_label_extractor(Gamma_Vector_Relative_Reflectance, prediction_spectra)\n",
    "prediction_critical_points_data = feature_and_label_extractor(Critical_Points, prediction_spectra)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a690e936-3ba5-4db4-90d2-201fe5796470",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(critical_points_data)\n",
    "pad_list(critical_points_data, filler = np.array([0,0]))\n",
    "pad_list(gamma_vector_relative_reflectance_data)\n",
    "data = pad_list(wavelength_vector_data)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b5cce2-8547-4314-b054-22f7bbc29acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.version_info)\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# The following lines adjust the granularity of reporting. \n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = \"{:.1f}\".format\n",
    "\n",
    "# The following line improves formatting when ouputting NumPy arrays.\n",
    "np.set_printoptions(linewidth = 200)\n",
    "\n",
    "def replace_strings_3(lst):\n",
    "    mapping = {\"kalinini\": 0, \"resplendens\": 1, \"cupreomarginata\": 2}\n",
    "    return [mapping.get(item, item) for item in lst]\n",
    "\n",
    "def replace_species_with_categorical(df):\n",
    "    \n",
    "    df.loc[df[\"species\"]==\"kalinini\",\"species\"] = 0\n",
    "    df.loc[df[\"species\"]==\"resplendens\", \"species\"] = 1\n",
    "    df.loc[df[\"species\"]==\"cupreomarginata\", \"species\"] = 2\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def plot_curve(epochs, hist, list_of_metrics):\n",
    "  \"\"\"Plot a curve of one or more classification metrics vs. epoch.\"\"\"  \n",
    "  # list_of_metrics should be one of the names shown in:\n",
    "  # https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#define_the_model_and_metrics  \n",
    "\n",
    "  plt.figure()\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(\"Value\")\n",
    "\n",
    "  for m in list_of_metrics:\n",
    "    x = hist[m]\n",
    "    plt.plot(epochs[1:], x[1:], label=m)\n",
    "\n",
    "  plt.legend()\n",
    "\n",
    "print(\"Loaded the plot_curve function.\")\n",
    "\n",
    "def transpose_list(lst):\n",
    "    return list(zip(*lst))\n",
    "    \n",
    "def get_nth_feature(data, n):\n",
    "    feature_vector = [data[0], [x[n] for x in data[1]] , data[2]] \n",
    "    return feature_vector\n",
    "def scatter_plot_2_variables(df_1, df_2):\n",
    "\n",
    "    joint_df = pd.merge(df_1, df_2, on=[\"code\", \"species\"], how=\"inner\")\n",
    "    column_list = joint_df.columns.tolist()\n",
    "    print(column_list)\n",
    "    column_list = [x for x in column_list if x not in [\"code\", \"species\"] ]\n",
    "    print(column_list)\n",
    "    plt.figure()\n",
    "    sns.scatterplot(joint_df, x=column_list[0], y =column_list[1], hue=\"species\")\n",
    "    plt.show()\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def scatter_plot_3_variables(df_1, df_2, df_3):\n",
    "    \n",
    "    joint_df = pd.merge(df_1, df_2, on=[\"code\", \"species\"], how=\"inner\")\n",
    "    joint_df = pd.merge(joint_df, df_3, on=[\"code\", \"species\"], how=\"inner\")\n",
    "    \n",
    "    column_list = joint_df.columns.tolist()\n",
    "    print(column_list)\n",
    "    column_list = [x for x in column_list if x not in [\"code\", \"species\"] ]\n",
    "    print(column_list)\n",
    "    \n",
    "    x = joint_df[column_list[0]]\n",
    "    y = joint_df[column_list[1]]\n",
    "    z = joint_df[column_list[2]]\n",
    "    species = joint_df[\"species\"]\n",
    "    \n",
    "    # Create color map\n",
    "    colors = {'kalinini': 'r', 'resplendens': 'g', 'cupreomarginata': 'b'}\n",
    "    \n",
    "    # Create figure and 3D axis\n",
    "    fig = plt.figure(figsize=(15, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Plot points with color based on the fourth dimension\n",
    "    for category in set(species):\n",
    "        indices = species == category\n",
    "        ax.scatter(x[indices], y[indices], z[indices], c=colors[category], label=category, marker='o')\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xlabel(f'{column_list[0]}')\n",
    "    ax.set_ylabel(f'{column_list[1]}')\n",
    "    ax.set_zlabel(f'{column_list[2]}')\n",
    "    #ax.set_title('3D Scatter plot with species based on Fourth Dimension')\n",
    "    \n",
    "    # Add legend\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f559b3c-dca4-4724-9ce1-98d23f7df0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training data. Define dataframees\n",
    "\n",
    "#Use scalar metrics\n",
    "gal_df = pd.DataFrame(transpose_list(gamma_arbitrary_limits_data), columns =[\"code\", \"g_arbitrary_limits\", \"species\"]) \n",
    "gftp_df = pd.DataFrame(transpose_list(gamma_first_two_peaks_data), columns =[\"code\", \"g_first_2_peaks\", \"species\"])\n",
    "gaucfmc_df = pd.DataFrame(transpose_list(gamma_area_under_curve_first_min_cut_data), columns =[\"code\", \"g_area_und_curve_first_min\", \"species\"]) \n",
    "gauc_df = pd.DataFrame(transpose_list(gamma_area_under_curve_data), columns =[\"code\", \"g_area_und_curve\", \"species\"])\n",
    "similarity_index_xy_df = pd.DataFrame(transpose_list())\n",
    "\n",
    "#merge scalar metrics into a single dataframe\n",
    "complete_df = pd.merge(gal_df, gftp_df, on=[\"species\",\"code\"], how=\"inner\")\n",
    "complete_df = pd.merge(complete_df, gauc_df, on=[\"species\",\"code\"], how=\"inner\")\n",
    "complete_df = pd.merge(complete_df, gaucfmc_df, on=[\"species\",\"code\"], how=\"inner\")\n",
    "complete_df.drop(columns=['code'], inplace=True)\n",
    "\n",
    "complete_df = replace_species_with_categorical(complete_df)\n",
    "#complete_df.drop(columns=['species'], inplace=True)\n",
    "complete_df\n",
    "\n",
    "#shuffle the df\n",
    "shuffled_df = complete_df.sample(frac=1, random_state=42)  # Random_state for reproducibility\n",
    "\n",
    "# Define the fraction of data to be used for training\n",
    "train_fraction = 0.66  # For example, 80% for training, 20% for testing\n",
    "\n",
    "# Calculate the number of rows for the training set\n",
    "train_size = int(train_fraction * len(complete_df))\n",
    "\n",
    "# Split the shuffled DataFrame into train and test sets\n",
    "train_data = shuffled_df.iloc[:train_size]\n",
    "test_data = shuffled_df.iloc[train_size:]\n",
    "\n",
    "# Split the shuffled DataFrame into features and labels\n",
    "train_data_features = train_data.drop(columns=['species'], inplace=False)\n",
    "train_data_labels =  train_data.drop(columns=['g_arbitrary_limits','g_first_2_peaks','g_area_und_curve_first_min','g_area_und_curve'], inplace=False)\n",
    "test_data_features =  test_data.drop(columns=['species'], inplace=False)\n",
    "test_data_labels =  test_data.drop(columns=['g_arbitrary_limits','g_first_2_peaks','g_area_und_curve_first_min','g_area_und_curve'], inplace=False)\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Assuming y_train and y_test are your integer labels\n",
    "train_data_labels_one_hot = to_categorical(train_data_labels, num_classes=3)\n",
    "test_data_labels_one_hot = to_categorical(test_data_labels, num_classes=3)\n",
    "\n",
    "print(train_data[train_data_labels[\"species\"]==2])\n",
    "print(train_data_features[train_data_labels[\"species\"]==0])\n",
    "#print(len(train_data))\n",
    "#print(len(test_data))\n",
    "#print(test_features)\n",
    "#print(test_labels)\n",
    "\n",
    "# Select 2\n",
    "train_data_features = np.array(train_data_features.drop(columns=['g_arbitrary_limits',\"g_area_und_curve\"], inplace=False).values)\n",
    "test_data_features =  np.array(test_data_features.drop(columns=['g_arbitrary_limits',\"g_area_und_curve\"], inplace=False).values)\n",
    "\n",
    "print(train_data_features)\n",
    "#print(train_data_labels[train_data_labels[\"species\"]==0])\n",
    "print(train_data_labels_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af49d08-188d-4066-bb7d-c63d02c42f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_3(my_learning_rate):\n",
    "  \"\"\"Create and compile a deep neural net.\"\"\"\n",
    "  \n",
    "  # All models in this course are sequential.\n",
    "  model = tf.keras.models.Sequential()\n",
    "\n",
    "  # The features are stored in a two-dimensional 28X28 array. \n",
    "  # Flatten that two-dimensional array into a one-dimensional \n",
    "  # 784-element array.\n",
    "  model.add(tf.keras.layers.Flatten(input_shape=(2,)))\n",
    "\n",
    "  # Define the first hidden layer.   \n",
    "  model.add(tf.keras.layers.Dense(units=8, activation='relu'))\n",
    "\n",
    "  # Define the first hidden layer.   \n",
    "  model.add(tf.keras.layers.Dense(units=4, activation='relu'))\n",
    "\n",
    "  # Define the first hidden layer.   \n",
    "  model.add(tf.keras.layers.Dense(units=4, activation='relu'))\n",
    "  \n",
    "  # Define a dropout regularization layer. \n",
    "  model.add(tf.keras.layers.Dropout(rate=0.1))\n",
    "\n",
    "  # Define the output layer. The units parameter is set to 10 because\n",
    "  # the model must choose among 10 possible output values (representing\n",
    "  # the digits from 0 to 9, inclusive).\n",
    "  #\n",
    "  # Don't change this layer.\n",
    "  model.add(tf.keras.layers.Dense(units=3, activation='softmax'))     \n",
    "                           \n",
    "  # Construct the layers into a model that TensorFlow can execute.  \n",
    "  # Notice that the loss function for multi-class classification\n",
    "  # is different than the loss function for binary classification.  \n",
    "  #model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=my_learning_rate),loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
    "  model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "  return model    \n",
    "\n",
    "\n",
    "def train_model(model, train_features, train_label, epochs,\n",
    "                batch_size=None, validation_split=0.1):\n",
    "  \"\"\"Train the model by feeding it data.\"\"\"\n",
    "\n",
    "  history = model.fit(x=train_features, y=train_label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=True)\n",
    "                      #validation_split=validation_split)\n",
    " \n",
    "  # To track the progression of training, gather a snapshot\n",
    "  # of the model's metrics at each epoch. \n",
    "  epochs = history.epoch\n",
    "  hist = pd.DataFrame(history.history)\n",
    "\n",
    "  return epochs, hist    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43828edf-9500-4fde-a32c-50cc773f3d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.007\n",
    "epochs = 160\n",
    "batch_size = 3\n",
    "validation_split = 0.05\n",
    "\n",
    "# Establish the model's topography.\n",
    "my_model = create_model_3(learning_rate)\n",
    "\n",
    "# Train the model on the normalized training set.\n",
    "#epochs, hist = train_model(my_model, x_train, y_train, \n",
    "                           #epochs, batch_size, validation_split)\n",
    "epochs, hist = train_model(my_model, train_data_features, train_data_labels_one_hot, \n",
    "                           epochs, batch_size, validation_split)\n",
    "\n",
    "# Plot a graph of the metric vs. epochs.\n",
    "list_of_metrics_to_plot = ['accuracy']\n",
    "plot_curve(epochs, hist, list_of_metrics_to_plot)\n",
    "\n",
    "# Evaluate against the test set.\n",
    "print(\"\\n Evaluate the new model against the test set:\")\n",
    "my_model.evaluate(x=test_data_features, y=test_data_labels_one_hot, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f3c3ca-d4c4-4a0e-972b-e82897d912b5",
   "metadata": {},
   "source": [
    "### Notas: \n",
    "Mejores resultados usando el valor de batch size  4 o 3, acc 0.7\n",
    " Para 5, acc baja a 0.6\n",
    " Learning rate, buenos resultados en: 0.008-0.009. Acc 0.7\n",
    "aprendizaje inestable: Cambiamos learning rate a 0.007 y epochs a 600\n",
    " Pasar regularizaci√≥n de 0 a 0.1 hace que el aprendizaje sea m√°s suave\n",
    " Pasar de 4,4 a 8,4 hace el aprendizaje mas suave\n",
    " Pasar de 4,4 a 8,6 hace el aprendizaje menos efectivo\n",
    " Pasar de 4,4 a 8,4,4 hace el aprendizaje mas suave\n",
    "aumentar el validation split de 0.1 a 0.3 reduce el acc a 0.3 0.6 0.6\n",
    "bajar el validation split 0.1 a 0.05 reduce el acc a 0.5 0.6 0.7\n",
    "dejar el val. split en 0.1 deja acc en 0.5 0.6 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2312c97-d8ab-4e53-af25-279981393f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model \n",
    "\n",
    "# Define the file path where you want to save your model\n",
    "model_file_path = 'trained_model_4_var.h5'\n",
    "\n",
    "# Save the model\n",
    "my_model.save(model_file_path)\n",
    "\n",
    "print(\"Model saved successfully at:\", model_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e39f580-834b-4d0d-8c2c-333c24046e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "model_file_path = 'trained_model_4_var.h5'\n",
    "loaded_model2 = tf.keras.models.load_model(model_file_path)\n",
    "\n",
    "#load prediction data\n",
    "\n",
    "#Define dataframe\n",
    "pred_gal_df = pd.DataFrame(transpose_list(prediction_gamma_arbitrary_limits_data), columns =[\"code\", \"g_arbitrary_limits\", \"real_species\"]) \n",
    "pred_gftp_df = pd.DataFrame(transpose_list(prediction_gamma_first_two_peaks_data), columns =[\"code\", \"g_first_2_peaks\", \"real_species\"])\n",
    "pred_gauc_df = pd.DataFrame(transpose_list(prediction_gamma_area_under_curve_data), columns =[\"code\", \"g_area_und_curve\", \"real_species\"])\n",
    "pred_gaucfmc_df = pd.DataFrame(transpose_list(prediction_gamma_area_under_curve_first_min_cut_data), columns =[\"code\", \"g_area_und_curve_first_min\", \"real_species\"]) \n",
    "\n",
    "\n",
    "#drop species\n",
    "\n",
    "drop_pred_gal_df = pred_gal_df.copy()\n",
    "drop_pred_gal_df.drop(columns=[ \"real_species\"], inplace=True)\n",
    "\n",
    "drop_pred_gftp_df = pred_gftp_df.copy()\n",
    "drop_pred_gftp_df.drop(columns=[ \"real_species\"], inplace=True)\n",
    "\n",
    "drop_pred_gauc_df= pred_gauc_df.copy()\n",
    "drop_pred_gauc_df.drop(columns=[ \"real_species\"], inplace=True)\n",
    "\n",
    "drop_pred_gaucfmc_df=pred_gaucfmc_df.copy()\n",
    "drop_pred_gaucfmc_df.drop(columns=[ \"real_species\"], inplace=True)\n",
    "\n",
    "\n",
    "#merge on code\n",
    "pred_complete_df = pd.merge(drop_pred_gal_df, drop_pred_gftp_df, on=[\"code\"], how=\"inner\")\n",
    "pred_complete_df = pd.merge(pred_complete_df, drop_pred_gauc_df, on=[\"code\"], how=\"inner\")\n",
    "pred_complete_df = pd.merge(pred_complete_df, drop_pred_gaucfmc_df, on=[\"code\"], how=\"inner\")\n",
    "\n",
    "no_code_pred_complete_df= pred_complete_df.copy()\n",
    "no_code_pred_complete_df.drop(columns=[ \"code\"], inplace=True)\n",
    "pred_complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcff6cbd-be4d-4894-9ef5-03af5c2c90fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 2\n",
    "no_code_pred_complete_df = no_code_pred_complete_df.drop(columns=['g_arbitrary_limits',\"g_area_und_curve\"], inplace=False)\n",
    "\n",
    "print(no_code_pred_complete_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e43d65-8ede-46f7-bc98-4849897ee62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prediction_features = pd.DataFrame({name:np.array(value) for name, value in no_code_pred_complete_df.items()})\n",
    "\n",
    "print(prediction_features)\n",
    "#print(pred_complete_df)\n",
    "#convert prediction_features to tensor\n",
    "for element in prediction_features:\n",
    "    prediction_features[element] = tf.convert_to_tensor(np.array(prediction_features[element]), dtype=tf.int64) \n",
    "\n",
    "\n",
    "# Predict using the loaded model\n",
    "predictions = loaded_model2.predict(prediction_features)\n",
    "\n",
    "# Print the predictions\n",
    "#print(predictions)\n",
    "prediction_df = pd.DataFrame(predictions, columns=[\"kalinini\", \"resplendens\"])\n",
    "prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f171b28b-aef4-4a6a-a1fe-28499358f1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_prediction_data = pd.merge(pred_gal_df, pred_gftp_df , on=[\"code\",\"real_species\"], how=\"inner\")\n",
    "merged_prediction_data = pd.merge(merged_prediction_data, pred_gauc_df , on=[\"code\",\"real_species\"], how=\"inner\")\n",
    "merged_prediction_data = pd.merge(merged_prediction_data, pred_gaucfmc_df , on=[\"code\",\"real_species\"], how=\"inner\")\n",
    "merged_prediction_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f76c03-8a45-4bb3-b1f7-d7a4bd8ee84a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e3ee75-6dc2-4b03-98cd-1c35389a4afd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da0e246-ecd9-4fb7-99c6-6b72801c7e10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146bcf98-0ab6-471a-970b-30f0620140cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0750d20a-f226-4be6-a2d4-a0e0f2e20125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c63b38-ab00-497e-a06a-ad5ec691969f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
