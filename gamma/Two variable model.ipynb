{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f814dbfe-e40f-4555-969c-058526eb7f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esteb\\cicima\\escarabajos\\gamma\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Add the current directory to the Python path\n",
    "sys.path.append(current_directory)\n",
    "print(current_directory)\n",
    "\n",
    "from spectraltools import Specimen_Collection, Spectrum, create_path_if_not_exists\n",
    "from metrics import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import math\n",
    "import re\n",
    "import scipy\n",
    "import tensorflow as tf\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "922f2211-60c3-42b9-9e13-a88edf9da0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"This section allows the user to choose their workplace location.\n",
    "This is important if the user has multiple locations and operating systems in which this \n",
    "script is run\"\"\"\n",
    "\n",
    "#select location\n",
    "working_at = \"wfh\"\n",
    "\n",
    "#Training data is used when we are already certain of species and genera for a particular sample\n",
    "training_data_is_used = False\n",
    "\n",
    "if working_at == \"colaboratory\":\n",
    "  from google.colab import drive\n",
    "  drive.mount(\"/content/drive\")\n",
    "  #base folder\n",
    "  \"\"\"Select the location for your base folder\"\"\"\n",
    "    \n",
    "  base_folder = r\"/content/drive/My Drive/CICIMA/escarabajos_files\"\n",
    "  \n",
    "elif working_at == \"wfh\":\n",
    "\n",
    "    \"\"\"Select the location of your base folder\"\"\"\n",
    "    base_folder = r\"C:\\Users\\esteb\\cicima\\escarabajos\"\n",
    "\n",
    "elif working_at == \"cicima_desktop\":\n",
    "  \n",
    "    \"\"\"Select the location of your base folder\"\"\"\n",
    "    base_folder = r\"C:\\Users\\EstebanSoto\\Jupyter\\escarabajos\"\n",
    "\n",
    "elif working_at == \"cicima_laptop\":\n",
    "    \n",
    "    \"\"\"Select the location of your base folder\"\"\"\n",
    "    base_folder = r\"/home/vinicio/escarabajos\"\n",
    "\n",
    "#define the location of the tables with information about the collections and its parent directory\n",
    "\n",
    "collection_tables_main_path =  os.path.join(base_folder, \"L1050_data\",\"collections\")\n",
    "collection_files_main_path = os.path.join(base_folder, \"L1050_data\")\n",
    "\n",
    "# Define report location\n",
    "report_location = os.path.join(base_folder, \"reports\",\"data_analysis\")\n",
    "\n",
    "#collection_descriptor = r\"CICIMAUCR and ANGSOL\" tododelete\n",
    "\n",
    "#File location and metadata location for collection 1\n",
    "angsol_collection_path = os.path.join(collection_files_main_path,\"ANGSOL\",\"average\") \n",
    "angsol_collection_metadata = os.path.join(collection_tables_main_path,\"CICIMA-beetles-general-inventory - ANGSOL.txt\") \n",
    "\n",
    "#File location and metadata location for collection 2\n",
    "cicimaucr_collection_path = os.path.join(collection_files_main_path,r\"TRA_data_CICIMA_INBUCR\",\"CICIMAUCR\",\"reflectance\")  #listo\n",
    "cicimaucr_collection_2_path = os.path.join(collection_files_main_path,r\"CICIMA-2024-01-REFLECTANCE\",\"average\")\n",
    "cicimaucr_collection_3_path = os.path.join(collection_files_main_path,r\"CICIMA-2024-03-REFLECTANCE\",\"without iris nor lens\",\"average\")\n",
    "cicimaucr_collection_4_path = os.path.join(collection_files_main_path,r\"CICIMA-2024-05-REFLECTANCE\",\"average\")\n",
    "cicima_ucr_metadata = os.path.join(collection_tables_main_path,r\"CICIMA-beetles-general-inventory - CICIMAUCR.txt\") \n",
    "\n",
    "#File location and metadata location for collection 3\n",
    "inbucr_collection_path = os.path.join(collection_files_main_path,r\"INBUCR\",\"average\") #listo\n",
    "inbucr_metadata = os.path.join(collection_tables_main_path,r\"CICIMA-beetles-general-inventory - INBUCR.txt\") \n",
    "\n",
    "#File location and metadata location for collection 4\n",
    "bioucr_collection_path = os.path.join(collection_files_main_path,r\"BIOUCR\",\"average\")  #listo\n",
    "bioucr_metadata = os.path.join(collection_tables_main_path,r\"CICIMA-beetles-general-inventory - BIOUCR.txt\") \n",
    "\n",
    "#agregated data location, here averages and std will be saved when training data and retreived when classifying spectra\n",
    "agregated_data_location = os.path.join(base_folder, \"aggregated_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b69b7d8f-51dd-4b86-b530-0fdf2ff98f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "nan\n",
      "nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<spectraltools.Spectrum at 0x2238d7ddd30>,\n",
       " <spectraltools.Spectrum at 0x2238d7dd280>,\n",
       " <spectraltools.Spectrum at 0x2238d7dd700>,\n",
       " <spectraltools.Spectrum at 0x2238d87eb80>,\n",
       " <spectraltools.Spectrum at 0x2238d89dee0>,\n",
       " <spectraltools.Spectrum at 0x2238d89deb0>,\n",
       " <spectraltools.Spectrum at 0x2238d89d850>,\n",
       " <spectraltools.Spectrum at 0x2238d89d160>,\n",
       " <spectraltools.Spectrum at 0x2238d89ddf0>,\n",
       " <spectraltools.Spectrum at 0x2238d89d2e0>,\n",
       " <spectraltools.Spectrum at 0x2238d89d5e0>,\n",
       " <spectraltools.Spectrum at 0x2238d89dfa0>,\n",
       " <spectraltools.Spectrum at 0x2238d89dd90>,\n",
       " <spectraltools.Spectrum at 0x2238d8a8940>,\n",
       " <spectraltools.Spectrum at 0x2238e881160>,\n",
       " <spectraltools.Spectrum at 0x2238e881f10>,\n",
       " <spectraltools.Spectrum at 0x2238e881d30>,\n",
       " <spectraltools.Spectrum at 0x2238e881a30>,\n",
       " <spectraltools.Spectrum at 0x2238e881b20>,\n",
       " <spectraltools.Spectrum at 0x2238e881fa0>,\n",
       " <spectraltools.Spectrum at 0x2238e881100>,\n",
       " <spectraltools.Spectrum at 0x2238e889b80>,\n",
       " <spectraltools.Spectrum at 0x2238e889940>,\n",
       " <spectraltools.Spectrum at 0x2238e889ee0>,\n",
       " <spectraltools.Spectrum at 0x2238d8a87f0>,\n",
       " <spectraltools.Spectrum at 0x2238e889a00>,\n",
       " <spectraltools.Spectrum at 0x2238e894c10>,\n",
       " <spectraltools.Spectrum at 0x2238e8896d0>]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Collections\n",
    "angsol_collection = Specimen_Collection(\"ANGSOL\", angsol_collection_path, angsol_collection_metadata, \"HIGH\")\n",
    "angsol_collection.set_description(\"ANGSOL collection has specimens that belong to Angel Sol√≠s. The confidence that we have about specimen identification is high.\")\n",
    "\n",
    "cicimaucr_collection = Specimen_Collection(\"CICIMAUCR1\", cicimaucr_collection_path, cicima_ucr_metadata, \"HIGH\")\n",
    "cicimaucr_collection_2 = Specimen_Collection(\"CICIMAUCR2\", cicimaucr_collection_2_path, cicima_ucr_metadata, \"HIGH\")\n",
    "cicimaucr_collection_3 = Specimen_Collection(\"CICIMAUCR3\", cicimaucr_collection_3_path, cicima_ucr_metadata, \"HIGH\")\n",
    "cicimaucr_collection_3.set_description(\"\"\"The most part of CICIMA specimens belongs to this collecttion\"\"\")\n",
    "\n",
    "cicimaucr_collection_4 = Specimen_Collection(\"CICIMAUCR4\", cicimaucr_collection_4_path, cicima_ucr_metadata, \"HIGH\")\n",
    "cicimaucr_collection_4.set_description(\"\"\"This collection has 3 kalinini specimens which were not used in training. \n",
    "                                        These are intended to be used as test subjects\"\"\")\n",
    "\n",
    "inbucr_collection = Specimen_Collection(\"INBUCR\", inbucr_collection_path, inbucr_metadata, \"MID\")\n",
    "bioucr_collection = Specimen_Collection(\"BIOUCR\", bioucr_collection_path, bioucr_metadata, \"LOW\")\n",
    "\n",
    "collection_list = [\n",
    "                    #angsol_collection,\n",
    "                    cicimaucr_collection,\n",
    "                    cicimaucr_collection_2,\n",
    "                    cicimaucr_collection_3,\n",
    "                    #inbucr_collection,\n",
    "                    #bioucr_collection,\n",
    "                    ]\n",
    "collection_names_set = set([collection.name for collection in collection_list])\n",
    "collection_names = \" \".join( sorted(collection_names_set))\n",
    "\n",
    "prediction_list = [\n",
    "                    #angsol_collection,\n",
    "                    #cicimaucr_collection,\n",
    "                    #cicimaucr_collection_2,\n",
    "                    #cicimaucr_collection_3,\n",
    "                    cicimaucr_collection_4,\n",
    "                    #inbucr_collection,\n",
    "                    #bioucr_collection,\n",
    "                    ]\n",
    "prediction_collection_names_set = set([collection.name for collection in collection_list])\n",
    "prediction_collection_names = \" \".join( sorted(collection_names_set))\n",
    "\n",
    "\n",
    "#print(collection_names)\n",
    "#date\n",
    "from datetime import datetime\n",
    "current_date = datetime.now().date()\n",
    "\n",
    "def get_filtered_spectra(collection_list):\n",
    "\n",
    "    all_spectra = []\n",
    "    \n",
    "    for collection in collection_list:\n",
    "        all_spectra += collection.get_spectra()\n",
    "\n",
    "    all_spectra = [item for item in all_spectra if item.get_species() in [\"kalinini\", \"resplendens\", \"cupreomarginata\"]]\n",
    "    return all_spectra\n",
    "    \n",
    "def get_spectra(collection_list):\n",
    "\n",
    "    all_spectra = []\n",
    "    \n",
    "    for collection in collection_list:\n",
    "        all_spectra += collection.get_spectra()\n",
    "\n",
    "    return all_spectra  \n",
    "    \n",
    "training_spectra = get_filtered_spectra(collection_list)\n",
    "prediction_spectra  = get_spectra(prediction_list) \n",
    "\n",
    "for spectrum in prediction_spectra:\n",
    "    print(spectrum.get_species())\n",
    "\n",
    "training_spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ca2d18c-6d96-4c8c-92ed-d12ae453aa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_list(lst, filler=0):\n",
    "    metrics = lst[1]\n",
    "    \n",
    "    for metric in metrics:\n",
    "        #print(metric)\n",
    "        \n",
    "        max_length = max([len(list) for list in metrics])\n",
    "        \n",
    "        #print(max_length)\n",
    "        \n",
    "        padded_list = []\n",
    "\n",
    "        for element in metrics:\n",
    "\n",
    "            padded_sublist = element\n",
    "            \n",
    "            padded_sublist += [filler] * (max_length - len(padded_sublist))\n",
    "            padded_list.append(padded_sublist)\n",
    "            #print(padded_sublist)\n",
    "    final_list = [lst[0], (padded_list), lst[2]]\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fccf47a7-9c20-471a-bab5-1f5556c28589",
   "metadata": {},
   "outputs": [],
   "source": [
    "import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f3a45db8-0a26-430d-9fdc-c638f18bf2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Training data \n",
    "scalar_metrics = [Gamma_Arbitrary_Limits,Gamma_First_Two_Peaks,Gamma_Area_Under_Curve_Naive, Gamma_Area_Under_Curve_First_Min_Cut]\n",
    "#scalar\n",
    "gamma_arbitrary_limits_data = feature_and_label_extractor(Gamma_Arbitrary_Limits, training_spectra)\n",
    "gamma_first_two_peaks_data = feature_and_label_extractor(Gamma_First_Two_Peaks, training_spectra)\n",
    "gamma_area_under_curve_data = feature_and_label_extractor(Gamma_Area_Under_Curve_Naive, training_spectra)\n",
    "gamma_area_under_curve_first_min_cut_data = feature_and_label_extractor(Gamma_Area_Under_Curve_First_Min_Cut, training_spectra)\n",
    "\n",
    "\n",
    "#vectorial\n",
    "vectorial_metrics = [Wavelength_Vector, Maximum_Points, Minimum_Points, Maximum_Points_Normalized, Minimum_Points_Normalized, Critical_Points ]\n",
    "\n",
    "gamma_vector_relative_reflectance_data = feature_and_label_extractor(Gamma_Vector_Relative_Reflectance, training_spectra)\n",
    "wavelength_vector_data = feature_and_label_extractor(Wavelength_Vector, training_spectra)\n",
    "critical_points_data = feature_and_label_extractor(Critical_Points, training_spectra)\n",
    "\n",
    "maximum_points_data = feature_and_label_extractor(Maximum_Points, training_spectra)\n",
    "minimum_points_data = feature_and_label_extractor(Minimum_Points, training_spectra)\n",
    "maximum_points_normalized_data =feature_and_label_extractor(Maximum_Points_Normalized, training_spectra)\n",
    "minimum_points_normalized_data =feature_and_label_extractor(Minimum_Points_Normalized, training_spectra)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "1c8158fc-31bc-497f-a63a-aec9da99fb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_aggregate(data):\n",
    "    \n",
    "    length = len(data[0])\n",
    "\n",
    "    data_points = []\n",
    "\n",
    "    #for each specimen\n",
    "    \n",
    "    for i in range(0, length): \n",
    "        code = data[0][i]\n",
    "        vector = data[1][i]\n",
    "        species =data[2][i]\n",
    "\n",
    "        data_point = {}\n",
    "        data_point[\"code\"] = code\n",
    "        data_point[\"vector\"] = vector\n",
    "        data_point[\"species\"] = species\n",
    "\n",
    "        data_points.append(data_point)\n",
    "\n",
    "    #Now, for each species \n",
    "    aggregates = {}\n",
    "    \n",
    "    for species in [\"kalinini\", \"resplendens\", \"cupreomarginata\"]:\n",
    "        \n",
    "        specimens = [x for x in data_points if x[\"species\"] == species]\n",
    "\n",
    "        #extract vectors\n",
    "        vectors = [element[\"vector\"] for element in specimens]\n",
    "\n",
    "        #first entry\n",
    "        vector_first_entries =  [element[0] for element in vectors]\n",
    "        vector_second_entries =  [element[1] for element in vectors]\n",
    "        \n",
    "        #get max length of \n",
    "        print(type(vector_first_entries[0]))\n",
    "        if not isinstance((vector_first_entries[0]), np.float64):\n",
    "            max_length_x = max([len(x) for x in vector_first_entries])\n",
    "            max_length_y = max([len(x) for x in vector_second_entries])\n",
    "        else:\n",
    "            max_length_x = 1\n",
    "            max_length_y = 1\n",
    "        \n",
    "        #get number of vectors\n",
    "        number_of_specimens = len(specimens)\n",
    "        \n",
    "        #add zeroes\n",
    "        #for first entry\n",
    "        \n",
    "        new_subset_vectors = []\n",
    "        \n",
    "        new_vector_first_entry = []\n",
    "        for first_entry_i in vector_first_entries:\n",
    "            #for the first and second entry\n",
    "            if isinstance(first_entry_i, np.float64):\n",
    "                length_first_entry = 1\n",
    "                number_of_zeroes = max_length_x - length_first_entry\n",
    "                extend_vector = np.array([0]*number_of_zeroes)\n",
    "                #print(extend_vector)\n",
    "                #print(first_entry_i)\n",
    "                first_entry_i = np.concatenate(([first_entry_i], extend_vector))\n",
    "                new_vector_first_entry.append(first_entry_i)\n",
    "            else:\n",
    "                length_first_entry = len(first_entry_i)\n",
    "                number_of_zeroes = max_length_x - length_first_entry\n",
    "                extend_vector = np.array([0]*number_of_zeroes)\n",
    "                #print(extend_vector)\n",
    "                #print(first_entry_i)\n",
    "                first_entry_i = np.concatenate((first_entry_i, extend_vector))\n",
    "                new_vector_first_entry.append(first_entry_i)\n",
    "\n",
    "        new_vector_second_entry =  []\n",
    "        \n",
    "        for second_entry_i in vector_second_entries:\n",
    "            #for the first and second entry\n",
    "            if isinstance(second_entry_i, np.float64):\n",
    "                length_second_entry = 1\n",
    "                number_of_zeroes = max_length_x - length_second_entry\n",
    "                extend_vector = np.array([0]*number_of_zeroes)\n",
    "                second_entry_i = np.concatenate(([second_entry_i], extend_vector))\n",
    "                new_vector_second_entry.append(second_entry_i)\n",
    "            else:\n",
    "                length_second_entry = len(second_entry_i)\n",
    "                number_of_zeroes = max_length_x - length_second_entry\n",
    "                extend_vector = np.array([0]*number_of_zeroes)\n",
    "                second_entry_i = np.concatenate((second_entry_i, extend_vector))\n",
    "                new_vector_second_entry.append(second_entry_i)\n",
    "        #print(f\"{new_vector_first_entry=}\")\n",
    "        #print(f\"{new_vector_second_entry=}\")\n",
    "\n",
    "        #now calculate averages \n",
    "\n",
    "        x_averages = []\n",
    "        x_std = []\n",
    "        for i in range(max_length_x): \n",
    "            vector_i = []\n",
    "\n",
    "            for n in range(0, number_of_specimens):\n",
    "                value_n = new_vector_first_entry[n][i]\n",
    "                if not( (value_n < 0.1) & (value_n > -0.1) ) : #if value is not zero\n",
    "                    vector_i.append(value_n)\n",
    "            #then get the total of elements, convert it into a numpy array , calculate the average. \n",
    "            x_averages.append(np.mean(np.array(vector_i)))\n",
    "            x_std.append(np.std(np.array(vector_i)))\n",
    "            \n",
    "        y_averages =[]\n",
    "        y_std = []\n",
    "        \n",
    "        for i in range(max_length_y): \n",
    "            vector_x = []\n",
    "            for n in range(0, number_of_specimens):\n",
    "                value_n = new_vector_second_entry[n][i]\n",
    "                if not( (value_n < 0.1) & (value_n > -0.1) ) : #if value is not zero\n",
    "                    vector_x.append(value_n)\n",
    "            #then get the total of elements, convert it into a numpy array , calculate the average. \n",
    "            y_averages.append(np.mean(np.array(vector_x)))\n",
    "            y_std.append(np.std(np.array(vector_x)))\n",
    "        \n",
    "        info = np.array( [x_averages, y_averages, x_std, y_std]).T\n",
    "        df = pd.DataFrame(info, columns= [f\"{species}_x_avg\", f\"{species}_y_avg\", f\"{species}_x_std\", f\"{species}_y_std\"])\n",
    "        aggregates[species] = df\n",
    "        #print(aggregates)\n",
    "    df_2 = pd.DataFrame([])\n",
    "    for element in aggregates:\n",
    "        df = (aggregates[element])\n",
    "        df_2 = pd.concat([df, df_2], axis=1)\n",
    "        #print(df_2)\n",
    "    return df_2\n",
    "    \n",
    "\n",
    "def save_vector_aggregate(metric_class, spectra, agregated_data_location):\n",
    "    #get metric values for spectra\n",
    "    data = feature_and_label_extractor(metric_class, spectra)\n",
    "    #create vector aggregate df\n",
    "    df = vector_aggregate(data)  \n",
    "    #create path location\n",
    "    path_location = os.path.join(agregated_data_location, \"metric_avg_std\")\n",
    "    create_path_if_not_exists(path_location)\n",
    "    path_and_filename = os.path.join( path_location, f'{metric_class.get_name()}.txt')\n",
    "    #save to csv\n",
    "    df.to_csv( path_and_filename, index=True, sep = \"\\t\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4a4aec18-1158-4502-a4b2-bf5b538a564c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "<class 'numpy.float64'>\n",
      "Directory 'C:\\Users\\esteb\\cicima\\escarabajos\\aggregated_data\\metric_avg_std' already exists.\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Directory 'C:\\Users\\esteb\\cicima\\escarabajos\\aggregated_data\\metric_avg_std' already exists.\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Directory 'C:\\Users\\esteb\\cicima\\escarabajos\\aggregated_data\\metric_avg_std' already exists.\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Directory 'C:\\Users\\esteb\\cicima\\escarabajos\\aggregated_data\\metric_avg_std' already exists.\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Directory 'C:\\Users\\esteb\\cicima\\escarabajos\\aggregated_data\\metric_avg_std' already exists.\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "Directory 'C:\\Users\\esteb\\cicima\\escarabajos\\aggregated_data\\metric_avg_std' already exists.\n"
     ]
    }
   ],
   "source": [
    "for metric in vectorial_metrics: \n",
    "    save_vector_aggregate(metric, training_spectra, agregated_data_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e80b45-ff5c-4ee4-9291-bd9b9e7fd527",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6616f54b-9b33-4ced-9fbc-c1c9eaf0affc",
   "metadata": {},
   "outputs": [],
   "source": [
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f86a5f-e1d4-40fc-9415-fd7b38264f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0743c0b-5a80-4a88-87f8-af219a07aa2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_points_data\n",
    "try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba06b25-64e0-49cf-83fe-65fec67eee74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save averages and std for training data\n",
    "metrics_list = [Gamma_Arbitrary_Limits, Gamma_First_Two_Peaks, Gamma_Area_Under_Curve_Naive, Gamma_Area_Under_Curve_First_Min_Cut, \n",
    "                Gamma_Area_Under_Curve_First_Min_Cut]\n",
    "\n",
    "for metric in metrics_list:\n",
    "    save_aggregated_data(metric,training_spectra,agregated_data_location)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef85ed7-7afc-459d-ba90-9ff83f0e318c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read averages and std\n",
    "avg_std_location = os.path.join(agregated_data_location, \"metric_avg_std\")\n",
    "dfs = read_aggregated_data(avg_std_location)\n",
    "dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8cbf28-4422-4095-af2a-5402c3d8608d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3276d2d2-3bdf-48f6-8b8f-95088968fd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "#similarity index\n",
    "\n",
    "def similarity_index(spectrum, averages_df, standard_deviations_df , species):\n",
    "    def distance(x, x0, sigma_0):\n",
    "        index =  (((x - x0)**4)**(1/4))\n",
    "        #print(f\" x {x} - x0 {x0} = {(x - x0)} and index = {index}\")\n",
    "        return index\n",
    "        #def get_gamma_factor(self, spectrum):\n",
    "    #get first n peaks of the gamma\n",
    "    n = 5\n",
    "    peaklist = PeakList(spectrum)\n",
    "    peaks = peaklist.get_peaks()[0:n]\n",
    "\n",
    "    #loads species average values:\n",
    "\n",
    "    averages_df = averages_df[averages_df[\"species\"] == species]\n",
    "    average_x_df = averages_df[[\"x0\",\"x1\",\"x2\",\"x4\",\"x5\"]].values[0]\n",
    "\n",
    "    standard_deviations_df = standard_deviations_df[standard_deviations_df[\"species\"] == species]\n",
    "    standard_deviation_x_df = standard_deviations_df[[\"x0\",\"x1\",\"x2\",\"x4\",\"x5\"]].values[0]\n",
    "\n",
    "    #print(f\"averages_df: {averages_df}\")\n",
    "    #for each peak, calculate the distances to the averages x values\n",
    "    similarity_index = 0.0\n",
    "\n",
    "    for peak_i,xi_0, sigmai_0 in zip(peaks, average_x_df, standard_deviation_x_df):\n",
    "        xi = peak_i.x_value\n",
    "        similarity_index += distance(xi, xi_0, sigmai_0)\n",
    "\n",
    "    #normalize for the number of points\n",
    "    similarity_index = similarity_index/n\n",
    "    #print(f\"gamma: {gamma}\")\n",
    "    return similarity_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42be5642-9c7f-4ab6-a80e-bf266f97035f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction data\n",
    "\n",
    "prediction_gamma_arbitrary_limits_data = feature_and_label_extractor(Gamma_Arbitrary_Limits, prediction_spectra)\n",
    "prediction_gamma_first_two_peaks_data = feature_and_label_extractor(Gamma_First_Two_Peaks, prediction_spectra)\n",
    "prediction_gamma_area_under_curve_data = feature_and_label_extractor(Gamma_Area_Under_Curve_Naive, prediction_spectra)\n",
    "prediction_gamma_area_under_curve_first_min_cut_data = feature_and_label_extractor(Gamma_Area_Under_Curve_First_Min_Cut, prediction_spectra)\n",
    "prediction_gamma_vector_relative_reflectance_data = feature_and_label_extractor(Gamma_Vector_Relative_Reflectance, prediction_spectra)\n",
    "prediction_critical_points_data = feature_and_label_extractor(Critical_Points, prediction_spectra)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916c47b6-b30e-46d0-a704-8763006bae27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a690e936-3ba5-4db4-90d2-201fe5796470",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "#print(critical_points_data)\n",
    "pad_list(critical_points_data, filler = np.array([0,0]))\n",
    "pad_list(gamma_vector_relative_reflectance_data)\n",
    "data = pad_list(wavelength_vector_data)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b5cce2-8547-4314-b054-22f7bbc29acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.version_info)\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# The following lines adjust the granularity of reporting. \n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = \"{:.1f}\".format\n",
    "\n",
    "# The following line improves formatting when ouputting NumPy arrays.\n",
    "np.set_printoptions(linewidth = 200)\n",
    "\n",
    "def replace_strings_3(lst):\n",
    "    mapping = {\"kalinini\": 0, \"resplendens\": 1, \"cupreomarginata\": 2}\n",
    "    return [mapping.get(item, item) for item in lst]\n",
    "\n",
    "def replace_species_with_categorical(df):\n",
    "    \n",
    "    df.loc[df[\"species\"]==\"kalinini\",\"species\"] = 0\n",
    "    df.loc[df[\"species\"]==\"resplendens\", \"species\"] = 1\n",
    "    df.loc[df[\"species\"]==\"cupreomarginata\", \"species\"] = 2\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def plot_curve(epochs, hist, list_of_metrics):\n",
    "  \"\"\"Plot a curve of one or more classification metrics vs. epoch.\"\"\"  \n",
    "  # list_of_metrics should be one of the names shown in:\n",
    "  # https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#define_the_model_and_metrics  \n",
    "\n",
    "  plt.figure()\n",
    "  plt.xlabel(\"Epoch\")\n",
    "  plt.ylabel(\"Value\")\n",
    "\n",
    "  for m in list_of_metrics:\n",
    "    x = hist[m]\n",
    "    plt.plot(epochs[1:], x[1:], label=m)\n",
    "\n",
    "  plt.legend()\n",
    "\n",
    "print(\"Loaded the plot_curve function.\")\n",
    "\n",
    "def transpose_list(lst):\n",
    "    return list(zip(*lst))\n",
    "    \n",
    "def get_nth_feature(data, n):\n",
    "    feature_vector = [data[0], [x[n] for x in data[1]] , data[2]] \n",
    "    return feature_vector\n",
    "def scatter_plot_2_variables(df_1, df_2):\n",
    "\n",
    "    joint_df = pd.merge(df_1, df_2, on=[\"code\", \"species\"], how=\"inner\")\n",
    "    column_list = joint_df.columns.tolist()\n",
    "    print(column_list)\n",
    "    column_list = [x for x in column_list if x not in [\"code\", \"species\"] ]\n",
    "    print(column_list)\n",
    "    plt.figure()\n",
    "    sns.scatterplot(joint_df, x=column_list[0], y =column_list[1], hue=\"species\")\n",
    "    plt.show()\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def scatter_plot_3_variables(df_1, df_2, df_3):\n",
    "    \n",
    "    joint_df = pd.merge(df_1, df_2, on=[\"code\", \"species\"], how=\"inner\")\n",
    "    joint_df = pd.merge(joint_df, df_3, on=[\"code\", \"species\"], how=\"inner\")\n",
    "    \n",
    "    column_list = joint_df.columns.tolist()\n",
    "    print(column_list)\n",
    "    column_list = [x for x in column_list if x not in [\"code\", \"species\"] ]\n",
    "    print(column_list)\n",
    "    \n",
    "    x = joint_df[column_list[0]]\n",
    "    y = joint_df[column_list[1]]\n",
    "    z = joint_df[column_list[2]]\n",
    "    species = joint_df[\"species\"]\n",
    "    \n",
    "    # Create color map\n",
    "    colors = {'kalinini': 'r', 'resplendens': 'g', 'cupreomarginata': 'b'}\n",
    "    \n",
    "    # Create figure and 3D axis\n",
    "    fig = plt.figure(figsize=(15, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Plot points with color based on the fourth dimension\n",
    "    for category in set(species):\n",
    "        indices = species == category\n",
    "        ax.scatter(x[indices], y[indices], z[indices], c=colors[category], label=category, marker='o')\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax.set_xlabel(f'{column_list[0]}')\n",
    "    ax.set_ylabel(f'{column_list[1]}')\n",
    "    ax.set_zlabel(f'{column_list[2]}')\n",
    "    #ax.set_title('3D Scatter plot with species based on Fourth Dimension')\n",
    "    \n",
    "    # Add legend\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f559b3c-dca4-4724-9ce1-98d23f7df0f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training data. Define dataframees\n",
    "\n",
    "gal_df = pd.DataFrame(transpose_list(gamma_arbitrary_limits_data), columns =[\"code\", \"g_arbitrary_limits\", \"species\"]) \n",
    "gftp_df = pd.DataFrame(transpose_list(gamma_first_two_peaks_data), columns =[\"code\", \"g_first_2_peaks\", \"species\"])\n",
    "gaucfmc_df = pd.DataFrame(transpose_list(gamma_area_under_curve_first_min_cut_data), columns =[\"code\", \"g_area_und_curve_first_min\", \"species\"]) \n",
    "gauc_df = pd.DataFrame(transpose_list(gamma_area_under_curve_data), columns =[\"code\", \"g_area_und_curve\", \"species\"])\n",
    "\n",
    "complete_df = pd.merge(gal_df, gftp_df, on=[\"species\",\"code\"], how=\"inner\")\n",
    "complete_df = pd.merge(complete_df, gauc_df, on=[\"species\",\"code\"], how=\"inner\")\n",
    "complete_df = pd.merge(complete_df, gaucfmc_df, on=[\"species\",\"code\"], how=\"inner\")\n",
    "complete_df.drop(columns=['code'], inplace=True)\n",
    "\n",
    "complete_df = replace_species_with_categorical(complete_df)\n",
    "#complete_df.drop(columns=['species'], inplace=True)\n",
    "complete_df\n",
    "\n",
    "shuffled_df = complete_df.sample(frac=1, random_state=42)  # Random_state for reproducibility\n",
    "\n",
    "# Define the fraction of data to be used for training\n",
    "train_fraction = 0.66  # For example, 80% for training, 20% for testing\n",
    "\n",
    "# Calculate the number of rows for the training set\n",
    "train_size = int(train_fraction * len(complete_df))\n",
    "\n",
    "# Split the shuffled DataFrame into train and test sets\n",
    "train_data = shuffled_df.iloc[:train_size]\n",
    "test_data = shuffled_df.iloc[train_size:]\n",
    "\n",
    "# Split the shuffled DataFrame into features and labels\n",
    "train_data_features = train_data.drop(columns=['species'], inplace=False)\n",
    "train_data_labels =  train_data.drop(columns=['g_arbitrary_limits','g_first_2_peaks','g_area_und_curve_first_min','g_area_und_curve'], inplace=False)\n",
    "test_data_features =  test_data.drop(columns=['species'], inplace=False)\n",
    "test_data_labels =  test_data.drop(columns=['g_arbitrary_limits','g_first_2_peaks','g_area_und_curve_first_min','g_area_und_curve'], inplace=False)\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Assuming y_train and y_test are your integer labels\n",
    "train_data_labels_one_hot = to_categorical(train_data_labels, num_classes=2)\n",
    "test_data_labels_one_hot = to_categorical(test_data_labels, num_classes=2)\n",
    "\n",
    "print(train_data[train_data_labels[\"species\"]==2])\n",
    "print(train_data_features[train_data_labels[\"species\"]==0])\n",
    "#print(len(train_data))\n",
    "#print(len(test_data))\n",
    "#print(test_features)\n",
    "#print(test_labels)\n",
    "\n",
    "# Select 2\n",
    "train_data_features = train_data_features.drop(columns=['g_arbitrary_limits',\"g_area_und_curve\"], inplace=False)\n",
    "test_data_features =  test_data_features.drop(columns=['g_arbitrary_limits',\"g_area_und_curve\"], inplace=False)\n",
    "\n",
    "print(train_data_labels[train_data_labels[\"species\"]==0])\n",
    "print(train_data_labels_one_hot[train_data_labels[\"species\"]==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af49d08-188d-4066-bb7d-c63d02c42f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_3(my_learning_rate):\n",
    "  \"\"\"Create and compile a deep neural net.\"\"\"\n",
    "  \n",
    "  # All models in this course are sequential.\n",
    "  model = tf.keras.models.Sequential()\n",
    "\n",
    "  # The features are stored in a two-dimensional 28X28 array. \n",
    "  # Flatten that two-dimensional array into a one-dimensional \n",
    "  # 784-element array.\n",
    "  model.add(tf.keras.layers.Flatten(input_shape=(2,)))\n",
    "\n",
    "  # Define the first hidden layer.   \n",
    "  model.add(tf.keras.layers.Dense(units=8, activation='relu'))\n",
    "\n",
    "  # Define the first hidden layer.   \n",
    "  model.add(tf.keras.layers.Dense(units=4, activation='relu'))\n",
    "\n",
    "  # Define the first hidden layer.   \n",
    "  model.add(tf.keras.layers.Dense(units=4, activation='relu'))\n",
    "  \n",
    "  # Define a dropout regularization layer. \n",
    "  model.add(tf.keras.layers.Dropout(rate=0.1))\n",
    "\n",
    "  # Define the output layer. The units parameter is set to 10 because\n",
    "  # the model must choose among 10 possible output values (representing\n",
    "  # the digits from 0 to 9, inclusive).\n",
    "  #\n",
    "  # Don't change this layer.\n",
    "  model.add(tf.keras.layers.Dense(units=2, activation='softmax'))     \n",
    "                           \n",
    "  # Construct the layers into a model that TensorFlow can execute.  \n",
    "  # Notice that the loss function for multi-class classification\n",
    "  # is different than the loss function for binary classification.  \n",
    "  #model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=my_learning_rate),loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
    "  model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "  return model    \n",
    "\n",
    "\n",
    "def train_model(model, train_features, train_label, epochs,\n",
    "                batch_size=None, validation_split=0.1):\n",
    "  \"\"\"Train the model by feeding it data.\"\"\"\n",
    "\n",
    "  history = model.fit(x=train_features, y=train_label, batch_size=batch_size,\n",
    "                      epochs=epochs, shuffle=True)\n",
    "                      #validation_split=validation_split)\n",
    " \n",
    "  # To track the progression of training, gather a snapshot\n",
    "  # of the model's metrics at each epoch. \n",
    "  epochs = history.epoch\n",
    "  hist = pd.DataFrame(history.history)\n",
    "\n",
    "  return epochs, hist    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43828edf-9500-4fde-a32c-50cc773f3d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following variables are the hyperparameters.\n",
    "learning_rate = 0.007\n",
    "epochs = 160\n",
    "batch_size = 3\n",
    "validation_split = 0.05\n",
    "\n",
    "# Establish the model's topography.\n",
    "my_model = create_model_3(learning_rate)\n",
    "\n",
    "# Train the model on the normalized training set.\n",
    "#epochs, hist = train_model(my_model, x_train, y_train, \n",
    "                           #epochs, batch_size, validation_split)\n",
    "epochs, hist = train_model(my_model, train_data_features, train_data_labels_one_hot, \n",
    "                           epochs, batch_size, validation_split)\n",
    "\n",
    "# Plot a graph of the metric vs. epochs.\n",
    "list_of_metrics_to_plot = ['accuracy']\n",
    "plot_curve(epochs, hist, list_of_metrics_to_plot)\n",
    "\n",
    "# Evaluate against the test set.\n",
    "print(\"\\n Evaluate the new model against the test set:\")\n",
    "my_model.evaluate(x=test_data_features, y=test_data_labels_one_hot, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f3c3ca-d4c4-4a0e-972b-e82897d912b5",
   "metadata": {},
   "source": [
    "### Notas: \n",
    "Mejores resultados usando el valor de batch size  4 o 3, acc 0.7\n",
    " Para 5, acc baja a 0.6\n",
    " Learning rate, buenos resultados en: 0.008-0.009. Acc 0.7\n",
    "aprendizaje inestable: Cambiamos learning rate a 0.007 y epochs a 600\n",
    " Pasar regularizaci√≥n de 0 a 0.1 hace que el aprendizaje sea m√°s suave\n",
    " Pasar de 4,4 a 8,4 hace el aprendizaje mas suave\n",
    " Pasar de 4,4 a 8,6 hace el aprendizaje menos efectivo\n",
    " Pasar de 4,4 a 8,4,4 hace el aprendizaje mas suave\n",
    "aumentar el validation split de 0.1 a 0.3 reduce el acc a 0.3 0.6 0.6\n",
    "bajar el validation split 0.1 a 0.05 reduce el acc a 0.5 0.6 0.7\n",
    "dejar el val. split en 0.1 deja acc en 0.5 0.6 0.65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2312c97-d8ab-4e53-af25-279981393f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model \n",
    "\n",
    "# Define the file path where you want to save your model\n",
    "model_file_path = 'trained_model_4_var.h5'\n",
    "\n",
    "# Save the model\n",
    "my_model.save(model_file_path)\n",
    "\n",
    "print(\"Model saved successfully at:\", model_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e39f580-834b-4d0d-8c2c-333c24046e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved model\n",
    "model_file_path = 'trained_model_4_var.h5'\n",
    "loaded_model2 = tf.keras.models.load_model(model_file_path)\n",
    "\n",
    "#load prediction data\n",
    "\n",
    "#Define dataframe\n",
    "pred_gal_df = pd.DataFrame(transpose_list(prediction_gamma_arbitrary_limits_data), columns =[\"code\", \"g_arbitrary_limits\", \"real_species\"]) \n",
    "pred_gftp_df = pd.DataFrame(transpose_list(prediction_gamma_first_two_peaks_data), columns =[\"code\", \"g_first_2_peaks\", \"real_species\"])\n",
    "pred_gauc_df = pd.DataFrame(transpose_list(prediction_gamma_area_under_curve_data), columns =[\"code\", \"g_area_und_curve\", \"real_species\"])\n",
    "pred_gaucfmc_df = pd.DataFrame(transpose_list(prediction_gamma_area_under_curve_first_min_cut_data), columns =[\"code\", \"g_area_und_curve_first_min\", \"real_species\"]) \n",
    "\n",
    "\n",
    "#drop species\n",
    "\n",
    "drop_pred_gal_df = pred_gal_df.copy()\n",
    "drop_pred_gal_df.drop(columns=[ \"real_species\"], inplace=True)\n",
    "\n",
    "drop_pred_gftp_df = pred_gftp_df.copy()\n",
    "drop_pred_gftp_df.drop(columns=[ \"real_species\"], inplace=True)\n",
    "\n",
    "drop_pred_gauc_df= pred_gauc_df.copy()\n",
    "drop_pred_gauc_df.drop(columns=[ \"real_species\"], inplace=True)\n",
    "\n",
    "drop_pred_gaucfmc_df=pred_gaucfmc_df.copy()\n",
    "drop_pred_gaucfmc_df.drop(columns=[ \"real_species\"], inplace=True)\n",
    "\n",
    "\n",
    "#merge on code\n",
    "pred_complete_df = pd.merge(drop_pred_gal_df, drop_pred_gftp_df, on=[\"code\"], how=\"inner\")\n",
    "pred_complete_df = pd.merge(pred_complete_df, drop_pred_gauc_df, on=[\"code\"], how=\"inner\")\n",
    "pred_complete_df = pd.merge(pred_complete_df, drop_pred_gaucfmc_df, on=[\"code\"], how=\"inner\")\n",
    "\n",
    "no_code_pred_complete_df= pred_complete_df.copy()\n",
    "no_code_pred_complete_df.drop(columns=[ \"code\"], inplace=True)\n",
    "pred_complete_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcff6cbd-be4d-4894-9ef5-03af5c2c90fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 2\n",
    "no_code_pred_complete_df = no_code_pred_complete_df.drop(columns=['g_arbitrary_limits',\"g_area_und_curve\"], inplace=False)\n",
    "\n",
    "print(no_code_pred_complete_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e43d65-8ede-46f7-bc98-4849897ee62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prediction_features = pd.DataFrame({name:np.array(value) for name, value in no_code_pred_complete_df.items()})\n",
    "\n",
    "print(prediction_features)\n",
    "#print(pred_complete_df)\n",
    "#convert prediction_features to tensor\n",
    "for element in prediction_features:\n",
    "    prediction_features[element] = tf.convert_to_tensor(np.array(prediction_features[element]), dtype=tf.int64) \n",
    "\n",
    "\n",
    "# Predict using the loaded model\n",
    "predictions = loaded_model2.predict(prediction_features)\n",
    "\n",
    "# Print the predictions\n",
    "#print(predictions)\n",
    "prediction_df = pd.DataFrame(predictions, columns=[\"kalinini\", \"resplendens\"])\n",
    "prediction_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f171b28b-aef4-4a6a-a1fe-28499358f1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_prediction_data = pd.merge(pred_gal_df, pred_gftp_df , on=[\"code\",\"real_species\"], how=\"inner\")\n",
    "merged_prediction_data = pd.merge(merged_prediction_data, pred_gauc_df , on=[\"code\",\"real_species\"], how=\"inner\")\n",
    "merged_prediction_data = pd.merge(merged_prediction_data, pred_gaucfmc_df , on=[\"code\",\"real_species\"], how=\"inner\")\n",
    "merged_prediction_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f76c03-8a45-4bb3-b1f7-d7a4bd8ee84a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e3ee75-6dc2-4b03-98cd-1c35389a4afd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da0e246-ecd9-4fb7-99c6-6b72801c7e10",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146bcf98-0ab6-471a-970b-30f0620140cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0750d20a-f226-4be6-a2d4-a0e0f2e20125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c63b38-ab00-497e-a06a-ad5ec691969f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
