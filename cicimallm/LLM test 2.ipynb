{
 "cells": [
  {
   "cell_type": "raw",
   "id": "f2f59556-a594-40c8-9c81-03d2b492d106",
   "metadata": {},
   "source": [
    "from transformers import GPTJForCausalLM, GPT2Tokenizer\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Measure time for loading the model\n",
    "start_time = time.time()\n",
    "model = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "model_load_time = time.time() - start_time\n",
    "print(f\"Model loaded in {model_load_time:.2f} seconds.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c354a901-4705-413a-b821-2997b791f5a0",
   "metadata": {},
   "source": [
    "from transformers import GPTJForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# Start time for model loading\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Load the model with reduced memory usage\n",
    "    model = GPTJForCausalLM.from_pretrained(\n",
    "        \"EleutherAI/gpt-j-6B\",\n",
    "        low_cpu_mem_usage=True,  # Reduce memory usage during loading\n",
    "        torch_dtype=torch.float16  # Use float16 for reduced memory footprint\n",
    "    )\n",
    "    \n",
    "    # Measure and print model load time\n",
    "    model_load_time = time.time() - start_time\n",
    "    print(f\"Model loaded in {model_load_time:.2f} seconds.\")\n",
    "    \n",
    "    # Move the model to GPU if available, otherwise keep it on CPU\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Start time for tokenizer loading\n",
    "    tokenizer_start_time = time.time()\n",
    "    \n",
    "    # Load the tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "    \n",
    "    # Measure and print tokenizer load time\n",
    "    tokenizer_load_time = time.time() - tokenizer_start_time\n",
    "    print(f\"Tokenizer loaded in {tokenizer_load_time:.2f} seconds.\")\n",
    "\n",
    "    print(\"Model and tokenizer loaded successfully!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2b958bae-3486-4bca-b95c-f5a476d0bef0",
   "metadata": {},
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "import time\n",
    "# Measure time for loading the tokenizer\n",
    "start_time = time.time()\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "tokenizer_load_time = time.time() - start_time\n",
    "print(f\"Tokenizer loaded in {tokenizer_load_time:.2f} seconds.\")\n",
    "\n",
    "# Set pad_token_id if not already set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b1452656-ac91-4994-8d04-921d30879f2e",
   "metadata": {},
   "source": [
    "def get_response(input_text):\n",
    "    # Encode input with attention mask\n",
    "    input_text = \"Easy cowboy, everything is okay\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    attention_mask = (input_ids != tokenizer.pad_token_id).long()  # Generate attention mask\n",
    "    \n",
    "    # Generate output with attention mask\n",
    "    output = model.generate(\n",
    "        input_ids, \n",
    "        attention_mask=attention_mask, \n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode and print the output\n",
    "    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(\"Generated text:\", decoded_output)\n",
    "    \n",
    "def conversation():\n",
    "    while True:\n",
    "        input_text = input()\n",
    "        get_response(input_text)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d43c3162-93f6-47f8-b022-c7c05bc1c648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "CUDA available: True\n",
      "Device name: NVIDIA GeForce RTX 3050\n",
      "(8, 6)\n"
     ]
    }
   ],
   "source": [
    "from transformers.integrations import is_tensorboard_available\n",
    "print(is_tensorboard_available())\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU found\")\n",
    "import torch\n",
    "print(torch.cuda.get_device_capability())"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7f809846-ce88-45c9-927d-00beae43b94f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# pip install tensorboard tqdm\n",
    "from transformers import TrainingArguments\n",
    "import os\n",
    "from transformers import GPTJForCausalLM, GPT2Tokenizer, Trainer, TrainingArguments, ProgressCallback\n",
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm  # For custom progress bars\n",
    "import torch\n",
    "\n",
    "# Step 1: Load all .txt files from the folder with a progress bar\n",
    "def load_txt_files(folder_path):\n",
    "    data = []\n",
    "    for file_name in tqdm(os.listdir(folder_path), desc=\"Loading Text Files\"):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                data.append({\"text\": file.read()})\n",
    "    return data\n",
    "\n",
    "# Step 2: Tokenize the data with a progress bar\n",
    "def tokenize_data(tokenizer, dataset):\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=1024)\n",
    "    return dataset.map(tokenize_function, batched=True, desc=\"Tokenizing Data\")\n",
    "\n",
    "# Step 3: Prepare the dataset\n",
    "folder_path = r\"E:\\downloads\\txt_output\"  # Path to the folder containing .txt files\n",
    "data = load_txt_files(folder_path)\n",
    "dataset = Dataset.from_list(data)  # Convert to Hugging Face Dataset\n",
    "\n",
    "# Load tokenizer and tokenize the dataset\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT-J doesn't have a pad token, so use EOS\n",
    "tokenized_dataset = tokenize_data(tokenizer, dataset)\n",
    "\n",
    "# Step 4: Load the GPT-J model\n",
    "model = GPTJForCausalLM.from_pretrained(\n",
    "    \"EleutherAI/gpt-neo-1.3B\", \n",
    "    torch_dtype=torch.float16, \n",
    "    ignore_mismatched_sizes=True\n",
    ").cuda()\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Step 5: Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"no\",\n",
    "    eval_steps=500,\n",
    "    logging_steps=100,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    gradient_accumulation_steps=16,\n",
    "    per_device_train_batch_size=1,  # Adjust based on your GPU VRAM\n",
    "    per_device_eval_batch_size=1,\n",
    "    gradient_accumulation_steps=8,  # Accumulate gradients for larger effective batch size\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,  # Use mixed precision if GPU supports it\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"tensorboard\",  # Optional: log to TensorBoard\n",
    "    dataloader_num_workers=4,\n",
    "    logging_strategy=\"steps\",  # Log training progress\n",
    ")\n",
    "\n",
    "# Step 6: Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,  # Fixed to use tokenizer\n",
    "    callbacks=[ProgressCallback],  # Add progress bar for training\n",
    ")\n",
    "\n",
    "# Step 7: Train the model\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Step 8: Save the model and tokenizer\n",
    "print(\"Training complete. Saving the model and tokenizer...\")\n",
    "trainer.save_model(\"./fine_tuned_gptj\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_gptj\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "496fef65-6de2-4f8a-b3c5-c3eae95fbf6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenized_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Split the dataset into train and eval\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m split_datasets \u001b[38;5;241m=\u001b[39m tokenized_dataset\u001b[38;5;241m.\u001b[39mtrain_test_split(test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m)  \u001b[38;5;66;03m# Use 10% for evaluation\u001b[39;00m\n\u001b[0;32m      3\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m split_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      4\u001b[0m eval_dataset \u001b[38;5;241m=\u001b[39m split_datasets[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenized_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Split the dataset into train and eval\n",
    "split_datasets = tokenized_dataset.train_test_split(test_size=0.1)  # Use 10% for evaluation\n",
    "train_dataset = split_datasets[\"train\"]\n",
    "eval_dataset = split_datasets[\"test\"]\n",
    "\n",
    "# Update the Trainer initialization\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,  # Add eval dataset here\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0c77097b-fd49-4a62-92d5-3111a60f75a3",
   "metadata": {},
   "source": [
    "import os\n",
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "# Step 1: Load all .txt files from the folder with a progress bar\n",
    "def load_txt_files(folder_path):\n",
    "    data = []\n",
    "    for file_name in tqdm(os.listdir(folder_path), desc=\"Loading Text Files\"):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                data.append({\"text\": file.read()})\n",
    "    return data\n",
    "\n",
    "# Step 2: Tokenize the data with a progress bar\n",
    "def tokenize_data(tokenizer, dataset):\n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=1024)\n",
    "    return dataset.map(tokenize_function, batched=True, desc=\"Tokenizing Data\")\n",
    "\n",
    "# Step 3: Prepare the dataset\n",
    "folder_path = r\"E:\\downloads\\txt_output\"  # Path to the folder containing .txt files\n",
    "data = load_txt_files(folder_path)\n",
    "dataset = Dataset.from_list(data)  # Convert to Hugging Face Dataset\n",
    "\n",
    "# Load tokenizer and tokenize the dataset\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-2.7B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT-Neo doesn't have a pad token, so use EOS\n",
    "tokenized_dataset = tokenize_data(tokenizer, dataset)\n",
    "\n",
    "# Step 4: Load the GPT-Neo model\n",
    "model = GPTNeoForCausalLM.from_pretrained(\n",
    "    \"EleutherAI/gpt-neo-2.7B\",\n",
    "    torch_dtype=torch.float16\n",
    ").cuda()\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Step 5: Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    eval_strategy=\"no\",\n",
    "    eval_steps=500,\n",
    "    logging_steps=100,\n",
    "    save_steps=1000,\n",
    "    save_total_limit=2,\n",
    "    per_device_train_batch_size=8,  # Adjust based on your GPU VRAM\n",
    "    gradient_accumulation_steps=8,  # Accumulate gradients for larger effective batch size\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    fp16=True,  # Use mixed precision if GPU supports it\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"tensorboard\",\n",
    "    dataloader_num_workers=4,\n",
    ")\n",
    "\n",
    "# Step 6: Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Step 7: Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Step 8: Save the model and tokenizer\n",
    "trainer.save_model(\"./fine_tuned_gptneo\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_gptneo\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0039f92a-e796-483e-a748-4a4718bcfe5e",
   "metadata": {},
   "source": [
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "# Load a smaller model\n",
    "model = GPTNeoForCausalLM.from_pretrained(\n",
    "    \"EleutherAI/gpt-neo-125M\", \n",
    "    torch_dtype=torch.float16  # Enable mixed precision\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Ensure padding token is set\n",
    "\n",
    "# Training arguments with reduced batch size\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=1,  # Reduced batch size for low VRAM\n",
    "    gradient_accumulation_steps=16,  # Simulate larger batch size\n",
    "    fp16=True,  # Use mixed precision\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    save_steps=1000,\n",
    "    logging_steps=100,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,  # Ensure dataset is prepared\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model and tokenizer\n",
    "trainer.save_model(\"./fine_tuned_gptneo_125m\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_gptneo_125m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb8c5212-3a23-4691-a3d4-88836bbecf9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "CUDA available: True\n",
      "Device name: NVIDIA GeForce RTX 3050\n",
      "(8, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Text Files: 100%|████████████████████████████████████████████████████████████| 52/52 [00:00<00:00, 4001.83it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02369b2f805148f882abf1cd88ee5c3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing Data:   0%|          | 0/52 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esteb\\AppData\\Local\\Temp\\ipykernel_28844\\2189832020.py:82: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `CustomTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = CustomTrainer(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'CausalLMOutputWithPast' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 91\u001b[0m\n\u001b[0;32m     82\u001b[0m trainer \u001b[38;5;241m=\u001b[39m CustomTrainer(\n\u001b[0;32m     83\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     84\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     85\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_dataset,\n\u001b[0;32m     86\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,  \u001b[38;5;66;03m# Keep tokenizer for preprocessing\u001b[39;00m\n\u001b[0;32m     87\u001b[0m )\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Save the model and tokenizer\u001b[39;00m\n\u001b[0;32m     94\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./fine_tuned_gptneo_125m\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tensorflow\\Lib\\site-packages\\transformers\\trainer.py:2123\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2121\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2122\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2123\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   2124\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   2125\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   2126\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   2127\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   2128\u001b[0m     )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tensorflow\\Lib\\site-packages\\transformers\\trainer.py:2481\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2475\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2476\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2477\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2478\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2479\u001b[0m )\n\u001b[0;32m   2480\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2481\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m   2483\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2484\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2485\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2486\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2487\u001b[0m ):\n\u001b[0;32m   2488\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2489\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tensorflow\\Lib\\site-packages\\transformers\\trainer.py:3612\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3610\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m   3611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 3612\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   3613\u001b[0m     \u001b[38;5;66;03m# Finally we need to normalize the loss for reporting\u001b[39;00m\n\u001b[0;32m   3614\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_items_in_batch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tensorflow\\Lib\\site-packages\\accelerate\\accelerator.py:2231\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2227\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[0;32m   2230\u001b[0m     \u001b[38;5;66;03m# deepspeed handles loss scaling by gradient_accumulation_steps in its `backward`\u001b[39;00m\n\u001b[1;32m-> 2231\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n\u001b[0;32m   2232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[0;32m   2233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed_engine_wrapped\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'CausalLMOutputWithPast' and 'int'"
     ]
    }
   ],
   "source": [
    "from transformers.integrations import is_tensorboard_available\n",
    "print(is_tensorboard_available())\n",
    "import torch\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU found\")\n",
    "import torch\n",
    "print(torch.cuda.get_device_capability())\n",
    "\n",
    "\n",
    "# pip install tensorboard tqdm\n",
    "from transformers import TrainingArguments\n",
    "import os\n",
    "from transformers import GPTJForCausalLM, GPT2Tokenizer, Trainer, TrainingArguments, ProgressCallback\n",
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm  # For custom progress bars\n",
    "import torch\n",
    "\n",
    "# Step 1: Load all .txt files from the folder with a progress bar\n",
    "def load_txt_files(folder_path):\n",
    "    data = []\n",
    "    for file_name in tqdm(os.listdir(folder_path), desc=\"Loading Text Files\"):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                data.append({\"text\": file.read()})\n",
    "    return data\n",
    "\n",
    "def tokenize_data(tokenizer, dataset):\n",
    "    def tokenize_function(examples):\n",
    "        inputs = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=1024,\n",
    "        )\n",
    "        return {\"input_ids\": inputs[\"input_ids\"], \"attention_mask\": inputs[\"attention_mask\"]}\n",
    "    \n",
    "    return dataset.map(tokenize_function, batched=True, desc=\"Tokenizing Data\")\n",
    "\n",
    "# Step 3: Prepare the dataset\n",
    "folder_path = r\"E:\\downloads\\txt_output\"  # Path to the folder containing .txt files\n",
    "data = load_txt_files(folder_path)\n",
    "dataset = Dataset.from_list(data)  # Convert to Hugging Face Dataset\n",
    "\n",
    "# Load tokenizer and tokenize the dataset\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-1.3B\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT-J doesn't have a pad token, so use EOS\n",
    "tokenized_dataset = tokenize_data(tokenizer, dataset)\n",
    "\n",
    "\n",
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "\n",
    "# Load model on CPU\n",
    "model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=1,  # Smallest batch size\n",
    "    gradient_accumulation_steps=16,  # Simulate larger batch size\n",
    "    fp16=False,  # Disable mixed precision on CPU\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=5e-5,\n",
    "    save_steps=1000,\n",
    "    logging_steps=100,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        # Directly return the model outputs without computing loss\n",
    "        outputs = model(**inputs)\n",
    "        return (outputs, outputs) if return_outputs else outputs\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,  # Keep tokenizer for preprocessing\n",
    ")\n",
    "\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model and tokenizer\n",
    "trainer.save_model(\"./fine_tuned_gptneo_125m\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_gptneo_125m\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bed99396-4342-4716-870a-58fc9f63e2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Text Files: 100%|████████████████████████████████████████████████████████████| 52/52 [00:00<00:00, 3059.35it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95eb10d7d0f344ddbed61337b4434b98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing Data:   0%|          | 0/52 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import GPTNeoForCausalLM, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Step 1: Load all .txt files from the folder\n",
    "def load_txt_files(folder_path):\n",
    "    data = []\n",
    "    for file_name in tqdm(os.listdir(folder_path), desc=\"Loading Text Files\"):\n",
    "        if file_name.endswith(\".txt\"):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "                data.append({\"text\": file.read()})\n",
    "    return data\n",
    "\n",
    "# Step 2: Tokenize the data and add labels\n",
    "def tokenize_data(tokenizer, dataset):\n",
    "    def tokenize_function(examples):\n",
    "        inputs = tokenizer(\n",
    "            examples[\"text\"],\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=1024,\n",
    "        )\n",
    "        inputs[\"labels\"] = inputs[\"input_ids\"].copy()  # Add labels for loss calculation\n",
    "        return inputs\n",
    "\n",
    "    return dataset.map(tokenize_function, batched=True, desc=\"Tokenizing Data\")\n",
    "\n",
    "# Step 3: Load dataset\n",
    "folder_path = r\"E:\\downloads\\txt_output\"  # Adjust to your dataset folder path\n",
    "data = load_txt_files(folder_path)\n",
    "dataset = Dataset.from_list(data)  # Convert list to Hugging Face Dataset\n",
    "\n",
    "# Step 4: Load tokenizer and tokenize dataset\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Ensure padding token is set\n",
    "tokenized_dataset = tokenize_data(tokenizer, dataset)\n",
    "\n",
    "# Step 5: Load model\n",
    "model = GPTNeoForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f5ccbd-803b-422c-bf7c-7445b80379b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0776a0-a0e6-4b59-ae1a-aba32a0163e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers torch\n",
    "from transformers import GPTJForCausalLM, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# Path to the fine-tuned model and tokenizer\n",
    "model_path = \"./fine_tuned_gptj\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "\n",
    "# Load the fine-tuned model\n",
    "model = GPTJForCausalLM.from_pretrained(model_path, torch_dtype=torch.float16).cuda()  # Use .cpu() if no GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b1c67f-3b7f-4275-91ee-1d3f42375092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(prompt, max_length=100, temperature=1.0, top_p=0.9):\n",
    "    # Tokenize the input prompt\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")  # Use .to(\"cpu\") if no GPU\n",
    "    \n",
    "    # Generate text\n",
    "    outputs = model.generate(\n",
    "        inputs[\"input_ids\"],\n",
    "        max_length=max_length,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        do_sample=True,\n",
    "        eos_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode the generated text\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "prompt = \"Once upon a time\"\n",
    "generated_text = generate_text(prompt)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d45571-ded7-41ff-9e0a-48faa98d3896",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install fastapi uvicorn\n",
    "\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "class Prompt(BaseModel):\n",
    "    text: str\n",
    "\n",
    "@app.post(\"/generate\")\n",
    "def generate(prompt: Prompt):\n",
    "    generated_text = generate_text(prompt.text)\n",
    "    return {\"response\": generated_text}\n",
    "\n",
    "# Run with: uvicorn app:app --reload\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "219cefbf-443f-4387-9825-4f4233cb829c",
   "metadata": {},
   "source": [
    "#######################################\n",
    "USE TRAINED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8ed634a1-fee6-4be6-9179-f1a0acbbf5a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: The specified folder is missing required model or tokenizer files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "\n",
    "# Path to the folder containing the model and tokenizer\n",
    "model_folder = r\"E:\\downloads\\fine_tuned_gptneo_125m\"\n",
    "\n",
    "# Step 1: Load the Model and Tokenizer\n",
    "def load_model_and_tokenizer(folder_path):\n",
    "    # Ensure the folder contains all necessary files\n",
    "    required_files = [\"config.json\", \"pytorch_model.bin\", \"tokenizer.json\", \"vocab.json\", \"merges.txt\"]\n",
    "    if not all(os.path.isfile(os.path.join(folder_path, file)) for file in required_files):\n",
    "        raise ValueError(\"The specified folder is missing required model or tokenizer files.\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(folder_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(folder_path)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    print(f\"Model and tokenizer successfully loaded on {device}.\")\n",
    "    return tokenizer, model, device\n",
    "\n",
    "# Step 2: Question-Answer Interface\n",
    "def interactive_qa(tokenizer, model, device):\n",
    "    print(\"Interactive Q&A Interface\")\n",
    "    print(\"Type 'exit' to quit.\")\n",
    "    \n",
    "    while True:\n",
    "        # Get user input\n",
    "        question = input(\"\\nAsk a question: \")\n",
    "        if question.lower() == \"exit\":\n",
    "            print(\"Exiting interactive Q&A. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            max_tokens = int(input(\"Enter the maximum number of tokens for the answer (e.g., 50): \"))\n",
    "        except ValueError:\n",
    "            print(\"Invalid input for max tokens. Using default of 50.\")\n",
    "            max_tokens = 50\n",
    "\n",
    "        # Generate the response\n",
    "        inputs = tokenizer(question, return_tensors=\"pt\").to(device)\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_length=inputs[\"input_ids\"].shape[1] + max_tokens,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=50\n",
    "        )\n",
    "        answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        print(f\"\\nAnswer: {answer}\")\n",
    "\n",
    "# Step 3: Main Function to Load Model and Start Interface\n",
    "def main():\n",
    "    try:\n",
    "        tokenizer, model, device = load_model_and_tokenizer(model_folder)\n",
    "        interactive_qa(tokenizer, model, device)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Run the program\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d780444-ba73-4df2-b313-4b008b33e126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer successfully loaded on cuda.\n",
      "Interactive Q&A Interface\n",
      "Type 'exit' to quit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask a question:  cicima ucr give me paper article name\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: cicima ucr give me paper article name on the list and link for it\n",
      "<mrooney> ok\n",
      "<mrooney> ok cool\n",
      "<mrooney>  /join #ubuntu-it-offtopic\n",
      "<mrooney> https://www.tuxfiles.org/\n",
      "<mrooney>  /join #ubuntu-offtopic\n",
      "<mrooney>  /join #ubuntu-offtopic\n",
      "<mrooney>  /join #ubuntu-offtopic\n",
      "<mrooney>  /join #ubuntu-offtopic\n",
      "<mrooney> https://www.tuxfiles.org/\n",
      "<mrooney>  /join #ubuntu-offtopic\n",
      "<mrooney> https://www.tuxfiles.org/\n",
      "<hanswers> Hi\n",
      "<mrooney> https://www.tuxfiles.org/\n",
      "<hanswers> Hi\n",
      "<hanswers> Hi\n",
      "<hanswers> How can I enable the new initscript in the XF86?\n",
      "<hanswers>  I don't see a way to make a new initscript, is there?\n",
      "<hanswers>\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "\n",
      "Ask a question:  give me the names of every article with material science in its name\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Answer: give me the names of every article with material science in its name.\n",
      "\n",
      "The authors of this publication provide full text of their research. The authors are not responsible for the content of any content on this page.\n",
      "\n",
      "The content on this page may not be reproduced, published, displayed, or referred to in any way without prior author's written permission in accordance with the terms of the Licence.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Path to the folder containing the model and tokenizer\n",
    "#model_folder = r\"E:\\downloads\\fine_tuned_gptneo_125m\"\n",
    "model_folder = r\"fine_tuned_gptneo_125m\"\n",
    "\n",
    "# Step 1: Load the Model and Tokenizer\n",
    "def load_model_and_tokenizer(folder_path):\n",
    "    # Ensure the folder contains all necessary files\n",
    "    required_files = [\"config.json\", \"model.safetensors\", \"tokenizer.json\", \"vocab.json\", \"merges.txt\"]\n",
    "    if not all(os.path.isfile(os.path.join(folder_path, file)) for file in required_files):\n",
    "        raise ValueError(\"The specified folder is missing required model or tokenizer files.\")\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(folder_path)\n",
    "    model = AutoModelForCausalLM.from_pretrained(folder_path, trust_remote_code=True)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    print(f\"Model and tokenizer successfully loaded on {device}.\")\n",
    "    return tokenizer, model, device\n",
    "\n",
    "# Step 2: Question-Answer Interface\n",
    "def interactive_qa(tokenizer, model, device):\n",
    "    print(\"Interactive Q&A Interface\")\n",
    "    print(\"Type 'exit' to quit.\")\n",
    "    \n",
    "    while True:\n",
    "        # Get user input\n",
    "        question = input(\"\\nAsk a question: \")\n",
    "        if question.lower() == \"exit\":\n",
    "            print(\"Exiting interactive Q&A. Goodbye!\")\n",
    "            break\n",
    "\n",
    "        try:\n",
    "            max_tokens = 250 #int(input(\"Enter the maximum number of tokens for the answer (e.g., 50): \"))\n",
    "        except ValueError:\n",
    "            print(\"Invalid input for max tokens. Using default of 50.\")\n",
    "            max_tokens = 50\n",
    "\n",
    "        # Generate the response\n",
    "        inputs = tokenizer(question, return_tensors=\"pt\").to(device)\n",
    "        output = model.generate(\n",
    "            **inputs,\n",
    "            max_length=inputs[\"input_ids\"].shape[1] + max_tokens,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            top_k=50\n",
    "        )\n",
    "        answer = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "        print(f\"\\nAnswer: {answer}\")\n",
    "\n",
    "# Step 3: Main Function to Load Model and Start Interface\n",
    "def main():\n",
    "    try:\n",
    "        tokenizer, model, device = load_model_and_tokenizer(model_folder)\n",
    "        interactive_qa(tokenizer, model, device)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "\n",
    "# Run the program\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7c0fab13-2e4d-4149-a654-d2ba5251bcae",
   "metadata": {},
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPTNeoForCausalLM, AutoTokenizer\n",
    "\n",
    "# Enable debugging for CUDA errors\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "# Variables\n",
    "folder_path = r\"E:\\downloads\\txt_output\"  # Path to your .txt files\n",
    "output_dir = \"./fine_tuned_gptneo_125m\"\n",
    "max_length = 1024\n",
    "batch_size = 1\n",
    "epochs = 3\n",
    "learning_rate = 5e-6\n",
    "\n",
    "# Step 1: Load Tokenizer and Model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token  # Set pad_token to eos_token\n",
    "\n",
    "# Check CUDA availability and load the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "model = GPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125M\").to(device)\n",
    "\n",
    "# Step 2: Custom Dataset Class\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, folder_path, tokenizer, max_length):\n",
    "        self.samples = []\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                filepath = os.path.join(folder_path, filename)\n",
    "                with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "                    content = f.read()\n",
    "                tokenized = tokenizer(\n",
    "                    content,\n",
    "                    truncation=True,\n",
    "                    max_length=max_length,\n",
    "                    padding=\"max_length\",\n",
    "                    return_tensors=\"pt\",\n",
    "                )\n",
    "                self.samples.append({\n",
    "                    \"input_ids\": tokenized[\"input_ids\"].squeeze(0),\n",
    "                    \"attention_mask\": tokenized[\"attention_mask\"].squeeze(0),\n",
    "                })\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "\n",
    "# Step 3: Load Dataset and DataLoader\n",
    "dataset = TextDataset(folder_path, tokenizer, max_length)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Step 4: Training Components\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Step 5: Training Loop\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    for step, batch in enumerate(dataloader):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print(f\"Step {step} | Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Step 6: Save Model and Tokenizer\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(\"Training complete and model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a313338-0fbc-404e-aebf-3fa1e6185912",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
