{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9bf1fb10-b701-4bcc-938d-280c9cf764e8",
   "metadata": {},
   "source": [
    "import openai\n",
    "\n",
    "# Set your OpenAI API key\n",
    "openai.api_key = 'your_openai_api_key'\n",
    "\n",
    "def ask_question(prompt):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo\",  # Replace with a different model if needed\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        max_tokens=500  # Adjust the token limit as necessary\n",
    "    )\n",
    "    return response['choices'][0]['message']['content'].strip()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Simple LLM Terminal Interface\")\n",
    "    print(\"Type 'exit' to quit.\")\n",
    "    while True:\n",
    "        user_input = input(\"Ask your question: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "        answer = ask_question(user_input)\n",
    "        print(\"\\nResponse:\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f580b48a-cf98-4fc0-a553-5220c8e4b02e",
   "metadata": {},
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "# Ensure to set your OpenAI API key in your environment variables for security\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "def ask_question(prompt):\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-4\",  # Use a newer model if available, or stick with \"gpt-3.5-turbo\"\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            max_tokens=500,  # Adjust the token limit as necessary\n",
    "            temperature=0.7  # You can modify this to control the randomness of the response\n",
    "        )\n",
    "        return response['choices'][0]['message']['content'].strip()\n",
    "    except openai.error.OpenAIError as e:\n",
    "        return f\"An error occurred: {e}\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Simple LLM Terminal Interface\")\n",
    "    print(\"Type 'exit' to quit.\")\n",
    "    while True:\n",
    "        user_input = input(\"\\nAsk your question: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        answer = ask_question(user_input)\n",
    "        print(\"\\nResponse:\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d2c32ee-d8b5-4b96-8683-aceca86a3934",
   "metadata": {},
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "# Ensure your OpenAI API key is stored in an environment variable for security\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "if not api_key:\n",
    "    raise ValueError(\"OpenAI API key not found. Set the OPENAI_API_KEY environment variable.\")\n",
    "openai.api_key = api_key\n",
    "\n",
    "def ask_question(prompt):\n",
    "    try:\n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=\"gpt-3.5-turbo\",  # or \"gpt-4\" if available\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            max_tokens=500,  # Adjust this as necessary\n",
    "            temperature=0.7  # Adjust for more or less randomness\n",
    "        )\n",
    "        return response.choices[0].message['content'].strip()\n",
    "    except openai.error.OpenAIError as e:\n",
    "        return f\"An OpenAI error occurred: {e}\"\n",
    "    except Exception as e:\n",
    "        return f\"An unexpected error occurred: {e}\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Simple LLM Terminal Interface\")\n",
    "    print(\"Type 'exit' to quit.\")\n",
    "    while True:\n",
    "        user_input = input(\"\\nAsk your question: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "        answer = ask_question(user_input)\n",
    "        print(\"\\nResponse:\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7bb121fe-fa70-4b41-880c-adf6655a2904",
   "metadata": {},
   "source": [
    "# python --version\n",
    "# nvcc --version\n",
    "# git clone https://github.com/kingoflolz/mesh-transformer-jax.git\n",
    "# cd mesh-transformer-jax\n",
    "pip install -r requirements.txt\n",
    "pip install transformers torch\n",
    "pip install setuptools\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7bc2af-a8c9-4feb-940b-603b049762b2",
   "metadata": {},
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"EleutherAI/gpt-j-6B\"  # Adjust this name if using a different model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "706ccdb2-24f8-466e-a9da-4633e966d2cb",
   "metadata": {},
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"EleutherAI/gpt-j-6B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Sample input text\n",
    "input_text = \"What is the future of artificial intelligence?\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Generate response\n",
    "output = model.generate(input_ids, max_length=100)\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Response:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32c171e5-d8e8-4b05-8955-5a623d30a2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at EleutherAI/gpt-j-6B were not used when initializing GPTJForCausalLM: ['transformer.h.0.attn.bias', 'transformer.h.0.attn.masked_bias', 'transformer.h.1.attn.bias', 'transformer.h.1.attn.masked_bias', 'transformer.h.10.attn.bias', 'transformer.h.10.attn.masked_bias', 'transformer.h.11.attn.bias', 'transformer.h.11.attn.masked_bias', 'transformer.h.12.attn.bias', 'transformer.h.12.attn.masked_bias', 'transformer.h.13.attn.bias', 'transformer.h.13.attn.masked_bias', 'transformer.h.14.attn.bias', 'transformer.h.14.attn.masked_bias', 'transformer.h.15.attn.bias', 'transformer.h.15.attn.masked_bias', 'transformer.h.16.attn.bias', 'transformer.h.16.attn.masked_bias', 'transformer.h.17.attn.bias', 'transformer.h.17.attn.masked_bias', 'transformer.h.18.attn.bias', 'transformer.h.18.attn.masked_bias', 'transformer.h.19.attn.bias', 'transformer.h.19.attn.masked_bias', 'transformer.h.2.attn.bias', 'transformer.h.2.attn.masked_bias', 'transformer.h.20.attn.bias', 'transformer.h.20.attn.masked_bias', 'transformer.h.21.attn.bias', 'transformer.h.21.attn.masked_bias', 'transformer.h.22.attn.bias', 'transformer.h.22.attn.masked_bias', 'transformer.h.23.attn.bias', 'transformer.h.23.attn.masked_bias', 'transformer.h.24.attn.bias', 'transformer.h.24.attn.masked_bias', 'transformer.h.25.attn.bias', 'transformer.h.25.attn.masked_bias', 'transformer.h.26.attn.bias', 'transformer.h.26.attn.masked_bias', 'transformer.h.27.attn.bias', 'transformer.h.27.attn.masked_bias', 'transformer.h.3.attn.bias', 'transformer.h.3.attn.masked_bias', 'transformer.h.4.attn.bias', 'transformer.h.4.attn.masked_bias', 'transformer.h.5.attn.bias', 'transformer.h.5.attn.masked_bias', 'transformer.h.6.attn.bias', 'transformer.h.6.attn.masked_bias', 'transformer.h.7.attn.bias', 'transformer.h.7.attn.masked_bias', 'transformer.h.8.attn.bias', 'transformer.h.8.attn.masked_bias', 'transformer.h.9.attn.bias', 'transformer.h.9.attn.masked_bias']\n",
      "- This IS expected if you are initializing GPTJForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPTJForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\esteb\\miniconda3\\envs\\tensorflow\\Lib\\site-packages\\transformers\\generation\\utils.py:1375: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Hello, how are you?\n",
      "\n",
      "I’m doing well, thanks for asking.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPTJForCausalLM, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = GPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\")\n",
    "\n",
    "# Set pad_token_id if not already set\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Encode input with attention mask\n",
    "input_text = \"Hello, how are you?\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "attention_mask = (input_ids != tokenizer.pad_token_id).long()  # Generate attention mask\n",
    "\n",
    "# Generate output with attention mask\n",
    "output = model.generate(\n",
    "    input_ids, \n",
    "    attention_mask=attention_mask, \n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode and print the output\n",
    "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated text:\", decoded_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45b54cd6-e0c0-4845-8378-13b09131c202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: You were a great investment did you know?\n",
      "\n",
      "I’m not sure if you�\n"
     ]
    }
   ],
   "source": [
    "# Encode input with attention mask\n",
    "input_text = \"You were a great investment did you know?\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "attention_mask = (input_ids != tokenizer.pad_token_id).long()  # Generate attention mask\n",
    "\n",
    "# Generate output with attention mask\n",
    "output = model.generate(\n",
    "    input_ids, \n",
    "    attention_mask=attention_mask, \n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode and print the output\n",
    "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated text:\", decoded_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "267f4af4-25a8-482b-88d0-d5557fdc3898",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: What did you say?\n",
      "\n",
      "\"I said, 'I'm not going to be a part\n"
     ]
    }
   ],
   "source": [
    "# Encode input with attention mask\n",
    "input_text = \"What did you say?\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "attention_mask = (input_ids != tokenizer.pad_token_id).long()  # Generate attention mask\n",
    "\n",
    "# Generate output with attention mask\n",
    "output = model.generate(\n",
    "    input_ids, \n",
    "    attention_mask=attention_mask, \n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode and print the output\n",
    "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated text:\", decoded_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bf8bc78-377f-4abc-a997-85185dc08df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Part of what?\n",
      "\n",
      "A:\n",
      "\n",
      "The problem is that you are using the wrong method\n"
     ]
    }
   ],
   "source": [
    "# Encode input with attention mask\n",
    "input_text = \"Part of what?\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "attention_mask = (input_ids != tokenizer.pad_token_id).long()  # Generate attention mask\n",
    "\n",
    "# Generate output with attention mask\n",
    "output = model.generate(\n",
    "    input_ids, \n",
    "    attention_mask=attention_mask, \n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode and print the output\n",
    "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated text:\", decoded_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a4e00c3-54a5-45e7-b1a0-731e99f81b94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated text: Easy cowboy, everything is okay.\" \"I'm not gonna hurt you.\" \"I'm not gonna\n"
     ]
    }
   ],
   "source": [
    "# Encode input with attention mask\n",
    "input_text = \"Easy cowboy, everything is okay\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "attention_mask = (input_ids != tokenizer.pad_token_id).long()  # Generate attention mask\n",
    "\n",
    "# Generate output with attention mask\n",
    "output = model.generate(\n",
    "    input_ids, \n",
    "    attention_mask=attention_mask, \n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "# Decode and print the output\n",
    "decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "print(\"Generated text:\", decoded_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce1bfb88-afc8-433a-a3b0-19d8048141cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(input_text):\n",
    "    # Encode input with attention mask\n",
    "    input_text = \"Easy cowboy, everything is okay\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    attention_mask = (input_ids != tokenizer.pad_token_id).long()  # Generate attention mask\n",
    "    \n",
    "    # Generate output with attention mask\n",
    "    output = model.generate(\n",
    "        input_ids, \n",
    "        attention_mask=attention_mask, \n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    \n",
    "    # Decode and print the output\n",
    "    decoded_output = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    print(\"Generated text:\", decoded_output)\n",
    "    \n",
    "def conversation():\n",
    "    while True:\n",
    "        input_text = input()\n",
    "        get_response(input_text)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cda3ef24-eb6b-411f-9528-26b6f3907c8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m conversation()\n",
      "Cell \u001b[1;32mIn[6], line 20\u001b[0m, in \u001b[0;36mconversation\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconversation\u001b[39m():\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m---> 20\u001b[0m         input_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m()\n\u001b[0;32m     21\u001b[0m         get_response(input_text)\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tensorflow\\Lib\\site-packages\\ipykernel\\kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[1;34m(self, prompt)\u001b[0m\n\u001b[0;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[1;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_request(\n\u001b[0;32m   1283\u001b[0m     \u001b[38;5;28mstr\u001b[39m(prompt),\n\u001b[0;32m   1284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_ident[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1285\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_parent(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshell\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1286\u001b[0m     password\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   1287\u001b[0m )\n",
      "File \u001b[1;32m~\\miniconda3\\envs\\tensorflow\\Lib\\site-packages\\ipykernel\\kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[1;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[0;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[0;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e492f00e-43fd-4132-b8a2-8bea91e8929b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
