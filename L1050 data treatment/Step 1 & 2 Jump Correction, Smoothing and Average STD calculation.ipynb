{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7021c4a",
   "metadata": {},
   "source": [
    "# Batch Lamda1050 jump correction script and smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d722b6",
   "metadata": {},
   "source": [
    "### Author: Vinicio Soto, CICIMA\n",
    "### This script works with raw Lambdas1050's ASC files in batch. It corrects Lambdas1050's detector jump between 857 nm and 858 nm and performs a Savitzky-Golay filtering on the corrected data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24254f0e",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9f993aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e36150b",
   "metadata": {},
   "source": [
    "### Instructions:\n",
    "\n",
    "Write your archive folders' path in file_folder_data_path list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba698280",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lists all archives in folder\n",
    "file_folder_data_path =[ r\"C:\\Users\\EstebanSoto\\Jupyter\\escarabajos\\L1050_data\\CICIMA-2024-05-REFLECTANCE\\DORSAL\"\n",
    "                        ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9906ebe",
   "metadata": {},
   "source": [
    "### Constants\n",
    "Define your constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e00b7fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine \n",
    "markersize = 1\n",
    "#date\n",
    "date = str(datetime.today().date())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8fdd61",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a26e3dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metadata_and_dataframe(file_location):\n",
    "    \"\"\"This function reads L1050 .ASC files directly and returns a dictionary with metadata and the dataframe with the measurements\"\"\"\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import re\n",
    "\n",
    "    def get_sample_code_from_filename(row_str, file_location):\n",
    "        #print(\"string\")\n",
    "        #print(file_location)\n",
    "        filename = os.path.basename(file_location)\n",
    "        re1 = r\"([a-zA-Z\\d]+)(?:-\\d)*(?:.Sample)*.(?:txt)*(?:ASC)*\"\n",
    "        #Names are in the form CODE-MEASUREMENTNUMBER.TXT\n",
    "        p = re.compile(re1)\n",
    "        m = p.match(filename)\n",
    "        # print(f\"match filename: {m}\")\n",
    "        if m:\n",
    "            # print(f\"group 1: {m.group(1)}\")\n",
    "            return(m.group(1))\n",
    "        return get_sample_code(file_str)\n",
    "\n",
    "    def get_sample_code(row_str):\n",
    "        #Tries to get the sample code from the file, if it does not match\n",
    "        #it tries to get it from the filename.\n",
    "        # print(\"string\")\n",
    "        # print(row_str)\n",
    "        re1 = r\"([a-zA-Z\\d]+)(?:-\\d)*(?:.Sample)*.(?:txt)*(?:ASC)*\"\n",
    "        #Names are in the form CODE-MEASUREMENTNUMBER.TXT\n",
    "        p = re.compile(re1)\n",
    "        m = p.match(row_str)\n",
    "        # print(f\"match: {m}\")\n",
    "        if m:\n",
    "            return(m.group(1))\n",
    "        else:\n",
    "            \"\"\n",
    "\n",
    "    def responses(str):\n",
    "        re1 = \"\\d+/(\\d+,\\d+) \\d+,\\d+/(\\d+,\\d+)\"\n",
    "        p = re.compile(re1)\n",
    "        m= p.match(str)\n",
    "        if m:\n",
    "            return m.group(1),m.group(2)\n",
    "        else:\n",
    "            return \"\",\"\"\n",
    "    def attenuator_settings(str):\n",
    "        re1 = \"S:(\\d+,\\d+) R:(\\d+,\\d+)\"\n",
    "        p = re.compile(re1)\n",
    "        m= p.match(str)\n",
    "        if m:\n",
    "            return m.group(1),m.group(2)\n",
    "        else:\n",
    "            return \"\",\"\"\n",
    "    def slit_pmt_aperture(str):\n",
    "        re1 = \"\\d+/servo \\d+,\\d+/(\\d+,\\d+)\"\n",
    "        p = re.compile(re1)\n",
    "        m= p.match(str)\n",
    "        if m:\n",
    "            return m.group(1)\n",
    "        else:\n",
    "            return \"\"\n",
    "    #Initializa metadata dict\n",
    "    metadata = {}\n",
    "\n",
    "    #Read header\n",
    "    lines = []\n",
    "    with open(file_location, encoding= \"latin1\") as myfile:\n",
    "        lines = myfile.readlines()[0:90]\n",
    "    metadata[\"header\"] = \"\".join(lines)\n",
    "\n",
    "\n",
    "    #read_metadata\n",
    "    f = open(file_location, encoding= \"latin1\")\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    with f as data_file:\n",
    "        for index, row in enumerate(data_file): #0-89\n",
    "\n",
    "            row_str = row.strip()\n",
    "            if index +1 == 3: #Filename and extension\n",
    "                metadata[\"filename\"]= row_str\n",
    "                metadata[\"code\"] = get_sample_code_from_filename(row_str, file_location)\n",
    "            if index + 1 == 4: #date DD/MM/YYYY\n",
    "                metadata[\"date\"]= row_str\n",
    "            if index + 1 == 5:#Time HH:MM:SS.SS\n",
    "                metadata[\"time\"]= row_str\n",
    "            if index + 1 == 8:#user\n",
    "                metadata[\"user\"]= row_str\n",
    "            if index + 1 == 9:#description\n",
    "                metadata[\"description\"]= row_str\n",
    "            if index + 1 == 10:#minimum wavelength\n",
    "                metadata[\"minimum_wavelength\"]= row_str\n",
    "            if index + 1 == 12:#equipment name\n",
    "                metadata[\"equipment\"]= row_str\n",
    "            if index + 1 == 13:#equipment series\n",
    "                metadata[\"series\"]= row_str\n",
    "            if index + 1 == 14:#data visualizer version, equipment version, date and time\n",
    "                metadata[\"software\"]= row_str\n",
    "            if index + 1 == 21:#Operating mode\n",
    "                metadata[\"operating_mode\"]= row_str\n",
    "            if index + 1 == 22: #Number of cycles\n",
    "                metadata[\"cycles\"]= row_str\n",
    "            if index + 1 == 32: #range/servo\n",
    "                metadata[\"slit_pmt\"]= slit_pmt_aperture(row_str)\n",
    "            if index + 1 == 33:\n",
    "                metadata[\"response_ingaas\"], metadata[\"response_pmt\"]= responses(row_str)\n",
    "            if index + 1 == 35: #pmt gain, if 0 is automatic\n",
    "                metadata[\"pmt_gain\"]= row_str\n",
    "            if index + 1 == 36: #InGaAs detector gain\n",
    "                metadata[\"ingaas_gain\"]= row_str\n",
    "            if index + 1 == 42:#monochromator wavelength nm\n",
    "                metadata[\"monochromator_change\"]= row_str\n",
    "            if index + 1 == 43:#lamp change wavelength\n",
    "                metadata[\"lamp_change\"]= row_str\n",
    "            if index + 1 == 44:#pmt wavelength\n",
    "                metadata[\"pmt_change\"]= row_str\n",
    "            if index + 1 == 45:#beam selector\n",
    "                metadata[\"beam_selector\"]= row_str\n",
    "            if index + 1 == 46:\n",
    "                metadata[\"cbm\"]= row_str\n",
    "            if index + 1 == 47: #cbd status, on/off\n",
    "                metadata[\"cbd_status\"]= row_str\n",
    "            if index + 1 == 48: #attenuator percentage\n",
    "                metadata[\"attenuator_sample\"], metadata[\"attenuator_reference\"]= attenuator_settings(row_str)\n",
    "            if index + 1 == 49:\n",
    "                metadata[\"polarizer\"]= row_str\n",
    "            if index + 1 == 80:\n",
    "                metadata[\"units\"]= row_str\n",
    "            if index + 1 == 81:\n",
    "                metadata[\"measuring_mode\"]= row_str\n",
    "            if index + 1 == 84:\n",
    "                metadata[\"maximum_wavelength\"]= row_str\n",
    "            if index + 1 == 85:\n",
    "                metadata[\"step\"]= row_str\n",
    "            if index + 1 == 86:\n",
    "                metadata[\"number_of_datapoints\"]= row_str\n",
    "            if index + 1 == 88:\n",
    "                metadata[\"maximum_measurement\"]= row_str\n",
    "            if index + 1 == 89:\n",
    "                metadata[\"minimum_measurement\"]= row_str\n",
    "            if index +1 == 90:\n",
    "                break\n",
    "        df = pd.read_csv(f, sep=\"\\t\", decimal =\",\", names=[\"wavelength\", metadata[\"measuring_mode\"]]).dropna()\n",
    "        \n",
    "        df[\"wavelength\"],df[metadata[\"measuring_mode\"]] = df[\"wavelength\"].astype(float), df[metadata[\"measuring_mode\"]].astype(float)\n",
    "        return metadata, df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1543d56",
   "metadata": {},
   "source": [
    "### Get genera and species function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ddca9f",
   "metadata": {},
   "source": [
    "### Correction process function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40b25e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jump_correction(file_folder_data_path, filename, metadata):\n",
    "    #measuring_mode\n",
    "    measuring_mode = metadata[\"measuring_mode\"]\n",
    "    \n",
    "    #constants\n",
    "    markersize = 1 #Marker size for plots\n",
    "\n",
    "    # read text file into pandas DataFrame \n",
    "    full_path= file_folder_data_path + \"\\\\\" + filename\n",
    "    \n",
    "    #Read header\n",
    "    lines = []\n",
    "    with open(full_path) as myfile:\n",
    "        lines = myfile.readlines()[0:90] \n",
    "    header = \"\".join(lines)\n",
    "    metadata[\"header\"] = header\n",
    "\n",
    "    #read body\n",
    "    f = open(full_path)\n",
    "    df = pd.read_csv(f, sep=\"\\t\", decimal = \",\", skiprows=90,  \n",
    "                     names=[\"wavelength\", measuring_mode])\n",
    "    \n",
    "    #convert columns to float\n",
    "    df['wavelength'] = df['wavelength'].astype(float)\n",
    "    df[measuring_mode] = df[measuring_mode].astype(float)\n",
    "    \n",
    "\n",
    "    #This code finds the rows before and after the detector change\n",
    "    #freq1 is the frequency before the change and freq2 is the frequency after the change\n",
    "    wavelenght1 = 857.000000 #857\n",
    "    wavelenght2 = 858.000000 #858\n",
    "    \n",
    "    #print(\"Loc\")\n",
    "    row1 = df.loc[ df['wavelength'] == wavelenght1]\n",
    "    row2 = df.loc[ df['wavelength'] == wavelenght2]\n",
    "    row12 = df.loc[ df['wavelength'] == wavelenght1-1]\n",
    "    row13 = df.loc[ df['wavelength'] == wavelenght1-2]\n",
    "    row14 = df.loc[ df['wavelength'] == wavelenght1-3]\n",
    "    row15 = df.loc[ df['wavelength'] == wavelenght1-4]\n",
    "    \n",
    "    #This code finds the values corresponding to those frequencies and creates a correction factor\n",
    "    \n",
    "    try:\n",
    "        data1 = row1.iat[0,1]\n",
    "        data2 = row2.iat[0,1]\n",
    "        data22 = row12.iat[0,1]\n",
    "        data23 = row13.iat[0,1]\n",
    "        data24 = row14.iat[0,1]\n",
    "        data25 = row15.iat[0,1]\n",
    "    \n",
    "        data1_avg = (data1 + data22+data23)/3 #+data24+data25)/5\n",
    "        data2_avg = (data2)\n",
    "        correction_factor= data1_avg/data2_avg  \n",
    "    \n",
    "        #Multiply all frequencies equal or greater than freq2 by correction_factor\n",
    "        df2 = df\n",
    "        df2.loc[df2[\"wavelength\"] >= wavelenght2 , [measuring_mode]] *= correction_factor\n",
    "        \n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        \n",
    "        print(e)\n",
    "        return pd.DataFrame([])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcf87b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_corrected_df(df, folder, filename,metadata):\n",
    "    #variables\n",
    "    header = metadata[\"header\"]\n",
    "    \n",
    "    #convert dataframe to numpy\n",
    "    my_numpy_df_data = df.to_numpy()\n",
    "    \n",
    "\n",
    "    #Create new folder\n",
    "    \n",
    "    corrected_path = os.path.join(folder , \"jump_corrected_files\")\n",
    "\n",
    "    if not os.path.exists(corrected_path):\n",
    "        os.mkdir(corrected_path)\n",
    "\n",
    "    #saves file, removes .ASC and removes .SAMPLE\n",
    "    unnecessary_text = [\".Sample\", \".Cycle1\"]\n",
    "    temp_text = filename.replace('.ASC', ''+'.txt')\n",
    "    \n",
    "    for text in unnecessary_text:\n",
    "        #print(temp_text)\n",
    "        temp_text = temp_text.replace(text, \"\")\n",
    "\n",
    "    #new_archive_name = corrected_path + temp_text\n",
    "    new_archive_name = os.path.join(corrected_path , temp_text)\n",
    "\n",
    "    with open(new_archive_name, 'wb') as f:\n",
    "        np.savetxt(new_archive_name, my_numpy_df_data,fmt = \"%.6f\", delimiter=\"\\t\", header=header, comments='', )\n",
    "    with open(new_archive_name, \"r\") as f:\n",
    "        data = f.read()\n",
    "    with open(new_archive_name, \"w\") as f:\n",
    "        f.write(data.replace(\"#DATA\\n\",\"#DATA\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3c74214",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "\n",
    "def savitzky_golay_filter(metadata, df) -> pd.DataFrame:\n",
    "    measuring_mode = metadata[\"measuring_mode\"]\n",
    "    y = df[measuring_mode] = df[measuring_mode].astype(float)\n",
    "    \n",
    "    # Apply Savitzky-Golay filter\n",
    "    window_length = 21  # Window length (odd number)\n",
    "    polyorder = 2  # Polynomial order\n",
    "    y_smooth = savgol_filter(y, window_length, polyorder)\n",
    "    \n",
    "    #create new dataframe\n",
    "    df_smooth = pd.DataFrame([])\n",
    "    df_smooth[\"wavelength\"] =df[\"wavelength\"]\n",
    "    df_smooth[measuring_mode] = y_smooth\n",
    "    \n",
    "    return df_smooth\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a48f977a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_list_extension(file_folder_data_path):\n",
    "    file_list = os.listdir(file_folder_data_path)\n",
    "\n",
    "    #file extension\n",
    "    file_extension = \".ASC\"\n",
    "\n",
    "    # filters a list of strings to create a new list containing only the elements that end with \".ASC\"\n",
    "\n",
    "    def filter_substring_elements(path_strings, substring):\n",
    "        filtered_paths = [path for path in path_strings if substring in path]\n",
    "        return filtered_paths\n",
    "\n",
    "    # Filtering elements ending with \".ASC\"\n",
    "    filtered_list = filter_substring_elements(file_list, file_extension)\n",
    "\n",
    "    # Displaying the filtered list\n",
    "    #print(filtered_list)\n",
    "    \n",
    "    return filtered_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4592545f",
   "metadata": {},
   "source": [
    "### Logic: Remove jumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4b9ec32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CICIMAUCR0002',\n",
       " 'CICIMAUCR0003',\n",
       " 'CICIMAUCR0004',\n",
       " 'CICIMAUCR0201',\n",
       " 'CICIMAUCR0204',\n",
       " 'CICIMAUCR0205',\n",
       " 'CICIMAUCR0209',\n",
       " 'CICIMAUCR0210',\n",
       " 'CICIMAUCR0212',\n",
       " 'CICIMAUCR0225',\n",
       " 'CICIMAUCR0250',\n",
       " 'CICIMAUCR0251'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### First we will get a list of filenamesin the folder\n",
    "filenames = []\n",
    "for path in file_folder_data_path: \n",
    "    filenames.extend(filter_list_extension(path))\n",
    "\n",
    "# From this list we will make a set of codes.\n",
    "#print(filenames)\n",
    "\n",
    "def get_sample_code(row_str):\n",
    "        #Tries to get the sample code from the filename\n",
    "        re1 = r\"([a-zA-Z\\d]+)(?:-\\d)*(?:.Sample)*.(?:txt)*(?:ASC)*\"\n",
    "        \n",
    "        #Names are in the form CODE-MEASUREMENTNUMBER.TXT\n",
    "        p = re.compile(re1)\n",
    "        m = p.match(row_str)\n",
    "\n",
    "        if m:\n",
    "            return(m.group(1))\n",
    "        else:\n",
    "            \"\"\n",
    "codes = []\n",
    "for filename in filenames:\n",
    "    codes.append(get_sample_code(filename))\n",
    "codes = set(codes)\n",
    "codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fd3915b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(filtered_list)=['CICIMAUCR0002-1.Sample.ASC', 'CICIMAUCR0002-2.Sample.ASC', 'CICIMAUCR0002-3.Sample.ASC', 'CICIMAUCR0003-1.Sample.ASC', 'CICIMAUCR0003-2.Sample.ASC', 'CICIMAUCR0003-3.Sample.ASC', 'CICIMAUCR0004-1.Sample.ASC', 'CICIMAUCR0004-2.Sample.ASC', 'CICIMAUCR0004-3.Sample.ASC', 'CICIMAUCR0201-1.Sample.ASC', 'CICIMAUCR0201-2.Sample.ASC', 'CICIMAUCR0201-3.Sample.ASC', 'CICIMAUCR0204-1.Sample.ASC', 'CICIMAUCR0204-2.Sample.ASC', 'CICIMAUCR0204-3.Sample.ASC', 'CICIMAUCR0205-1.Sample.ASC', 'CICIMAUCR0205-2.Sample.ASC', 'CICIMAUCR0205-3.Sample.ASC', 'CICIMAUCR0209-1.ASC', 'CICIMAUCR0209-2.ASC', 'CICIMAUCR0209-3.ASC', 'CICIMAUCR0210-1.ASC', 'CICIMAUCR0210-2.ASC', 'CICIMAUCR0210-3.ASC', 'CICIMAUCR0212-1.ASC', 'CICIMAUCR0212-2.ASC', 'CICIMAUCR0212-3.ASC', 'CICIMAUCR0225-1.Sample.ASC', 'CICIMAUCR0225-2.Sample.ASC', 'CICIMAUCR0225-3.Sample.ASC', 'CICIMAUCR0250-1.Sample.ASC', 'CICIMAUCR0250-2.Sample.ASC', 'CICIMAUCR0250-3.Sample.ASC', 'CICIMAUCR0251-1.Sample.ASC', 'CICIMAUCR0251-2.Sample.ASC', 'CICIMAUCR0251-3.Sample.ASC']\n",
      "len(filtered_list)=36\n",
      "len(filtered_list)=36\n",
      "0 files were corrected\n"
     ]
    }
   ],
   "source": [
    "#for each folder location:\n",
    "jump_corrected_dict = []\n",
    "file_counter = 0\n",
    "\n",
    "for folder in file_folder_data_path:\n",
    "    #create a list of all files in folder\n",
    "    \n",
    "    filtered_list = filter_list_extension(folder)\n",
    "    print(f\"{(filtered_list)=}\")\n",
    "    print(f\"{len(filtered_list)=}\")\n",
    "    \n",
    "    #if the folder has an archive called \"exclusion_list.txt\" read it and remove any file that is in the list\n",
    "    if \"exclusion_list.txt\" in os.listdir(folder):\n",
    "        exclusion_list_filename = os.path.join(folder,\"exclusion_list.txt\")\n",
    "        exclusion_list_df = pd.read_csv(exclusion_list_filename, header = 0, names=[\"filename\", \"reason\"], delimiter = \"\\t\")\n",
    "        exclusion_list = exclusion_list_df[\"filename\"].values\n",
    "        for excluded_file in exclusion_list:\n",
    "            filtered_list = [filename for filename in filtered_list if excluded_file not in filename ]\n",
    "        \n",
    "    print(f\"{len(filtered_list)=}\")\n",
    "    #create a subfolder called report with the correction process info  \n",
    "    report_path = os.path.join(folder, \"report\")  \n",
    "\n",
    "    if not os.path.exists(report_path):\n",
    "            os.mkdir(report_path)\n",
    "\n",
    "    new_file = os.path.join( report_path , f'original_data_{date}.pdf')\n",
    "    \n",
    "    #create a report in pdf\n",
    "    with matplotlib.backends.backend_pdf.PdfPages(new_file) as pdf:\n",
    "        \n",
    "        for code in codes: \n",
    "            #Code by code we will plot all spectra with the same name\n",
    "            code_file_counter = 1\n",
    "            \n",
    "            files_with_code = [file for file in filtered_list if code in file]\n",
    "            \n",
    "            plt.figure(figsize=(10, 4))\n",
    "            \n",
    "            for file in files_with_code:\n",
    "                \n",
    "                #print(f\"{code_file_counter}\")\n",
    "                #obtain metadata\n",
    "                complete_path = os.path.join(folder, file)\n",
    "                metadata, df = get_metadata_and_dataframe(complete_path)\n",
    "                filename = metadata[\"filename\"]\n",
    "\n",
    "                #After that we will average the spectra\n",
    "                #create figure\n",
    "                color = \"r\"\n",
    "                if code_file_counter == 2:\n",
    "                    color = \"b\"\n",
    "                elif code_file_counter ==3:\n",
    "                    color = \"g\"\n",
    "                    \n",
    "                #plots before changes\n",
    "                #plt.subplot(1, 2, 1)  # 1 row, 2 columns, subplot 1\n",
    "                plt.plot(df[\"wavelength\"], df[metadata[\"measuring_mode\"]], color = color, label = f\"{filename}\")\n",
    "                \n",
    "                #Then they will be jump corrected\n",
    "                #Fix jump correction\n",
    "                df_corrected = jump_correction(folder, file, metadata)\n",
    "                corrected_dict = {\"df\":df_corrected, \"complete_path\": complete_path, \"metadata\": metadata, \"code\":code}\n",
    "                \n",
    "                jump_corrected_dict.append(corrected_dict)\n",
    "                save_corrected_df(df_corrected, folder, filename, metadata)\n",
    "                \n",
    "                code_file_counter += 1\n",
    "                ### end for\n",
    "                \n",
    "            plt.title(f'1. Raw data. {code}')\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            #plt.show()\n",
    "            pdf.savefig()\n",
    "            plt.close()\n",
    "        \n",
    "        \n",
    "print(f\"{file_counter} files were corrected\")\n",
    "\n",
    "new_file = os.path.join( report_path , f'jump_correction_smoothing_average_report_{date}.pdf')\n",
    "\n",
    "smooth_dicts = []\n",
    "\n",
    "\n",
    "with matplotlib.backends.backend_pdf.PdfPages(new_file) as pdf:\n",
    "    \n",
    "    for code in codes:\n",
    "        file_counter = 1\n",
    "        #get dataframes for each code\n",
    "        dfs_dict = [element for element in jump_corrected_dict if element[\"code\"] == code]\n",
    "        #create figure\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        \n",
    "        for dict_i in dfs_dict:\n",
    "            #print(dict_i[\"df\"],dict_i[\"complete_path\"], dict_i[\"metadata\"])\n",
    "            df_corrected = dict_i[\"df\"]\n",
    "            metadata = dict_i[\"metadata\"]\n",
    "            complete_path = dict_i[\"complete_path\"]\n",
    "            filename = metadata[\"filename\"]\n",
    "            \n",
    "            #color\n",
    "            color = \"r\"\n",
    "            if file_counter== 2:\n",
    "                color = \"b\"\n",
    "            elif file_counter==3:\n",
    "                color = \"g\"\n",
    "            elif file_counter==4:\n",
    "                color = \"y\"\n",
    "                \n",
    "            #plots changes\n",
    "            if not df_corrected.empty:\n",
    "                #plt.subplot(1, 2, 2)\n",
    "                \n",
    "                plt.plot(df_corrected[\"wavelength\"], df_corrected[metadata[\"measuring_mode\"]], color = color, label = f'{filename}' )\n",
    "                plt.grid(True)\n",
    "                #plt.title(f'2: Jump correction.  {filename}')\n",
    "                #plt.show()\n",
    "                #pdf.savefig()\n",
    "                #plt.close()\n",
    "\n",
    "                #filter data\n",
    "                df_smooth = savitzky_golay_filter(metadata, df_corrected)\n",
    "                smooth_dict = {\"df\":df_smooth, \"complete_path\": complete_path, \"metadata\": metadata, \"code\":dict_i[\"code\"]}\n",
    "                smooth_dicts.append(smooth_dict)\n",
    "                file_counter = file_counter + 1\n",
    "                \n",
    "        #plt.show()\n",
    "        plt.legend()\n",
    "        plt.title(f\"Jump correction: {code}\")\n",
    "        pdf.savefig()\n",
    "        plt.close()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec3e953",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2adb7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b819dce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_with_same_name(directory):\n",
    "    #get files in directory\n",
    "    files = os.listdir(directory)\n",
    "    \n",
    "    sample_names = []\n",
    "    re1 = r\"([a-zA-Z\\d]+)(?:-\\d)*(?:.Sample)*.(?:txt)*(?:ASC)*\"\n",
    "    #Names are in the form CODE-MEASUREMENTNUMBER.TXT\n",
    "    for file in files:\n",
    "        #print(file)\n",
    "        p = re.compile(re1)\n",
    "        m = p.match(file)\n",
    "        if m:\n",
    "            sample_names.append(m.group(1))\n",
    "\n",
    "    return set(sample_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f82da9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_files(code_set, directory, pdf):\n",
    "    \n",
    "    #parent directory\n",
    "    save_directory = os.path.dirname(directory)\n",
    "    \n",
    "    #print(f\"{save_directory=}\")\n",
    "    \n",
    "    file_counter = 0\n",
    "    \n",
    "    for code in code_set:\n",
    "        #print(f\"{code=}\")\n",
    "        \n",
    "        files = [file for file in os.listdir(directory) if file.endswith('.txt') \n",
    "                 and \"fail\" not in file \n",
    "                 and \"test\" not in file\n",
    "                 and code in file\n",
    "                 and len(file)]\n",
    "        \n",
    "        #print(files)\n",
    "        \n",
    "        # Create an empty list to store dataframes\n",
    "        dataframes = []\n",
    "        \n",
    "        \n",
    "        # Read each file, create dataframes, and plot them\n",
    "        for file in files:\n",
    "            #print(f\"{file=}\")\n",
    "            #file path\n",
    "            file_path = os.path.join(directory, file)\n",
    "\n",
    "            #get metadata and dataframe\n",
    "            metadata, df = get_metadata_and_dataframe(file_path)\n",
    "\n",
    "            #metadata info\n",
    "            measuring_mode = metadata[\"measuring_mode\"]\n",
    "            sample_name = metadata[\"code\"] \n",
    "\n",
    "            # Read the file into a pandas DataFrame\n",
    "\n",
    "            #convert columns to float\n",
    "            df['wavelength'] = df['wavelength'].astype(float)\n",
    "            df[measuring_mode] = df[measuring_mode].astype(float)\n",
    "            \n",
    "            # Append the dataframe to the list\n",
    "            dataframes.append(df)\n",
    "            \n",
    "            #print(df)\n",
    "        \n",
    "        # Calculate the average of all dataframes\n",
    "        average_df = pd.concat(dataframes).groupby('wavelength').mean().reset_index()\n",
    "        std_dev_df = pd.concat(dataframes).groupby('wavelength').std().reset_index()\n",
    "\n",
    "        #print(f\"{average_df=}\")\n",
    "        #average plot\n",
    "        avg_plot = average_df.plot(x='wavelength', y=measuring_mode, kind='scatter', s=markersize, title = sample_name + \" avg\", figsize = (10, 4), grid=True)\n",
    "        #save figures\n",
    "        pdf.savefig()\n",
    "        #close plot\n",
    "        plt.close()\n",
    "        \n",
    "        #standard deviation plot\n",
    "        std_plot = std_dev_df.plot(x='wavelength', y=measuring_mode, color = \"r\", kind='scatter', s=markersize, title = sample_name + \" std dev\", figsize = (10,4), grid=True)\n",
    "        #save figures\n",
    "        pdf.savefig()\n",
    "        #close plot\n",
    "        plt.close()\n",
    "        \n",
    "        #SAVE FILE\n",
    "        #Convert to numpy\n",
    "        average_np = average_df.to_numpy()\n",
    "        std_dev_np = std_dev_df.to_numpy()\n",
    "        \n",
    "        #Create new folder\n",
    "      \n",
    "        avg_new_path = os.path.join(save_directory , \"average\")\n",
    "        std_dev_new_path = os.path.join(save_directory , \"std_dev\") \n",
    "\n",
    "        \n",
    "        if not os.path.exists(avg_new_path):\n",
    "            os.mkdir(avg_new_path)\n",
    "        if not os.path.exists(std_dev_new_path):\n",
    "            os.mkdir(std_dev_new_path)\n",
    "\n",
    "        #creates new filename\n",
    "        #print(f\"sample name: {sample_name}\")\n",
    "\n",
    "        new_avg_archive_name = os.path.join(avg_new_path, sample_name+'.txt')\n",
    "        new_std_dev_archive_name = os.path.join( std_dev_new_path , sample_name+'.txt')\n",
    "\n",
    "        #print(new_avg_archive_name)\n",
    "        #saves files\n",
    "        np.savetxt(new_avg_archive_name, average_np,fmt = \"%.6f\", delimiter=\"\\t\", header=metadata[\"header\"], comments='')\n",
    "        np.savetxt(new_std_dev_archive_name, std_dev_np,fmt = \"%.6f\", delimiter=\"\\t\", header=metadata[\"header\"], comments='')\n",
    "        #print(\"fin de la iteracion\")\n",
    "\n",
    "        def delete_blank_row(archive_name):\n",
    "            with open(archive_name, \"r\") as f:\n",
    "                data = f.read()\n",
    "            with open(archive_name, \"w\") as f:\n",
    "                f.write(data.replace(\"#DATA\\n\",\"#DATA\"))\n",
    "\n",
    "        delete_blank_row(new_avg_archive_name)\n",
    "        delete_blank_row(new_std_dev_archive_name)\n",
    "                         \n",
    "        file_counter = file_counter +1\n",
    "        \n",
    "    print(f\"{file_counter} averages were calculated\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "898b6d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 averages were calculated\n"
     ]
    }
   ],
   "source": [
    "for folder in file_folder_data_path:\n",
    "    \n",
    "    report_path = os.path.join(folder, \"report\")\n",
    "    avg_std_report_file = os.path.join(report_path, \"average_and_std_report.pdf\")\n",
    "    \n",
    "    with matplotlib.backends.backend_pdf.PdfPages(avg_std_report_file) as pdf:\n",
    "        \n",
    "        jump_corrected_path = os.path.join(folder , \"jump_corrected_files\")\n",
    "\n",
    "        code_set = get_files_with_same_name(jump_corrected_path)\n",
    "\n",
    "        average_files(code_set, jump_corrected_path , pdf)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb2802b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05018be1",
   "metadata": {},
   "source": [
    "\\v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec700321-d026-4f3d-a9cd-3e2107a07250",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
