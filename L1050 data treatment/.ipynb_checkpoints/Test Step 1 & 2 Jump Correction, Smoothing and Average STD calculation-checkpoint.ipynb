{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7021c4a",
   "metadata": {},
   "source": [
    "# Batch Lamda1050 jump correction script and smoothing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d722b6",
   "metadata": {},
   "source": [
    "### Author: Vinicio Soto, CICIMA\n",
    "### This script works with raw Lambdas1050's ASC files in batch. It corrects Lambdas1050's detector jump between 857 nm and 858 nm and performs a Savitzky-Golay filtering on the corrected data\n",
    "\n",
    "#### exclusion_list.txt: You can add a file with this name to the folder with the data to exclude any file with that name\n",
    "\n",
    "This script reads every l1050 file in folder and its subfolders and create a unique average, jump_corrected_files, report and std_dev folders\n",
    "It creates sections for each folder with its information\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24254f0e",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9f993aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd #Data analysis\n",
    "import numpy as np  #Array and numeric methods\n",
    "from matplotlib.backends.backend_pdf import PdfPages #Print PDFS\n",
    "import matplotlib #Print graphs\n",
    "import matplotlib.pyplot as plt #Print graphs\n",
    "import os #operating system\n",
    "import re #regular expression manipulation\n",
    "from datetime import datetime #date and time methods\n",
    "\n",
    "#import spectraltools\n",
    "#This script requires the file spectraltools.py to work\n",
    "import sys\n",
    "# Add the external folder to the system path\n",
    "current_dir = os.getcwd()\n",
    "external_folder_path = os.path.abspath(os.path.join(current_dir, '../libraries'))\n",
    "sys.path.append(external_folder_path)\n",
    "\n",
    "\n",
    "#This line of code allow us to access data in colab\n",
    "#functionality to reload modules\n",
    "\n",
    "import importlib\n",
    "import spectraltools\n",
    "import metrics\n",
    "import datapath_selector\n",
    "\n",
    "# clear the import cache\n",
    "importlib.reload(metrics)\n",
    "importlib.reload(spectraltools)\n",
    "importlib.reload(datapath_selector)\n",
    "# now you can import my_class and it'll be updated\n",
    "from metrics import *\n",
    "from spectraltools import *\n",
    "from datapath_selector import get_paths\n",
    "from pathlib import Path\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e36150b",
   "metadata": {},
   "source": [
    "### Instructions:\n",
    "\n",
    "Write your archive folders' path in file_folder_data_path list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba698280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EstebanSoto\\Jupyter\\escarabajos\\test\\data\\test_spectraltools\\reports\\jump_correction_CRAIC\n"
     ]
    }
   ],
   "source": [
    "#parent folder path\n",
    "\n",
    "#file_folder_data_path =[ #r\"C:\\Users\\EstebanSoto\\Jupyter\\escarabajos\\L1050_data\\CICIMA-2024-05-REFLECTANCE\\DORSAL\",\n",
    "                         #r\"C:\\Users\\EstebanSoto\\Jupyter\\escarabajos\\L1050_data\\CICIMA-2024-05-REFLECTANCE\\VENTRAL\",\n",
    "                         #r\"C:\\Users\\EstebanSoto\\Jupyter\\escarabajos\\L1050_data\\CICIMA-2024-03-REFLECTANCE\\without iris nor lens\",\n",
    "                         #r\"C:\\Users\\EstebanSoto\\Jupyter\\escarabajos\\L1050_data\\2024-04-INBUCR-REFLECTANCE\",\n",
    "                         #r\"C:\\Users\\EstebanSoto\\Jupyter\\escarabajos\\L1050_data\\2023-03-CICIMAUCR-2-REFLECTANCE\",\n",
    "                         #r\"C:\\Users\\esteb\\cicima\\escarabajos\\L1050_data\\CICIMA-2024-05-REFLECTANCE\\DORSAL\"\n",
    "                       #]\n",
    "parent_folder_data_path = Path(r\"C:\\Users\\EstebanSoto\\Jupyter\\escarabajos\\test\\data\\test_spectraltools\\jump_correction_CRAIC\") \n",
    "report_path = (parent_folder_data_path.parent / \"reports\" ) / parent_folder_data_path.name\n",
    "print(report_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9906ebe",
   "metadata": {},
   "source": [
    "### Constants\n",
    "Define your constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e00b7fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine \n",
    "markersize = 1\n",
    "#date\n",
    "date = str(datetime.today().date())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8fdd61",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253b68b9-f317-4e7c-8471-7cd7112c8934",
   "metadata": {},
   "source": [
    "#### Check if it is a L1050 file or a CRAIC file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1543d56",
   "metadata": {},
   "source": [
    "#### Get genera and species function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ddca9f",
   "metadata": {},
   "source": [
    "### Correction process function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40b25e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jump_correction(Spectrum):\n",
    "    \"\"\"Receives a l1050 spectrum and corrects its detector jump\"\"\"\n",
    "    metadata = Spectrum.get_metadata()\n",
    "    df = Spectrum.get_dataframe()\n",
    "    \n",
    "    #measuring_mode\n",
    "    measuring_mode = metadata[\"measuring_mode\"]\n",
    "    \n",
    "    #convert columns to float\n",
    "    df['wavelength'] = df['wavelength'].astype(float)\n",
    "    df[measuring_mode] = df[measuring_mode].astype(float)\n",
    "    \n",
    "\n",
    "    #This code finds the rows before and after the detector change\n",
    "    #freq1 is the frequency before the change and freq2 is the frequency after the change\n",
    "    wavelenght1 = 857.000000 #857\n",
    "    wavelenght2 = 858.000000 #858\n",
    "    \n",
    "    #print(\"Loc\")\n",
    "    row1 = df.loc[ df['wavelength'] == wavelenght1]\n",
    "    row2 = df.loc[ df['wavelength'] == wavelenght2]\n",
    "    row12 = df.loc[ df['wavelength'] == wavelenght1-1]\n",
    "    row13 = df.loc[ df['wavelength'] == wavelenght1-2]\n",
    "    row14 = df.loc[ df['wavelength'] == wavelenght1-3]\n",
    "    row15 = df.loc[ df['wavelength'] == wavelenght1-4]\n",
    "    \n",
    "    #This code finds the values corresponding to those frequencies and creates a correction factor\n",
    "    \n",
    "    try:\n",
    "        data1 = row1.iat[0,1]\n",
    "        data2 = row2.iat[0,1]\n",
    "        data22 = row12.iat[0,1]\n",
    "        data23 = row13.iat[0,1]\n",
    "        data24 = row14.iat[0,1]\n",
    "        data25 = row15.iat[0,1]\n",
    "    \n",
    "        data1_avg = (data1 + data22+data23)/3 #+data24+data25)/5\n",
    "        data2_avg = (data2)\n",
    "        correction_factor= data1_avg/data2_avg  \n",
    "    \n",
    "        #Multiply all frequencies equal or greater than freq2 by correction_factor\n",
    "        df2 = df\n",
    "        df2.loc[df2[\"wavelength\"] >= wavelenght2 , [measuring_mode]] *= correction_factor\n",
    "        \n",
    "        \n",
    "        return df\n",
    "    except Exception as e:\n",
    "        \n",
    "        print(e)\n",
    "        return pd.DataFrame([])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcf87b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_corrected_df(df, filename,metadata):\n",
    "    #variables\n",
    "    header = metadata[\"header\"]\n",
    "    \n",
    "    #convert dataframe to numpy\n",
    "    my_numpy_df_data = df.to_numpy()\n",
    "    \n",
    "    #Create new folder\n",
    "    corrected_path = filename\n",
    "\n",
    "    if not os.path.exists(corrected_path):\n",
    "        os.mkdir(corrected_path)\n",
    "\n",
    "    #saves file, removes .ASC and removes .SAMPLE\n",
    "    unnecessary_text = [\".Sample\", \".Cycle1\"]\n",
    "    temp_text = filename.replace('.ASC', ''+'.txt')\n",
    "    \n",
    "    for text in unnecessary_text:\n",
    "        #print(temp_text)\n",
    "        temp_text = temp_text.replace(text, \"\")\n",
    "\n",
    "    #new_archive_name = corrected_path + temp_text\n",
    "    new_archive_name = os.path.join(corrected_path , temp_text)\n",
    "\n",
    "    with open(new_archive_name, 'wb') as f:\n",
    "        np.savetxt(new_archive_name, my_numpy_df_data,fmt = \"%.6f\", delimiter=\"\\t\", header=header, comments='', )\n",
    "    with open(new_archive_name, \"r\") as f:\n",
    "        data = f.read()\n",
    "    with open(new_archive_name, \"w\") as f:\n",
    "        f.write(data.replace(\"#DATA\\n\",\"#DATA\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b04539-e690-4372-bc86-3f64421cf5b2",
   "metadata": {},
   "source": [
    "### Define Savitzky-Golay Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3c74214",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "\n",
    "def savitzky_golay_filter(metadata, df) -> pd.DataFrame:\n",
    "    \"\"\"Defines a Savitzky Golay Filter: Basically, smooths out the function. https://es.wikipedia.org/wiki/Filtro_de_Savitzky%E2%80%93Golay\"\"\"\n",
    "    measuring_mode = metadata[\"measuring_mode\"]\n",
    "    y = df[measuring_mode] = df[measuring_mode].astype(float)\n",
    "    \n",
    "    # Apply Savitzky-Golay filter\n",
    "    window_length = 21  # Window length (odd number)\n",
    "    polyorder = 2  # Polynomial order\n",
    "    y_smooth = savgol_filter(y, window_length, polyorder)\n",
    "    \n",
    "    #create new dataframe\n",
    "    df_smooth = pd.DataFrame([])\n",
    "    df_smooth[\"wavelength\"] =df[\"wavelength\"]\n",
    "    df_smooth[measuring_mode] = y_smooth\n",
    "    \n",
    "    return df_smooth\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1212b1-8865-4b9b-b772-4ed0aa1e2b45",
   "metadata": {},
   "source": [
    "### Define function that filters selected extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "522bc6c1-d113-47a0-8d86-218ef7a3e07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_all_filepaths(parent_folder):\n",
    "    filepaths = []\n",
    "    \n",
    "    # Walk through the directory\n",
    "    for root, dirs, files in os.walk(parent_folder):\n",
    "        for file in files:\n",
    "            # Combine the root directory with the file name to get the full path\n",
    "            filepath = os.path.join(root, file)\n",
    "            filepaths.append(filepath)\n",
    "    \n",
    "    return filepaths\n",
    "file_list = list_all_filepaths(parent_folder_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "273a2e5e-53a4-4cea-be87-6c9e88c228b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_exclusion_list(file_list):\n",
    "        \"\"\"Only files whose name is in the exclusion list\"\"\"\n",
    "        #file extension\n",
    "        file_extension = \"exclusion_list\"\n",
    "    \n",
    "        # filters a list of strings to create a new list containing only the elements that have exclusion_list\"\n",
    "    \n",
    "        def filter_substring_elements(path_strings, substring):\n",
    "            filtered_paths = [path for path in path_strings if substring in path]\n",
    "            return filtered_paths\n",
    "    \n",
    "        # Filter elements\n",
    "        filtered_list = filter_substring_elements(file_list, file_extension)\n",
    "    \n",
    "        # Displaying the filtered list\n",
    "        #print(filtered_list)\n",
    "        \n",
    "        return filtered_list\n",
    "exclusion_list = get_exclusion_list(file_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a48f977a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def filter_list_extension(file_list):\n",
    "    \"\"\"Only files that end in a particular extension are read\"\"\"\n",
    "    #file extension\n",
    "    file_extension = \".ASC\"\n",
    "\n",
    "    # filters a list of strings to create a new list containing only the elements that end with \".ASC\"\n",
    "\n",
    "    def filter_substring_elements(path_strings, substring):\n",
    "        filtered_paths = [path for path in path_strings if substring in path]\n",
    "        return filtered_paths\n",
    "\n",
    "    # Filtering elements ending with \".ASC\"\n",
    "    filtered_list = filter_substring_elements(file_list, file_extension)\n",
    "\n",
    "    # Displaying the filtered list\n",
    "    #print(filtered_list)\n",
    "    \n",
    "    return filtered_list\n",
    "filtered_list = filter_list_extension(file_list)\n",
    "filtered_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63349127-05e4-46f6-b62d-d27409c92ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_failed_runs(filtered_list):\n",
    "    \"\"\"Filter runs that failed\"\"\"\n",
    "    #file extension\n",
    "    fail_strings = [\"fail\"]\n",
    "\n",
    "    # filters a list of strings to create a new list containing only the elements that end with \".ASC\"\n",
    "\n",
    "    def filter_substring_elements(path_strings, substring):\n",
    "        filtered_paths = [path for path in path_strings if substring not in path]\n",
    "        return filtered_paths\n",
    "\n",
    "    # Filtering elements with any of the fail strings\n",
    "    for string in fail_strings: \n",
    "        filtered_list = filter_substring_elements(filtered_list, string)\n",
    "\n",
    "    # Displaying the filtered list\n",
    "    #print(filtered_list)\n",
    "    \n",
    "    return filtered_list\n",
    "\n",
    "successful_list = filter_failed_runs(filtered_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4592545f",
   "metadata": {},
   "source": [
    "### Logic: Remove jumps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4b9ec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "### First, we will get a list of filenames in the folder\n",
    "filenames = []\n",
    "for path in successful_list: \n",
    "    filenames.append(os.path.basename(path))\n",
    "\n",
    "# From this list we will make a set of codes.\n",
    "#print(filenames)\n",
    "\n",
    "def get_sample_code(row_str):\n",
    "        #Tries to get the sample code from the filename\n",
    "        re1 = r\"([a-zA-Z\\d]+)(?:-\\d)*(?:.Sample)*.(?:txt)*(?:ASC)*\"\n",
    "        \n",
    "        #Names are in the form CODE-MEASUREMENTNUMBER.TXT\n",
    "        p = re.compile(re1)\n",
    "        m = p.match(row_str)\n",
    "\n",
    "        if m:\n",
    "            return(m.group(1))\n",
    "        else:\n",
    "            \"\"\n",
    "codes = []\n",
    "for filename in filenames:\n",
    "    codes.append(get_sample_code(filename))\n",
    "codes = set(codes)\n",
    "codes = sorted(codes)\n",
    "\n",
    "#print(f\"codes {codes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a343144-05bf-4058-93a3-f61fb6fd4b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#for each file in the exclusion list\n",
    "exclusion_list_df = pd.DataFrame()\n",
    "for file in exclusion_list:\n",
    "    df = pd.read_csv(file, header = 0, names=[\"filename\", \"reason\"], delimiter = \"\\t\")\n",
    "    exclusion_list_df = pd.concat([exclusion_list_df, df], ignore_index = True)\n",
    "print(exclusion_list_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "056acf2a-2720-48fa-8b85-21d783164ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove excluded files\n",
    "def remove_excluded_files(successful_list, exclusion_list_df):\n",
    "    filtered_list = []\n",
    "\n",
    "    #if the file is not empty it gets all files that are not in that list\n",
    "    if not exclusion_list_df.empty:\n",
    "        #gets a exclusion list\n",
    "        exclusion_list = exclusion_list_df[\"filename\"].values\n",
    "        for excluded_file in exclusion_list:\n",
    "                    filtered_list = [filename for filename in successful_list if excluded_file not in filename ]\n",
    "    filtered_list = successful_list\n",
    "    return filtered_list\n",
    "    \n",
    "successful_list_without_excluded_files = remove_excluded_files(successful_list, exclusion_list_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d5f8c017-a4d3-4ab6-9068-b1bbd0074fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EstebanSoto\\AppData\\Local\\Temp\\ipykernel_44176\\3974710170.py:82: MatplotlibDeprecationWarning: Keeping empty pdf files is deprecated since 3.8 and support will be removed two minor releases later.\n",
      "  plt.close()\n"
     ]
    }
   ],
   "source": [
    "#Create report\n",
    "def create_report(filtered_list):\n",
    "    \"\"\"This function creates a report\"\"\"\n",
    "    jump_corrected_dict = []\n",
    "    file_counter = 0\n",
    "    \n",
    "    #create a subfolder called report with the correction process info  \n",
    "    report_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    #create report name\n",
    "    new_file = os.path.join( report_path , f'original_data_{date}.pdf')\n",
    "\n",
    "    #make a list of all files with code\n",
    "    files_with_code = []\n",
    "    for code in codes:\n",
    "        files_with_code += [file for file in filtered_list if code in file]\n",
    "\n",
    "    print(f\"{files_with_code=}\")\n",
    "    \n",
    "    #create a report in pdf\n",
    "    with matplotlib.backends.backend_pdf.PdfPages(new_file) as pdf:\n",
    "        \n",
    "        for code in codes: \n",
    "            #Code by code we will plot all the spectra with the same name\n",
    "            code_file_counter = 1\n",
    "            \n",
    "            files_with_code = [file for file in filtered_list if code in file]\n",
    "            \n",
    "            plt.figure(figsize=(10, 4))\n",
    "            \n",
    "            for file in files_with_code:\n",
    "                \n",
    "                #print(f\"{code_file_counter}\")\n",
    "                #create a spectrum\n",
    "\n",
    "                spectrum = Spectrum(file, collection)\n",
    "                \n",
    "                metadata, df = get_metadata_and_dataframe(file)\n",
    "                filename = metadata[\"filename\"]\n",
    "\n",
    "                #After that we will average the spectra\n",
    "                #create figure\n",
    "                color = \"r\"\n",
    "                if code_file_counter == 2:\n",
    "                    color = \"b\"\n",
    "                elif code_file_counter ==3:\n",
    "                    color = \"g\"\n",
    "                elif code_file_counter ==4:\n",
    "                    color = \"cyan\"\n",
    "                elif code_file_counter ==5:\n",
    "                    color = \"magenta\"\n",
    "                elif code_file_counter ==6:\n",
    "                    color = \"yellow\"\n",
    "                elif code_file_counter ==7:\n",
    "                    color = \"black\"\n",
    "            \n",
    "                \n",
    "                #plots before changes\n",
    "                plt.plot(df[\"wavelength\"], df[metadata[\"measuring_mode\"]], color = color, label = f\"{filename}\")\n",
    "                \n",
    "                #If the file is a l1050 file they will be jump corrected\n",
    "                #Fix jump correction\n",
    "                if check_l1050_file(file):\n",
    "                    df_corrected = jump_correction(spectrum)\n",
    "                else:\n",
    "                    df_corrected = spectrum.get_dataframe()\n",
    "                corrected_dict = {\"df\":df_corrected, \"complete_path\": file, \"metadata\": metadata, \"code\":code}\n",
    "                \n",
    "                jump_corrected_dict.append(corrected_dict)\n",
    "                save_corrected_df(df_corrected, filename, metadata)\n",
    "                \n",
    "                code_file_counter += 1\n",
    "                file_counter += 1\n",
    "                ### end for\n",
    "                \n",
    "            plt.title(f'1. Raw data. {code}')\n",
    "            plt.ylim(0,100)\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            #plt.show()\n",
    "            pdf.savefig()\n",
    "            plt.close()\n",
    "\n",
    "create_report(successful_list_without_excluded_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6264b7c1-d43c-4c70-8aa0-ffd31824b480",
   "metadata": {},
   "source": [
    "#for each folder location:\n",
    "jump_corrected_dict = []\n",
    "file_counter = 0\n",
    "\n",
    "for folder in file_folder_data_path:\n",
    "    #create a list of all files in folder\n",
    "    \n",
    "    filtered_list = filter_list_extension(folder)\n",
    "    print(f\"{(filtered_list)=}\")\n",
    "    print(f\"{len(filtered_list)=}\")\n",
    "    \n",
    "    #if the folder has an archive called \"exclusion_list.txt\" read it and remove any file that is in the list\n",
    "    if \"exclusion_list.txt\" in os.listdir(folder):\n",
    "        exclusion_list_filename = os.path.join(folder,\"exclusion_list.txt\")\n",
    "        exclusion_list_df = pd.read_csv(exclusion_list_filename, header = 0, names=[\"filename\", \"reason\"], delimiter = \"\\t\")\n",
    "        exclusion_list = exclusion_list_df[\"filename\"].values\n",
    "        for excluded_file in exclusion_list:\n",
    "            filtered_list = [filename for filename in filtered_list if excluded_file not in filename ]\n",
    "        \n",
    "    print(f\"{len(filtered_list)=}\")\n",
    "    \n",
    "    #create a subfolder called report with the correction process info  \n",
    "    report_path = os.path.join(folder, \"report\")  \n",
    "\n",
    "    if not os.path.exists(report_path):\n",
    "            os.mkdir(report_path)\n",
    "\n",
    "    new_file = os.path.join( report_path , f'original_data_{date}.pdf')\n",
    "    \n",
    "    #create a report in pdf\n",
    "    with matplotlib.backends.backend_pdf.PdfPages(new_file) as pdf:\n",
    "        \n",
    "        for code in codes: \n",
    "            #Code by code we will plot all the spectra with the same name\n",
    "            code_file_counter = 1\n",
    "            \n",
    "            files_with_code = [file for file in filtered_list if code in file]\n",
    "            \n",
    "            plt.figure(figsize=(10, 4))\n",
    "            \n",
    "            for file in files_with_code:\n",
    "                \n",
    "                #print(f\"{code_file_counter}\")\n",
    "                #obtain metadata\n",
    "                complete_path = os.path.join(folder, file)\n",
    "                metadata, df = get_metadata_and_dataframe(complete_path)\n",
    "                filename = metadata[\"filename\"]\n",
    "\n",
    "                #After that we will average the spectra\n",
    "                #create figure\n",
    "                color = \"r\"\n",
    "                if code_file_counter == 2:\n",
    "                    color = \"b\"\n",
    "                elif code_file_counter ==3:\n",
    "                    color = \"g\"\n",
    "                    \n",
    "                #plots before changes\n",
    "                #plt.subplot(1, 2, 1)  # 1 row, 2 columns, subplot 1\n",
    "                plt.plot(df[\"wavelength\"], df[metadata[\"measuring_mode\"]], color = color, label = f\"{filename}\")\n",
    "                \n",
    "                #Then they will be jump corrected\n",
    "                #Fix jump correction\n",
    "                df_corrected = jump_correction(folder, file, metadata)\n",
    "                corrected_dict = {\"df\":df_corrected, \"complete_path\": complete_path, \"metadata\": metadata, \"code\":code}\n",
    "                \n",
    "                jump_corrected_dict.append(corrected_dict)\n",
    "                save_corrected_df(df_corrected, folder, filename, metadata)\n",
    "                \n",
    "                code_file_counter += 1\n",
    "                file_counter += 1\n",
    "                ### end for\n",
    "                \n",
    "            plt.title(f'1. Raw data. {code}')\n",
    "            plt.ylim(0,100)\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            #plt.show()\n",
    "            pdf.savefig()\n",
    "            plt.close()\n",
    "        \n",
    "        \n",
    "print(f\"{file_counter} files were corrected\")\n",
    "\n",
    "new_file = os.path.join( report_path , f'jump_correction_smoothing_average_report_{date}.pdf')\n",
    "\n",
    "smooth_dicts = []\n",
    "\n",
    "\n",
    "with matplotlib.backends.backend_pdf.PdfPages(new_file) as pdf:\n",
    "    \n",
    "    for code in codes:\n",
    "        file_counter = 1\n",
    "        #get dataframes for each code\n",
    "        dfs_dict = [element for element in jump_corrected_dict if element[\"code\"] == code]\n",
    "        #create figure\n",
    "        plt.figure(figsize=(10, 4))\n",
    "        \n",
    "        for dict_i in dfs_dict:\n",
    "            #print(dict_i[\"df\"],dict_i[\"complete_path\"], dict_i[\"metadata\"])\n",
    "            df_corrected = dict_i[\"df\"]\n",
    "            metadata = dict_i[\"metadata\"]\n",
    "            complete_path = dict_i[\"complete_path\"]\n",
    "            filename = metadata[\"filename\"]\n",
    "            \n",
    "            #color\n",
    "            color = \"r\"\n",
    "            if file_counter== 2:\n",
    "                color = \"b\"\n",
    "            elif file_counter==3:\n",
    "                color = \"g\"\n",
    "            elif file_counter==4:\n",
    "                color = \"y\"\n",
    "                \n",
    "            #plots changes\n",
    "            if not df_corrected.empty:\n",
    "                #plt.subplot(1, 2, 2)\n",
    "                \n",
    "                plt.plot(df_corrected[\"wavelength\"], df_corrected[metadata[\"measuring_mode\"]], color = color, label = f'{filename}' )\n",
    "                plt.grid(True)\n",
    "                #plt.title(f'2: Jump correction.  {filename}')\n",
    "                #plt.show()\n",
    "                #pdf.savefig()\n",
    "                #plt.close()\n",
    "\n",
    "                #filter data\n",
    "                df_smooth = savitzky_golay_filter(metadata, df_corrected)\n",
    "                smooth_dict = {\"df\":df_smooth, \"complete_path\": complete_path, \"metadata\": metadata, \"code\":dict_i[\"code\"]}\n",
    "                smooth_dicts.append(smooth_dict)\n",
    "                file_counter = file_counter + 1\n",
    "                \n",
    "        #plt.show()\n",
    "        plt.ylim(0,100)\n",
    "        plt.legend()\n",
    "        plt.title(f\"Jump correction: {code}\")\n",
    "        pdf.savefig()\n",
    "        plt.close()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33133a4-400a-46c5-90ca-69a1e943bd02",
   "metadata": {},
   "source": [
    "#calculate averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b819dce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files_with_same_name(filtered_list):\n",
    "    #get files in directory\n",
    "    files = []\n",
    "    for file in filtered_list:\n",
    "        files.append( os.path.basename(file))\n",
    "    \n",
    "    #print(files)\n",
    "    \n",
    "    sample_names = []\n",
    "    re1 = r\"([a-zA-Z\\d]+)(?:-\\d)*(?:.Sample)*.(?:txt)*(?:ASC)*\"\n",
    "    #Names are in the form CODE-MEASUREMENTNUMBER.TXT\n",
    "    for file in files:\n",
    "        #print(file)\n",
    "        p = re.compile(re1)\n",
    "        m = p.match(file)\n",
    "        if m:\n",
    "            sample_names.append(m.group(1))\n",
    "\n",
    "    return set(sample_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f82da9b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_dataframes = None\n",
    "def average_files(file_list, code_set, directory, pdf):\n",
    "\n",
    "    \n",
    "    #parent directory\n",
    "    jump_corrected_path = os.path.join( directory , \"jump_corrected_files\")\n",
    "    save_directory = os.path.dirname(jump_corrected_path)\n",
    "    \n",
    "    #print(f\"{save_directory=}\")\n",
    "    file_counter = 0\n",
    "\n",
    "    #Look for every available code in the folder\n",
    "    for code in code_set:\n",
    "        print(f\"{code=}\") \n",
    "        \n",
    "        dataframes = []\n",
    "        # Read each file, create dataframes, and plot them\n",
    "        for file_path in file_list:\n",
    "            print(f\"{file_path=}\")\n",
    "\n",
    "            #get metadata and dataframe\n",
    "            print(\"Reading file...\")\n",
    "            print(\"Getting metadata and dataframe...\")\n",
    "            metadata, df = get_metadata_and_dataframe(file_path)\n",
    "\n",
    "            #metadata info\n",
    "            measuring_mode = metadata[\"measuring_mode\"]\n",
    "            sample_name = metadata[\"code\"] \n",
    "\n",
    "            # Read the file into a pandas DataFrame\n",
    "            #convert columns to float\n",
    "            print(\"Converting columns to float...\")\n",
    "            df['wavelength'] = df['wavelength'].astype(float)\n",
    "            df[measuring_mode] = df[measuring_mode].astype(float)\n",
    "            \n",
    "            print(\"Dropping index...\")\n",
    "            df = df.drop(\"index\", axis = 1)\n",
    "            \n",
    "            print(\"Printing head...\")\n",
    "            print(df.head())\n",
    "            # Append the dataframe to the list if not empty\n",
    "            if not df.empty and not None and len(df) != 0:\n",
    "                dataframes.append(df) \n",
    "            \n",
    "        # Calculate the average of all dataframes ignore empty ones\n",
    "        print(\"elements\")\n",
    "        \n",
    "        #print(dataframes)\n",
    "        print(len(dataframes))\n",
    "\n",
    "        #If there are no dataframes, continue\n",
    "        if len(dataframes) == 0:\n",
    "            continue\n",
    "        #If there is only one \n",
    "        elif len(dataframes) == 1:\n",
    "            average_df = dataframes[0]\n",
    "            std_dev_df = pd.DataFrame([])\n",
    "        #If there are many\n",
    "        else:\n",
    "            average_df = pd.concat(dataframes).groupby('wavelength').mean().reset_index()\n",
    "            std_dev_df = pd.concat(dataframes).groupby('wavelength').std().reset_index()\n",
    "\n",
    "        try: \n",
    "            #print(f\"{average_df=}\")\n",
    "            \n",
    "            #average plot\n",
    "            avg_plot = average_df.plot(x='wavelength', y=measuring_mode, kind='scatter', s=markersize, title = sample_name + \" avg\", figsize = (10, 4), grid=True)\n",
    "            #save figures\n",
    "            pdf.savefig()\n",
    "            #close plot\n",
    "            plt.close()\n",
    "            \n",
    "            #standard deviation plot\n",
    "            std_plot = std_dev_df.plot(x='wavelength', y=measuring_mode, color = \"r\", kind='scatter', s=markersize, title = sample_name + \" std dev\", figsize = (10,4), grid=True)\n",
    "            #save figures\n",
    "            pdf.savefig()\n",
    "            #close plot\n",
    "            plt.close()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            \n",
    "        #SAVE FILE\n",
    "        #Convert to numpy\n",
    "        average_np = average_df.to_numpy()\n",
    "        std_dev_np = std_dev_df.to_numpy()\n",
    "        \n",
    "        #Create new folder\n",
    "      \n",
    "        avg_new_path = os.path.join(save_directory , \"average\")\n",
    "        std_dev_new_path = os.path.join(save_directory , \"std_dev\") \n",
    "\n",
    "        \n",
    "        if not os.path.exists(avg_new_path):\n",
    "            os.mkdir(avg_new_path)\n",
    "        if not os.path.exists(std_dev_new_path):\n",
    "            os.mkdir(std_dev_new_path)\n",
    "\n",
    "        #creates new filename\n",
    "        #print(f\"sample name: {sample_name}\")\n",
    "\n",
    "        new_avg_archive_name = os.path.join(avg_new_path, sample_name+'.txt')\n",
    "        new_std_dev_archive_name = os.path.join( std_dev_new_path , sample_name+'.txt')\n",
    "\n",
    "        #print(new_avg_archive_name)\n",
    "        #saves files\n",
    "        np.savetxt(new_avg_archive_name, average_np,fmt = \"%.6f\", delimiter=\"\\t\", header=metadata[\"header\"], comments='')\n",
    "        np.savetxt(new_std_dev_archive_name, std_dev_np,fmt = \"%.6f\", delimiter=\"\\t\", header=metadata[\"header\"], comments='')\n",
    "        #print(\"fin de la iteracion\")\n",
    "\n",
    "        def delete_blank_row(archive_name):\n",
    "            with open(archive_name, \"r\") as f:\n",
    "                data = f.read()\n",
    "            with open(archive_name, \"w\") as f:\n",
    "                f.write(data.replace(\"#DATA\\n\",\"#DATA\"))\n",
    "\n",
    "        delete_blank_row(new_avg_archive_name)\n",
    "        delete_blank_row(new_std_dev_archive_name)\n",
    "                         \n",
    "        file_counter = file_counter +1\n",
    "        \n",
    "    print(f\"{file_counter} averages were calculated\")\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "08aa177d-8461-401a-9124-fe6aac787cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Calculate the average for each spectrum if it has the same name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c3ed935-c788-4d20-88b1-4db174b42b58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "0 averages were calculated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EstebanSoto\\AppData\\Local\\Temp\\ipykernel_44176\\2488914716.py:7: MatplotlibDeprecationWarning: Keeping empty pdf files is deprecated since 3.8 and support will be removed two minor releases later.\n",
      "  average_files(successful_list_without_excluded_files, code_set, parent_folder_data_path , pdf)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "avg_std_report_file = os.path.join(report_path, \"average_and_std_report.pdf\")\n",
    "#print(avg_std_report_file)\n",
    "\n",
    "with matplotlib.backends.backend_pdf.PdfPages(avg_std_report_file) as pdf:\n",
    "    code_set = get_files_with_same_name(successful_list_without_excluded_files)\n",
    "    print(code_set)\n",
    "    average_files(successful_list_without_excluded_files, code_set, parent_folder_data_path , pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "898b6d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EstebanSoto\\Jupyter\\escarabajos\\test\\data\\test_spectraltools\\reports\\jump_correction_CRAIC\\average_and_std_report.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EstebanSoto\\AppData\\Local\\Temp\\ipykernel_44176\\1893446972.py:12: MatplotlibDeprecationWarning: Keeping empty pdf files is deprecated since 3.8 and support will be removed two minor releases later.\n",
      "  average_files(code_set, jump_corrected_path , pdf)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "average_files() missing 1 required positional argument: 'pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m jump_corrected_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(parent_folder_data_path , \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjump_corrected_files\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m code_set \u001b[38;5;241m=\u001b[39m get_files_with_same_name(jump_corrected_path)\n\u001b[1;32m---> 12\u001b[0m \u001b[43maverage_files\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcode_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjump_corrected_path\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: average_files() missing 1 required positional argument: 'pdf'"
     ]
    }
   ],
   "source": [
    "#Create jump_corrected_files report\n",
    "\n",
    "avg_std_report_file = os.path.join(report_path, \"average_and_std_report.pdf\")\n",
    "print(avg_std_report_file)\n",
    "\n",
    "with matplotlib.backends.backend_pdf.PdfPages(avg_std_report_file) as pdf:\n",
    "    \n",
    "    jump_corrected_path = os.path.join(parent_folder_data_path , \"jump_corrected_files\")\n",
    "\n",
    "    code_set = get_files_with_same_name(jump_corrected_path)\n",
    "\n",
    "    average_files(code_set, jump_corrected_path , pdf)\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec700321-d026-4f3d-a9cd-3e2107a07250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb5257d-6211-47e9-9d4e-14b6e1940721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c11e5c-ff24-411c-9284-f6645bceba1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
